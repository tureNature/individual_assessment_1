<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unveiling the Tricks: Automated Detection of Dark Patterns in Mobile Applications</title>
			</titleStmt>
			<publicationStmt>
				<publisher>ACM</publisher>
				<availability status="unknown"><p>Copyright ACM</p>
				</availability>
				<date type="published" when="2023-08-11">11 Aug 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,110.03,131.50,64.44,10.59"><forename type="first">Jieshan</forename><surname>Chen</surname></persName>
							<email>jieshan.chen@data61.csiro.au</email>
							<idno type="ORCID">0000-0002-2700-7478</idno>
							<affiliation key="aff0">
								<orgName type="department">CSIRO&apos;s Data61</orgName>
								<address>
									<country>Australia Sidong Feng</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Monash University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">CSIRO&apos;s Data61</orgName>
								<orgName type="institution">Qinghua Lu</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">CSIRO&apos;s Data61</orgName>
								<address>
									<settlement>Chunyang Chen</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Monash University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Also with</orgName>
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="institution">UIST &apos;23</orgName>
								<address>
									<addrLine>October 29-November 1</addrLine>
									<postCode>2023</postCode>
									<settlement>San Francisco</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff7">
								<orgName type="institution">UIST &apos;23</orgName>
								<address>
									<addrLine>October 29-November 1</addrLine>
									<postCode>2023</postCode>
									<settlement>San Francisco</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiamou</forename><surname>Sun</surname></persName>
							<idno type="ORCID">0000-0002-5212-7068</idno>
						</author>
						<author>
							<persName><forename type="first">Sidong</forename><surname>Feng</surname></persName>
							<idno type="ORCID">0000-0001-7740-0377</idno>
						</author>
						<author>
							<persName><forename type="first">Zhenchang</forename><surname>Xing</surname></persName>
							<idno type="ORCID">0000-0001-7663-1421</idno>
						</author>
						<author>
							<persName><forename type="first">Qinghua</forename><surname>Lu</surname></persName>
							<idno type="ORCID">0000-0002-7783-5183</idno>
						</author>
						<author>
							<persName><forename type="first">Xiwei</forename><surname>Xu</surname></persName>
							<idno type="ORCID">0000-0002-2273-1862</idno>
						</author>
						<author>
							<persName><forename type="first">Chunyang</forename><surname>Chen</surname></persName>
							<idno type="ORCID">0000-0003-2011-9618</idno>
						</author>
						<title level="a" type="main">Unveiling the Tricks: Automated Detection of Dark Patterns in Mobile Applications</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology</title>
						<meeting>the 36th Annual ACM Symposium on User Interface Software and Technology						</meeting>
						<imprint>
							<publisher>ACM</publisher>
							<date type="published" when="2023-08-11">11 Aug 2023</date>
						</imprint>
					</monogr>
					<idno type="MD5">A4BAECB181A6ACD97109A6D5DCFE426C</idno>
					<idno type="DOI">10.1145/3586183.3606783</idno>
					<idno type="arXiv">arXiv:2308.05898v1[cs.HC]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2024-03-02T19:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>HCI theory</term>
					<term>concepts and models;</term>
					<term>Computing methodologies → Computer vision Dark Pattern</term>
					<term>Ethical Design</term>
					<term>User Interface</term>
					<term>Mobile App</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Mobile apps bring us many conveniences, such as online shopping and communication, but some use malicious designs called dark patterns to trick users into doing things that are not in their best interest. Many works have been done to summarize the taxonomy of these patterns and some have tried to mitigate the problems through various techniques. However, these techniques are either time-consuming, not generalisable or limited to specific patterns. To address these issues, we propose UIGuard, a knowledge-driven system that utilizes computer vision and natural language pattern matching to automatically detect a wide range of dark patterns in mobile UIs. Our system relieves the need for manually creating rules for each new UI/app and covers more types with superior performance. In detail, we integrated existing taxonomies into a consistent one, conducted a characteristic analysis and distilled knowledge from real-world examples and the taxonomy. Our UIGuard consists of two components, Property Extraction and Knowledge-Driven Dark Pattern Checker. We collected the first dark pattern dataset, which contains 4,999 benign UIs and 1,353 malicious UIs of 1,660 instances spanning 1,023 mobile apps. Our system achieves a superior performance in detecting dark patterns (micro averages: 0.82 in precision, 0.77 in recall, 0.79 in F1 score). A user study involving 58 participants further shows that UIGuard significantly increases users' knowledge of dark patterns.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Mobile apps have been indispensable parts in our daily life, bringing conveniences and essential functionalities in various aspects, such as online shopping, remote work, and communication with families and friends. However, the user interfaces (UIs) of these apps, which serve as a bridge between end-users and the app, can sometimes contain malicious design elements 1 , that may cause harms to end-users. One specific example of this is the use of "dark patterns", which are maliciously crafted user interfaces designed to trick users into doing something they do not want to do <ref type="bibr" coords="1,546.89,571.69,9.52,7.94" target="#b3">[4]</ref>. These dark patterns can take many forms and cause different types of damage. For example, preselection tricks (see Figure <ref type="figure" coords="1,522.04,593.61,9.61,7.94">1(c</ref>)) automatically select favorable options for the app, pressuring users to subscribe to unwanted newsletters. Roach motel patterns make it easy for users to sign up for a service but difficult or impossible for them to unsubscribe, causing frustration and inconvenience <ref type="bibr" coords="1,534.76,637.45,9.23,7.94" target="#b3">[4,</ref><ref type="bibr" coords="1,546.18,637.45,10.05,7.94" target="#b19">21]</ref>. Recent research found that 11% of 11K shopping websites contain dark patterns <ref type="bibr" coords="1,371.27,659.37,13.49,7.94" target="#b32">[34]</ref>, and 95% of 240 popular mobile apps include at least seven different types of malicious designs in their UIs on average <ref type="bibr" coords="1,348.61,681.28,13.36,7.94" target="#b16">[18]</ref>. Figure <ref type="figure" coords="2,81.40,298.33,3.38,7.70">1</ref>: Examples of dark patterns in mobile apps. (a) A screenshot displaying an ad page UI with two distinct buttons, "Close" and "Install", centrally located. (b) A UI screenshot showing podcast channels listed alongside an ad, identical to regular content, and featuring a small, possibly challenging-to-click close button on the top right. (c) A UI showcasing a settings page with the notification setting enabled. (d) A UI displaying a pop-up window encouraging users to sign up for a 30-day free trial, with small text at the bottom indicating automatic continuation of subscription for $32.99/month post-trial. (e) A payment page UI where users can purchase virtual currency with real currency.</p><p>To raise the awareness of dark patterns and categorise them, many researchers conducted different systematic empirical studies on understanding and evaluating the existence of dark patterns in various platforms <ref type="bibr" coords="2,118.11,413.97,13.40,7.94" target="#b16">[18,</ref><ref type="bibr" coords="2,133.27,413.97,10.27,7.94" target="#b19">21,</ref><ref type="bibr" coords="2,145.30,413.97,11.47,7.94" target="#b32">34]</ref> and contexts <ref type="bibr" coords="2,205.38,413.97,9.23,7.94" target="#b0">[1,</ref><ref type="bibr" coords="2,216.37,413.97,10.27,7.94" target="#b18">20,</ref><ref type="bibr" coords="2,228.40,413.97,10.27,7.94" target="#b20">22,</ref><ref type="bibr" coords="2,240.43,413.97,10.27,7.94" target="#b22">24,</ref><ref type="bibr" coords="2,252.46,413.97,10.05,7.94" target="#b33">35]</ref>. In 2010, Harry Brignull created a website called dark pattern <ref type="bibr" coords="2,252.96,424.93,9.52,7.94" target="#b3">[4]</ref>. He not only built a website that catalogued and defined 12 types of dark patterns on internet and provided illustrative examples, but also continuously collects and exposes new dark pattern instances to "name and shame" companies through crowd-sourcing techniques on Twitter <ref type="bibr" coords="2,93.12,479.72,9.27,7.94" target="#b4">[5]</ref>. Following Brignull, many studies have expanded our understanding of dark patterns and their implementation in user interfaces by identifying additional strategies and types, analyzing their potential harm, and conducting a comparative study. They shed light on the prevalence and impact of dark patterns on user behavior and decision-making in various contexts <ref type="bibr" coords="2,240.88,534.52,13.57,7.94" target="#b16">[18,</ref><ref type="bibr" coords="2,256.69,534.52,10.34,7.94" target="#b19">21,</ref><ref type="bibr" coords="2,269.28,534.52,10.34,7.94" target="#b21">23,</ref><ref type="bibr" coords="2,281.86,534.52,10.17,7.94" target="#b32">34]</ref>. However, these taxonomies are heterogeneous and lack consistency, which creates obstacles for classifying specific dark patterns.</p><p>In parallel, to mitigate and overcome the existence of dark patterns, some researchers proposed various solutions, including crowdsourcing techniques <ref type="bibr" coords="2,130.12,589.31,9.38,7.94" target="#b4">[5,</ref><ref type="bibr" coords="2,141.73,589.31,10.16,7.94" target="#b34">36]</ref>, developer/user patches <ref type="bibr" coords="2,244.71,589.31,13.41,7.94" target="#b25">[27]</ref>, or using some naive text-based classification techniques <ref type="bibr" coords="2,228.73,600.27,13.51,7.94" target="#b36">[38,</ref><ref type="bibr" coords="2,244.49,600.27,10.13,7.94" target="#b38">41]</ref>. However, these techniques are either time-consuming, hard to generalise or limited to certain patterns. Crowd-sourcing techniques aim to provide a platform for end-users to report instances of dark patterns in their daily life and raise awareness <ref type="bibr" coords="2,198.66,644.10,9.44,7.94" target="#b4">[5,</ref><ref type="bibr" coords="2,210.84,644.10,10.21,7.94" target="#b34">36]</ref>, but they are timeconsuming and may not be generalised to new UIs. People who do not read these posts are unable to learn this knowledge. Developer patches involve modifying code files to remove dark patterns and repackaging apps <ref type="bibr" coords="2,120.13,687.94,13.37,7.94" target="#b25">[27]</ref>, but this method requires programming expertise and may not scale well. Moreover, patches can accidentally affect app functionality, and repackaging apps can raise privacy concerns <ref type="bibr" coords="2,352.70,392.05,13.30,7.94" target="#b13">[14]</ref>. User patches allows users to provide screenshots of areas to be modified, but this approach requires significant user input and is not generalizable. <ref type="bibr" coords="2,430.63,413.97,13.36,7.94" target="#b13">[14]</ref>. Text-based classification techniques can automatically detect and highlight/remove partial dark patterns, but they require a large labelled dataset to train the model and may produce many false positives and false negatives as they only rely on texts <ref type="bibr" coords="2,383.95,457.80,13.30,7.94" target="#b36">[38]</ref>. Overall, these solutions have limitations in terms of scalability, generalizability, and effectiveness, and there is a need for further research to develop more effective methods for detecting and addressing dark patterns.</p><p>In this work, we take a step to unify the existing taxonomies and propose a knowledge-driven system called UIGuard that automatically detects dark patterns in mobile UIs. Our system leverages computer vision and natural language matching techniques to effectively and efficiently detect dark patterns, covers a wider range of types with superior performance and relieves the need for creating rules or patches for each new UI/app. To achieve it, we first analysed and merged existing taxonomies of dark patterns in mobile platforms <ref type="bibr" coords="2,341.51,589.31,13.53,7.94" target="#b16">[18,</ref><ref type="bibr" coords="2,357.28,589.31,10.32,7.94" target="#b19">21,</ref><ref type="bibr" coords="2,369.84,589.31,7.70,7.94" target="#b21">23</ref>] into a consistent one to form a solid knowledge base of our detection system. We then conducted a characteristic analysis on the unified taxonomy to inform the design of our automated dark pattern detection system at the screen level and element level, and identified identified six essential properties, including element coordinates and types, text contents, icon semantics, element status, element colors and relationships, for detecting dark patterns.</p><p>Our UIGuard leverages computer vision and natural language pattern matching techniques. It is purely vision-based, taking only a UI screenshot as input and requiring no other metadata or information. This enables the generalisability of our proposed techniques to other platforms. UIGuard consists of two steps, namely UI element property extraction and knowledge driven dark pattern checker. UI element property extraction involves seven modules to gradually extract vision and textual properties from a UI screenshot. Knowledge driven dark pattern checker incorporated the heuristics distilled from our knowledge base.</p><p>To evaluate the efficiency and usefulness of our proposed systems, we use Rico dataset <ref type="bibr" coords="3,152.98,186.42,14.85,7.94" target="#b15">[16]</ref> and its semantic labeling <ref type="bibr" coords="3,266.79,186.42,13.49,7.94" target="#b30">[32]</ref>, as our base datasets, to train the deep learning modules. To evaluate the whole system, we established a set of standards to annotate the types and locations of dark patterns. We selected the testset of Rico dataset as our annotation target, which includes 6,352 UIs of 1,023 apps from 27 app categories. The annotation process, carried out by two authors in two steps, resulted in 4,999 benign UIs and 1,353 malicious UIs. These malicious UIs contained a total of 1,660 instances of 14 types of dark patterns. The results show that UIGuard can accurately and efficiently detect dark patterns in mobile UIs, with an accuracy of 0.93 for classifying whether a UI contains dark patterns or not. The overall performance for detecting and recognizing the types of dark patterns achieves 0.83/0.82 in precision, 0.82/0.77 in recall and 0.82/0.79 in F1 score for the macro/micro average respectively. Our ablation experiments in Section 7.2.2, which evaluate the impacts of each module, further demonstrates the validity and necessity of each module.</p><p>Finally, we carried out a user study with 58 participants of diverse background, which validates that our tool could significantly help users to gain more knowledge on dark patterns, with the recall rate increasing from 18.5% to 57.8%. The participants highly appreciate UIGuard, and see its great potential in helping users, companies and regulators. We also explore the limitations and future directions of our work. We also note that it is crucial to consider the potential for misuse of the tool rigorously before incorporating them into regulatory practices</p><p>Our contributions can be summarised as:</p><p>• We conducted the first systematic analysis of existing dark pattern taxonomies on mobile apps, integrated existing taxonomies and carried out a characteristic analysis on them, which enlightens the design of our automated dark pattern detection tool. • We developed the first dark pattern detection tool, UIGuard, which adopts computer vision and natural language pattern matching techniques to distill information from user interfaces and identify dark patterns in mobile UIs. • We constructed the first large-scale dark pattern dataset <ref type="bibr" coords="3,282.61,591.69,9.31,7.94" target="#b7">[8]</ref>,</p><p>which contains 4,999 benign UIs and 1,353 malicious UIs of 1,660 instances spanning 1,023 mobile apps. • We conducted a user study with 58 participants, which verifies the usefulness of our tool and the potential educational usage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Our work is related to three topics, i.e., taxonomies of dark patterns, dark pattern detection and UI understanding,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Taxonomies of Dark Pattern</head><p>There are many existing empirical studies on the definition of dark patterns in various platforms, including websites and mobile apps <ref type="bibr" coords="3,353.44,123.41,13.61,7.94" target="#b16">[18,</ref><ref type="bibr" coords="3,369.43,123.41,10.35,7.94" target="#b19">21,</ref><ref type="bibr" coords="3,382.16,123.41,10.20,7.94" target="#b32">34]</ref>, and in different domains, such as game <ref type="bibr" coords="3,546.49,123.41,9.52,7.94" target="#b0">[1]</ref>, social media platforms <ref type="bibr" coords="3,405.42,134.37,13.49,7.94" target="#b22">[24]</ref>, shopping websites <ref type="bibr" coords="3,496.54,134.37,13.49,7.94" target="#b32">[34]</ref>, cookie banner <ref type="bibr" coords="3,333.74,145.33,13.61,7.94" target="#b20">[22,</ref><ref type="bibr" coords="3,350.69,145.33,11.59,7.94" target="#b33">35]</ref> and advertisements <ref type="bibr" coords="3,442.59,145.33,13.49,7.94" target="#b18">[20]</ref>. The general workflow of existing empirical studies start at exploring and collecting user interfaces of desktop or mobile applications, and researchers then summarize the types of dark patterns they identified in the user interfaces.</p><p>In 2010, Harry Brignull <ref type="bibr" coords="3,419.81,200.12,10.68,7.94" target="#b3">[4]</ref> created the dark pattern website, defining dark patterns as "tricks used in websites and apps that make you do things that you didn't mean to, like buying or signing up for something". He also created a twitter account <ref type="bibr" coords="3,508.76,233.00,10.46,7.94" target="#b4">[5]</ref> for people to report and discuss the dark patterns they see in their daily usage. Later, Gray et. al <ref type="bibr" coords="3,379.21,254.92,14.60,7.94" target="#b19">[21]</ref> manually collected 112 artifacts from popular online platforms for a two-month period, and extends Brignull's concept by summarizing the concepts in terms of five strategies, i.e., nagging, obstruction, sneaking, interface interference and forced action, and adding more dark patterns subtypes and examples. Based on these taxonomies, Mathur et al. <ref type="bibr" coords="3,498.86,309.71,13.19,7.94" target="#b32">[34]</ref> conducted a large-scale empirical study on around 11K shopping websites. They collected the data by simulating user shopping behaviours and clustering segments in collected websites. They manually checked the clusters and summarized the dark patterns existing in top shopping websites, and recognised 1,818 instances of dark patterns from around 11% shopping websites. They analysed five characteristics (asymmetric, covert, deceptive, hides information and restrictive) of dark patterns and six cognitive bias (anchoring, bandwagon, default, framing effects, scarcity bias and sunk cost fallacy), and investigated how dark patterns leverage these cognitive bias to affect user behaviours. Different from Mathur et. al's focus on shopping websites, Di Geronimo et. al <ref type="bibr" coords="3,421.89,441.22,14.65,7.94" target="#b16">[18]</ref> evaluated the dark pattern issues on 240 popular Android apps by recording 10-min usage videos for each app. They reused the taxonomies from Gray et. al <ref type="bibr" coords="3,543.35,463.14,14.85,7.94" target="#b19">[21]</ref> and identified some new dark pattern types (16 types and 31 cases). The taxonomies become more finer-grained and more complete when Gunawan et. al <ref type="bibr" coords="3,398.18,496.01,14.76,7.94" target="#b21">[23]</ref> conducted a comparative study of dark patterns across different modalities (mobile and web) in terms of the phase of usage.</p><p>In this paper, we focus on detecting dark patterns in mobile apps but we need a solid and consistent knowledge base of dark pattern as our foundation. Therefore, we carefully examine existing taxonomies, and integrated them into a single and unified one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Dark Pattern Detection</head><p>There are different strategies to detect dark patterns in the wild, including manual exploration, semi-automated clustering based methods and some naive text-based classification methods.</p><p>The detection of dark patterns started with some manual exploration, either by the domain researchers <ref type="bibr" coords="3,483.48,657.66,9.44,7.94" target="#b3">[4,</ref><ref type="bibr" coords="3,495.54,657.66,10.35,7.94" target="#b16">18,</ref><ref type="bibr" coords="3,508.50,657.66,10.35,7.94" target="#b19">21,</ref><ref type="bibr" coords="3,521.47,657.66,11.59,7.94" target="#b21">23]</ref> or the ordinary users <ref type="bibr" coords="3,375.14,668.62,9.44,7.94" target="#b4">[5,</ref><ref type="bibr" coords="3,387.23,668.62,10.21,7.94" target="#b34">36]</ref>. After these researchers or ordinary users detect the potential dark patterns in some applications, they will either share their experience with the related UI screenshot, established a website or published some papers, as a way to help and educate other end-users to avoid being tricked and raise the awareness. However, while the detection is of high accuracy, this method suffers from several drawbacks. First, it is very time-consuming and require people to have enough dark pattern knowledge. Second, as apps keep updating and new apps emerges over the time, such manual detection could not capture all these changes.</p><p>To mitigate the shortages of manual exploration, Mathur et al. <ref type="bibr" coords="4,64.76,164.51,14.85,7.94" target="#b32">[34]</ref> proposed semi-automatic techniques to simulate user behaviours and identify dark patterns in shopping websites. They adopted the clustering technique to ease the detection process by grouping related UI patterns, and then manually evaluated the clusters instead of going through each artifact one by one. As a result, they were able to analyse 53k product pages from 11k shopping websites. While their approach eases the process of manual exploration, it is still time-consuming and hard to apply to new UIs.</p><p>Based on their findings and dataset, some <ref type="bibr" coords="4,223.27,252.18,13.60,7.94" target="#b36">[38,</ref><ref type="bibr" coords="4,239.49,252.18,11.59,7.94" target="#b38">41]</ref> considered using classification techniques to detect these dark patterns in the wild. For example, Tung et al. <ref type="bibr" coords="4,169.22,274.09,14.85,7.94" target="#b38">[41]</ref> leveraged the text data from Mathur et al. <ref type="bibr" coords="4,101.85,285.05,14.60,7.94" target="#b32">[34]</ref> and train a Bernoulli Naive Bayes based classifier for websites, and Soe et al. <ref type="bibr" coords="4,153.18,296.01,14.75,7.94" target="#b36">[38]</ref> developed a gradient boosted tree classifier to identify the dark patterns in cookie banners. However, these techniques are all text-based and limited to certain dark patterns and may easily introduce many false positives as we analysed in Section 4 and Section 7.2.2. They also require a labelled training dataset of dark pattern to train their models.</p><p>Our research focuses on developing a system that can automatically detect dark patterns in mobile applications by incorporating domain knowledge. Our approach offers several advantages over previous work in this area. Firstly, our methods are scalable and can be used to analyze dark patterns across large numbers of apps. Additionally, our system is designed to monitor new UIs as they are released, ensuring that it remains effective even as the landscape of mobile applications evolves. Compared to existing approaches, our tool offers superior coverage and performance, and our modular method allows easy integration of new rules and property identification modules for new types of dark patterns and platforms. By promoting awareness of these unethical design practices and encouraging responsible usage of mobile apps, we hope to prevent the manipulation of users through the implementation of dark patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">UI Understanding</head><p>Understanding the semantic of user interfaces can assist many downstream tasks, such as accessibility testing and enhancement <ref type="bibr" coords="4,283.67,559.03,16.28,7.94;4,53.59,569.99,35.09,7.94">[10-12, 33, 44]</ref>, automated UI testing <ref type="bibr" coords="4,172.05,569.99,13.24,7.94" target="#b39">[42]</ref>, UI design search <ref type="bibr" coords="4,252.48,569.99,9.24,7.94" target="#b8">[9,</ref><ref type="bibr" coords="4,263.96,569.99,10.06,7.94" target="#b28">30]</ref>, conversational agents <ref type="bibr" coords="4,119.75,580.94,14.59,7.94" target="#b27">[29]</ref> and UI code generation <ref type="bibr" coords="4,219.38,580.94,9.23,7.94" target="#b6">[7,</ref><ref type="bibr" coords="4,230.24,580.94,10.05,7.94" target="#b12">13]</ref>. The detection of dark patterns also require the understanding of UI from different perspectives, such as UI element detection and icon understanding.</p><p>While previous research has explored the semantic understanding of UIs for various tasks, no existing work has focused specifically on detecting dark patterns. In our study, we conducted a comprehensive analysis of the characteristics of dark patterns, allowing us to extract richer information from UIs. In addition to the tasks mentioned in previous work, we also considered factors such as contrasting colors to measure visual saliency, analyzing text content, and understanding element status. Our approach is the first to enable automated dark pattern detection in mobile UIs, covering 14 different types of dark patterns and achieving superior performance. Our system, UIGuard, is designed to provide clear explanations of its decision-making process, giving end-users a better understanding of how it operates and the accuracy of its decisions. Through our work, we hope to promote awareness of the prevalence of dark patterns in mobile apps and prevent their unethical usage. By providing a reliable and transparent tool for detecting these patterns, we aim to empower users to make informed decisions about the apps they choose to use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BACKGROUND</head><p>Dark patterns have been well-investigated and defined by many researchers, and many taxonomies from different angles are been proposed. However, these taxonomies lack consistency and are analyzed from different perspectives. In this work, we focus on detecting dark patterns in mobile applications; therefore, we first built a solid and consistent knowledge base to guide the design of our automated dark pattern detection system. We based on three existing taxonomies related to mobile apps <ref type="bibr" coords="4,481.02,290.64,13.61,7.94" target="#b16">[18,</ref><ref type="bibr" coords="4,497.10,290.64,10.35,7.94" target="#b19">21,</ref><ref type="bibr" coords="4,509.91,290.64,10.21,7.94" target="#b21">23]</ref>, and integrated them in Table <ref type="table" coords="4,394.55,301.60,3.01,7.94" target="#tab_0">1</ref>, which shows five strategy categories, types and subtypes of dark patterns, definitions and cases, as our basic taxonomy.. With their provided definitions and related examples, we can better understand and find the root and essence of these dark patterns. We first merged some cases in <ref type="bibr" coords="4,487.43,345.43,13.60,7.94" target="#b16">[18,</ref><ref type="bibr" coords="4,503.34,345.43,11.59,7.94" target="#b21">23]</ref> into one as they are highly similar. For example, <ref type="bibr" coords="4,453.72,356.39,14.72,7.94" target="#b21">[23]</ref> has three separate cases "Optional add-on items are preselected", "Consent notice includes email/SMS subscriptions with a preselected opt-in checkbox" and "checkbox options are preselected". We merged them into one single case " notification/subscriptions or other options are preselected" as the rationale of detecting these cases are the same. We also removed some website related cases as we focus on mobile apps. For example, we removed "the cookie consent is preselected". However, as we include the preselection type, the cookie consent one can actually be considered as a special case of it. Note that our technique could be easily generalised to these websites related cases, but we first focus on mobile UIs to demonstrate the potential of our approach.</p><p>Specifically, Gray et al. summarized five categories of dark patterns as seen in Table <ref type="table" coords="4,398.82,498.86,3.07,7.94" target="#tab_0">1</ref>.</p><p>• Nagging is defined as a repeated app action that interrupts the users' current task and nags them to do something else.</p><p>Examples are like popping up an irrelevant windows for advertisements or ratings. </p><formula xml:id="formula_0" coords="5,432.84,583.84,93.57,5.11">× × ✓ × ✓ × × ×<label>2</label></formula><p>Pay to avoid ads</p><formula xml:id="formula_1" coords="5,432.84,600.15,94.81,5.11">× × ✓ × ✓ × × × 97</formula><p>• Interface Inference privileges some options over others to confuse and hide the information from users. It contains four types. Hidden Information, which diminishes the visibility of information relevant to users. Preselection selected unfavorable options by default. Aesthetic Manipulation leverages visual effects to distract/attract users' attention from/to specific options or actions. It includes four sub-types, Toying with emotion (e.g., countdown offer to make users nervous), False Hierarchy (specific options are more prominent than others), Disguised Ad (Ads looks the same as other content) and Tricked Questions(e.g., double negation questions). • Forced Action forces users to perform some actions to get rewards, unlock features or achieve some tasks. Social Pyramid asks users to leverage their contacts to get rewards, Privacy Zuckering made users to share more information that are not necessary, and Gamification grinds users by repeated tasks to get some rewards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ANALYSIS</head><p>To better lay the foundation for the automated dark pattern detection tool for mobile apps, we conducted a characteristic analysis on these dark patterns at the screen level and element-level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Screen-level perspective</head><p>Based on the unified taxonomy, we classified these dark patterns into two categories: static dark patterns and dynamic dark patterns.</p><p>A static dark pattern means that the dark pattern is specific to a single user interface and does not depend on the context or history of the user's interactions. They can be identified by analyzing the current UI, without the need to consider the user's past actions. This may involve extracting information about the elements on the UI, such as their coordinates, types, colors, text content, and icon semantics, as well as the relationships between these elements. For example, in Figure <ref type="figure" coords="6,140.11,306.97,12.63,7.94">1(a)</ref>, to detect the False Hierarchy dark pattern, we might group the "Close" and "Install" buttons together and compare their text and background colors to determine if the button that favors the app provider's benefit is more prominent. Another example of a static dark pattern is Disguised Ads. As seen in Figure <ref type="figure" coords="6,87.61,361.77,3.14,7.94">1</ref>(b), it requires us to first identify the advertisement block by extracting the text content of the badge and locating the ad info icon, and understand this block is visually similar to other contents. This example also involve a General Aesthetic Manipulation dark pattern as the close button on the top-right of the ad block is very small, and the end-user may easily wrongly click the ad and trigger an unwanted AD page, wasting their time and bringing bad user experience. Understanding the semantic and size of icon is vital for examining this dark pattern. There are also many other dark pattern types fall into this category, such as Intermediate Currency and Forced Continuity. We marked static dark patterns as ✓in the "Is Context-Independent?" column of Table <ref type="table" coords="6,254.91,482.31,3.07,7.94" target="#tab_0">1</ref>.</p><p>In comparison, dynamic dark patterns are context-dependent and require additional information beyond the current user interface in order to be identified. This may include previous UIs, the elements that triggered the current page or animation effect, and the user's intent. For example, in order to verify the existence of a Bait and Switch dark pattern, we need to understand the user's intent and determine whether the current UI meets their expectations. Similarly, Hidden Costs dark patterns can be detected by comparing previous UIs to see if the app revealed these costs beforehand. On the other hand, Roach Motel dark patterns require an analysis of all UIs within the app to determine whether there are options for logging out, unsubscribing, or deleting the account. In our analysis, for dynamic dark patterns, we denoted × in the "Is Context-Independent?" column of Table <ref type="table" coords="6,202.66,635.74,3.07,7.94" target="#tab_0">1</ref>.</p><p>Some dynamic dark patterns can be identified based on the current UI, even though they are context-dependent. One example is Nagging, which involves repeated interruptions of the user's task through pop-up windows that ask the user to rate or upgrade the app, or watch an advertisement (as shown in Figure <ref type="figure" coords="6,250.93,690.53,12.35,7.94">1(a)</ref>). These UIs typically appear without being triggered by the user, and their occurrence suggests a high possibility of a dark pattern. In this case, we can warn users about the potential for dark patterns. Another example is Preselection, which involves determining whether an option is selected by the user or enabled by default(see Figure <ref type="figure" coords="6,542.08,120.67,11.67,7.94">1(c)</ref>). It may not be possible to determine this information even with contextual UIs, so we also consider it an "in-between" type that warrants a warning to users. Users may choose to turn off the detection of this dark pattern type if they do not need the warnings. For this type of dark pattern, it is necessary to understand the element type and status, as well as the text content. In our analysis, we marked these "in-between" dark patterns with "Hybrid" in Table <ref type="table" coords="6,553.51,197.38,3.03,7.94" target="#tab_0">1</ref>.</p><p>There is one type of dark pattern that cannot be identified solely from the UI: Price Comparison Prevention. This type of dark pattern requires interaction with the elements on the UI in order to determine if texts can be copied. To detect this pattern, we need to obtain instrumental information about the copyability of certain elements. In our analysis, we marked this dark patterns with a "/" in Table <ref type="table" coords="6,349.53,274.09,3.07,7.94" target="#tab_0">1</ref>.</p><p>In our paper, we presented the first systematic approach to detecting dark patterns within a single UI. As such, we focus specifically on static and "in-between" dark patterns, which can be identified based on the current UI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Element-level perspective</head><p>Dark patterns can be related to a single element or a combination of elements. To identify single element dark patterns, we can evaluate the properties of each element individually, such as its coordinates, contained text, colors, icon semantics, and element type. For example, Forced Continuity dark patterns can be detected by examining the text content of an element, as in Figure <ref type="figure" coords="6,544.03,411.08,12.44,7.94">1(d)</ref>. It contains a text element saying "7 days free, then $84.00/year", which automatically opt-in users after the free trail ends. The size and color of the text may also be relevant for identifying Hidden Information dark patterns.</p><p>Multiple element dark patterns can be further divided into two categories. One type requires analyzing the attributes of several elements to confirm the presence of a dark pattern, such as the coexistence of virtual and real currency in Figure <ref type="figure" coords="6,485.42,498.75,3.01,7.94">1</ref>(e), which indicates the presence of Intermediate Currency issues. The other type requires a higher level of information, such as the relationship between UI elements. For instance, Figure <ref type="figure" coords="6,468.78,531.63,3.31,7.94">1</ref>(a) shows an example of a False Hierarchy dark pattern, which requires grouping related buttons (such as the accept and cancel buttons) and comparing their visual saliency to determine if one option is privileged. This process involves understanding the relative position and characteristics of the elements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Characteristic Identification</head><p>Our analysis of dark patterns has identified six key characteristics that are necessary for detecting these patterns within user interfaces. These include element meta information such as coordinates and element types, which can help identify the shape and relationships of elements; the text content of elements, which is useful for determining the semantics of certain patterns; the status of certain types of elements, such as checkboxes; icon semantics, which can sometimes be used to identify the overall meaning of a UI page or block; background and text colors, which are relevant for visual patterns like False Hierarchy; and higher-level information such as the grouping of similar elements and the relationships between UI elements, which is important for multiple element patterns. By considering all of these factors, we can develop a comprehensive and effective strategy for detecting dark patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">APPROACH</head><p>Figure <ref type="figure" coords="7,79.15,395.83,4.14,7.94" target="#fig_1">2</ref> shows the flowchart of our system, UIGuard, which takes a UI screenshot as input and reports the types and locations of recognized dark pattern instance in the screen. Generally, UIGuard consists of two parts, namely Property Extraction and Knowledge-Driven Dark Pattern Checker. The Property Extraction module is responsible to intelligently extract essential properties from the UI screenshot, and the Knowledge-Driven Dark Pattern Checker powered by the knowledge we distilled from the integrated taxonomy, can conduct analysis on these information and examine the types and location of dark pattern instances in the current UI. As an example in Figure <ref type="figure" coords="7,121.41,505.42,3.06,7.94" target="#fig_1">2</ref>, the user inputs a UI screenshot, and UIGuard identifies two dark pattern instances within that UI, highlights them with red boxes and provides detailed explanations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">UI Element Property Extraction</head><p>5.1.1 UI Element Detection. The first step of our system is to recognize UI elements in a given UI screenshot. In our work, we use Faster-RCNN <ref type="bibr" coords="7,103.70,585.28,14.60,7.94" target="#b35">[37]</ref> to localize and recognize UI elements in UIs, and then merges the detection in this step with the results from Text Content Extraction module to form more accurate detections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Text Content Extraction.</head><p>To extract text content in UI, we adopt a mature OCR model, PaddleOCR <ref type="bibr" coords="7,200.73,635.74,13.31,7.94">[17]</ref>, which supports over 80 language, and achieves high accuracy. We use their pretrained model for English language, and obtain the bounding boxes of text lines and text contents in the UI screenshot. Instead of inputting the detected UI element from the previous step one by one, we directly feed the whole UI screenshot to the OCR engine to save computing time and to retrieve miss-detected text elements. After we obtain the text lines (coodinates and text contents), we merges these text detections with the detections from FasterRCNN.</p><p>Merging with FasterRCNN detections: UI elements normally do not overlap with each other except one case when an ImageView element is served as a background of other elements. Therefore, we first sort the UI elements by their size in ascending order. For each UI element, we calculate the interaction over union (IoU -𝑖𝑛𝑡𝑒𝑟𝑎𝑐𝑡𝑖𝑜𝑛𝑎𝑟𝑒𝑎/𝑢𝑛𝑖𝑜𝑖𝑛𝑎𝑟𝑒𝑎) values with each detected text line. If the IoU value over a predefined threshold, we consider it as a match, and add the corresponding text content to the UI element. As one UI element may contain several text lines, we allow one UI element to match with several text lines. However, one text line can only be matched with one UI element. After we iterate all UI element, the unmatched text lines will be considered as TextView elements that is missed by our UI Element Detection module.</p><p>In addition, we notice that the bounding boxes of text elements from UI Element Detection are not very precise while OCR engines can extract highly precise bounding boxes of the text lines. Leveraging this feature, we further refine the detected bounding boxes by replace the bounding boxes of UI text elements with the merged bounding box of the matched text lines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Icon Semantic Understanding.</head><p>Understanding the semantics of icons are important to recognise the existence of dark patterns. For example, star icons will appear in the Nagging patterns, and the triangle Ad information icons will always appear in ad-related dark patterns. Hence, for UI elements detected as ImageView or Im-ageButton elements, we trained a ResNet-18 model <ref type="bibr" coords="7,505.12,595.94,14.66,7.94" target="#b23">[25]</ref> to classify the icons into finer semantic categories.</p><p>In addition, we notice that some advertisement-specific icons, such as and , are normally not included in the view hierarchy, which means that our localisation model is "trained to miss them". However, these icons are necessary to detect ad-related dark patterns like Disguised Ads. Actually, most advertisements will have the exact same ad-specific icons. The reason is that many advertisement network companies 2 , such as Google Inc., Facebook, Microsoft  and AT&amp;T, join the adChoices 3 self-regulatory program, which aims to establish and enforce responsible practices for online behavioural advertising and give consumers enhanced transparency and control. By joining this program, their advertisement is required to provide the adchoice icon, i.e., a triangle info icon , to indicate that the ads are recommended based on their online behaviours. Therefore, we adopt the template matching technique <ref type="bibr" coords="8,207.86,301.83,10.43,7.94" target="#b5">[6]</ref> to find these missing ad-specific icons. We then obtain the foreground color by computing the euclidean distance between the rest color with the background color, and find the one with farthest distance. The rationale inside is that we assume the designers will make the background and foreground colors having highest contrast ratio so that end-users could clearly separate the foreground content from the background.</p><p>5.1.6 UI Element Grouping. Apart from the properties of UI elements, their relationships are also important, especially for False Hierarchy tricks, which need to compare the visual saliency among related UI elements. Normally, the app providers or designers will manipulate the color and size of buttons or text elements to highlight or dampen some options. Therefore, we use density-based spatial clustering of applications with noise (DBSCAN) algorithm <ref type="bibr" coords="8,278.50,561.65,13.22,7.94" target="#b17">[19]</ref>, which is a density-based clustering algorithm, to perform elements grouping, and only consider text elements and buttons. The advantages of this clustering algorithm is that we do not need to pre-define the number of clustering. The basic idea is that based on the element features, it can group UI elements in a high-density region, while marking those UI elements in low-density region as outliers. To define whether an area is of high-density or of low-density, we need to define a distance function and two thresholds, i.e., a distance threshold 𝛼 and a density threshold 𝛽. We consider four features, including element types, size (width and height), coordinates and their text content, 3 https://youradchoices.com/ based on which we compute the distance between any two elements, to group relevant UI elements. After relevant elements are grouped together, we examine their color to see if they have big visual differences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Knowledge-Driven Dark Pattern Checker</head><p>To effectively detect dark patterns in mobile UIs, we utilized our integrated taxonomy of dark patterns and drew upon existing literature for examples and datasets. We specifically analyzed the partially released videos from Di Geronimo et al. <ref type="bibr" coords="8,466.45,337.76,13.40,7.94" target="#b16">[18]</ref>, UXP2 gallery<ref type="foot" coords="8,534.30,335.62,3.38,6.44" target="#foot_0">4</ref> from Gray et al. <ref type="bibr" coords="8,357.12,348.72,14.59,7.94" target="#b19">[21]</ref> and the Deceptive Design collection <ref type="bibr" coords="8,504.85,348.72,10.42,7.94" target="#b3">[4]</ref> from Harry Brignull. From this, we distilled key patterns that allow for automated detection of dark patterns in UIs. We show partial patterns in Table <ref type="table" coords="8,351.40,381.60,3.13,7.94" target="#tab_1">2</ref>, and a complete one can be seen in <ref type="bibr" coords="8,495.29,381.60,9.52,7.94" target="#b7">[8]</ref>. For instance, the Pay to Avoid Ads pattern can be identified by searching for phrases like "remove ads" or "disable advertising", while the FA -Watch to Unlock Features pattern can be detected by searching for phrases like "watch ad". In addition to text content, the semantic meanings of icons can aid in the identification of many AD-related dark patterns, as the presence of certain icons or AD badges often indicates the presence of advertising. Therefore, combining the existence of ad indicators and the location of that block, we can recognise NG -Pop up AD tricks.</p><p>Some patterns also require an understanding of the status of UI elements. The Preselection pattern, in which the option favored by the app provider is selected by default, can be identified by searching for notification, privacy policy, and usage data-related text, and then checking if the corresponding checkbox is preselected. The False Hierarchy pattern can be detected by comparing the visual differences between a group of relevant elements, and evaluating the content of buttons.</p><p>Upon obtaining UI elements from previous steps, our Knowledge-Driven Dark Pattern Checker uses the knowledge to scan the UI elements in a given UI and identify any instances of these patterns. As seen in Figure <ref type="figure" coords="8,380.96,611.74,3.01,7.94" target="#fig_1">2</ref>, UIGuard was able to accurately detect two dark patterns in the example UI: nagging ads and false hierarchy. This demonstrates the power and effectiveness of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DATASETS AND IMPLEMENTATION</head><p>In this section, we present the datasets used to train and evaluate our proposed system, UIGuard. First, we discuss the deep learning datasets used to train and test our deep learning modules. Then, we introduce the dark pattern dataset that we annotated and used to evaluate the overall performance of UIGuard in detecting dark patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Deep learning datasets and model training</head><p>We used the Rico dataset <ref type="bibr" coords="9,151.49,156.41,14.85,7.94" target="#b15">[16]</ref> as the basis for our experimental dataset. While it is collected in 2016, it remains the most widelyused and high-quality dataset for Android UI designs <ref type="bibr" coords="9,240.71,178.32,13.23,7.94" target="#b14">[15]</ref>. It consists of 66,261 GUIs from 9,384 free Android applications and includes view hierarchy information for each UI, such as layout information and element attributes (e.g., element type, content, coordinates). While it lacks semantic descriptions, Liu et al. <ref type="bibr" coords="9,225.41,222.16,14.82,7.94" target="#b30">[32]</ref> added explicit names to UI elements based on predefined heuristics. They identified 25 UI component categories, such as Page Indicators, CARDs, Checkboxes, and Icons, through an interactive open coding process. These two datasets allow us to train our models for element localization, icon recognition, and status recognition with minor efforts. Despite the absence of new datasets, they mostly built upon and enhanced the Rico dataset from other aspects, such as captioning UIs and removing noises <ref type="bibr" coords="9,146.20,309.83,13.50,7.94" target="#b26">[28,</ref><ref type="bibr" coords="9,161.94,309.83,10.13,7.94" target="#b30">32]</ref>.</p><p>6.1.1 Element Localisation Dataset. We utilized the Rico dataset to train our element localization model (i.e., FasterRCNN) as it provides bounding boxes and types for contained GUI elements. We selected 15 types of Android GUI elements (e.g., Button, ImageView, TextView) as our target objects <ref type="foot" coords="9,170.04,368.82,3.38,6.44" target="#foot_1">5</ref> . We filtered out any UIs that do not contain these elements, resulting in a dataset of 50,524 UIs. We splitted the dataset into training, validation, and testing sets (80:10:10) and ensured that UIs from one app are only included in a single split to prevent potential data leakage.</p><p>We initialised the model parameters using the pretrained model of COCO dataset <ref type="bibr" coords="9,118.41,436.72,14.78,7.94" target="#b29">[31]</ref> and finetuned all parameters using the processed Rico dataset. The model was trained on a Nvidia 1080 GPU for 160 epochs with a batch size of 8, an initial training rate of 0.001, and weights are updated by an Adam optimizer <ref type="bibr" coords="9,232.44,469.60,15.04,7.94" target="#b24">[26]</ref>. To remove duplicate detection, we applied non-max suppression to choose the most confident detection from the overlapping detection. We set the interaction over union (IoU) threshold to 0.5, and found the best confidence threshold using the validation dataset, which is 0.65.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Icon Recognition Dataset.</head><p>To train our icon recognition model, we used the annotations from Liu et. al <ref type="bibr" coords="9,194.09,541.70,13.22,7.94" target="#b30">[32]</ref>. In total, they identified 135 icon types and labelled 73,449 icons. We removed duplicate icons by comparing the color histogram of icons, and considered icon types that appear at least 200 times in our training dataset. Note that icon types that appear less than 200 times are considered as an additional Other type. After that, we noticed some icons are noisy data or misclassified. Therefore, we manually examined all data and removed these wrong data. As a result, we had 81 target icon types, such as star and close icons, and one other type <ref type="foot" coords="9,268.37,627.22,3.38,6.44" target="#foot_2">6</ref> . Note that while for now, we only require the identification of several icons, we still train a general icon recognition model for future extension. In addition, as the bounding boxes of detections may not be as accurate as the ground truth bounding boxes, we performed some data augmentation techniques to enhance the robustness. Specifically, we randomly added some noises to the groundtruth bounding boxes to simulate the potential inaccurate detections. All icons were splited into training, validation and testing dataset using the same method as the element localization dataset. This means that if an icon's UI is in the training dataset, the icon will also be placed in the training dataset.</p><p>We rescaled the icon to a input size of 224 × 224. We initialised the ResNet-18 model parameter using the pretrained model on ImageNet dataset and finetuned it on a Nvidia 1080 GPU for 100 epochs with a batch size of 128 and an initial training rate of 0.001. The model weights were updated by stochastic gradient descent optimizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.3">Status Recognition Dataset.</head><p>For our status recognition model, we extracted UI elements with a type of "Checkbox", "Switch" or "Toggle Button" from the semantic dataset. After we obtained the data, we then manually annotated the status of them as the Rico semantic dataset does not contain such information. Same as the icon classification model, we also performed data augmentation by randomly adding some noises to the bounding boxes. As a result, we obtained 1,845 checked elements and 1,383 unchecked elements. As some non-checkbox elements may be wrongly detected as checkboxes, we trained a 3-class model to deal with the problem. Therefore, we randomly extracted 3,093 other elements as a negative class for the model to mitigate the potential classification errors inherited from element localization step. This dataset was splited following the same strategy as in previous datasets.</p><p>The model was trained by inputting a RGB image of input size 224 × 224 on NVIDIA 1080 GPU. The optimizer was SGD and the initial learning rate is 0.001. We used a batch size of 128 and trained the model for 100 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">The Dark Pattern Dataset</head><p>To evaluate the overall performance of our proposed system, we need an annotated dataset of dark patterns in UIs that specifies the types and positions of these patterns. However, existing dark pattern datasets are small-scale, incomplete, or only partially labeled. Harry Brignull's Deceptive Design Website provides one example of each included dark pattern type, while Gray et al.'s UXP2 gallery <ref type="bibr" coords="9,367.95,525.41,14.62,7.94" target="#b19">[21]</ref> contains 112 artifacts collected from popular online platforms, including UIs from both websites and mobile apps. These datasets are small and do not focus on mobile applications, potentially leading to biased results in evaluation. Di Geronimo et al. <ref type="bibr" coords="9,328.72,569.24,14.79,7.94" target="#b16">[18]</ref> collected a 10-minute usage video for each of 240 popular Android mobile apps and identified 1,787 dark pattern instances, but only released 15 videos and we were unable to obtain the remaining data from them <ref type="foot" coords="9,411.48,599.97,3.38,6.44" target="#foot_3">7</ref> . The released videos only contain 105 instances, making them incomplete and small-scale. Additionally, all existing datasets only include labels for the corresponding dark pattern types, but do not specify the location of the patterns within the UIs.</p><p>To overcome these limitations, we decided to manually annotate our Rico testset. We chose Rico because it is a relatively comprehensive dataset of UIs collected from a diverse range of apps in different Annotating the existence and types of dark patterns: To begin the annotation process, two of the authors studied the integrated taxonomies in Section 4 and examined examples provided by Gray et al. <ref type="bibr" coords="10,64.12,340.55,14.59,7.94" target="#b19">[21]</ref> to gain a shared understanding of dark patterns. To validate this understanding, we used the 15 videos from Di Geronimo et al. <ref type="bibr" coords="10,64.19,362.46,14.59,7.94" target="#b16">[18]</ref> and independently annotated the existence of dark patterns in each video, discussing and comparing our annotations to those of Di Geronimo et al. Next, each author independently annotated all 6,352 UIs in the Rico testset for the presence of dark patterns and recorded the types of any identified patterns. After completing this step, we discussed our annotations and resolved any discrepancies. This step resulted in the identification of 1,660 instances of 14 dark pattern types from 1,353 UIs and 4,999 benign UIs. As we only identified 14 types of dark patterns in the Rico test dataset, our experiments focused on these types.</p><p>Annotating the locations of dark patterns: To ensure consistency in the annotation process, we randomly selected at most 5 UIs from each dark pattern type to serve as standard labeling examples. Based on our analysis in Section 4, two authors independently annotated the locations of these UIs using the open-source tool LabelImg <ref type="bibr" coords="10,279.45,515.89,14.60,7.94">[40]</ref> and discussed the annotations to form standards for labeling positions. These standards included labeling all relevant UI elements and their element types that may indicate the presence of dark patterns, as well as annotating a container bounding box to encompass the span of the dark pattern. After labeling the sample UIs, we achieved agreement on different annotations and resolved conflicts. The two authors then independently labeled the remaining UIs and resolved any remaining conflicts upon completion. "</p><p>We use Cohen's Kappa to evaluate the inter-rater agreements. The agreement for DP presence is 0.97, and for identifying specific types, it is 0.89. These results indicate high agreement between annotators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">ACCURACY EVALUATION</head><p>In this section, we aimed to evaluate the accuracy of our proposed system to answer two research questions: How accurate is each DL module in our proposed system (RQ1)? How accurate is our proposed system in detecting dark patterns (RQ2)?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">RQ1: Accuracy of each DL module</head><p>To evaluate the accuracy of each DL modules in our proposed techniques, we used the corresponding testing datasets as stated in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.1">Element localisation and classification. Baseline:</head><p>We considered four baselines: UIED, FRCNN (nontext), FRCNN (nontext) with PaddleOCR results and our ablation model (FRCNN-all). UIED <ref type="bibr" coords="10,543.57,361.77,14.63,7.94" target="#b12">[13]</ref> is a state-of-the-art UI element detection technique that combines deep learning and traditional methods to achieve precise localization of UI elements. For text detection, it uses the EAST <ref type="bibr" coords="10,543.35,394.64,14.85,7.94" target="#b42">[45]</ref> model and for non-text element detection, it employs a novel oldfashioned method with a top-down coarse-to-fine strategy based on the unique characteristics of UIs and UI elements, along with a CNN classifier to classify the elements. However, our task does not require such high precision in element detection, as end-users will examine the results manually. Therefore, we also considered the Faster RCNN model trained to detect only non-text UI elements, referred to as FRCNN (nontext) for brevity, and used PaddleOCR to detect text elements and merged them with the non-text element detections, called FRCNN (nontext) + PaddleOCR. Lastly, we compared an ablation version of our proposed technique, FRCNN (all), to evaluate the performance of the FasterRCNN-only method.</p><p>Metric: We used precision, recall and F1-score to evaluate the models. Precision is defined as 𝑇 𝑃/(𝑇 𝑃+𝐹 𝑃), recall is 𝑇 𝑃/(𝑇 𝑃+𝐹 𝑁 ) and F1 score is computed by (2×𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛×𝑅𝑒𝑐𝑎𝑙𝑙)/(𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛+𝑅𝑒𝑐𝑎𝑙𝑙). A True Positive (TP) refers to a detected element matching the ground truth box and its type is right. A False Positive (FP) refers to a detected element that does either not match a ground-truth object or fail to predict the right element type. TP is determined based on the interaction over union (IoU) value of two boxes. The IoU value is calculated by 𝑖𝑛𝑡𝑒𝑟𝑎𝑐𝑡𝑖𝑜𝑛_𝑎𝑟𝑒𝑎/𝑢𝑛𝑖𝑜𝑛_𝑎𝑟𝑒𝑎 of the two boxes. If a detected instance has a IoU with the groudtruth box over a preset threshold, and is predicted as a right class, we consider it as a match. We used the regular IoU threshold 0.5 for our task <ref type="bibr" coords="10,504.88,657.66,13.56,7.94" target="#b35">[37,</ref><ref type="bibr" coords="10,520.67,657.66,10.17,7.94" target="#b39">42]</ref>, as the final results would be examined to a human. In addition, we note that OCR engine (both EAST and PaddleOCR) will detect precise bounding box for text elements that the groundtruth bounding box may contain it and have a low IoU value. Therefore, if a text element is contained by a groundtruth text box, we also consider it as a match.</p><p>Results: Table <ref type="table" coords="11,117.98,109.71,4.09,7.94" target="#tab_2">3</ref> shows the performance of all models. Our model performs better than most baselines in most metrics, achieving 0.49 in F1 in non-text element detection, 0.62 in F1 in text element detection and 0.58 in F1 for all elements. Under a loose requirement of precision, deep learning models are able to achieve better performance on non-text element detection than UIED, especially in UIs with noisy background. For text-element, PaddleOCR performs similar to EAST used in UIED, both reaching around 0.50 in F1. After merging deep learning results with PaddleOCR, the performance of both FRCNN(nontext) in detecting non-text elements slightly degrades. Surprisingly, as we can see from the table, FRCNN(all) can achieve better performance than FRCNN(nontext)+PaddleOCR in detecting non-text elements, around 0.06 higher in F1 score. We investigated the reasons and found that OCR engines can detect texts on advertisements, and the groundtruth annotations normally do not contain the advertisement components and elements, which degrades the OCR engines. Our final model, UIGuard confirms the usefulness of the OCR engine, achieving 0.61 in precision, 0.55 in recall and 0.58 in F1 in detecting all elements.</p><p>7.1.2 Icon Semantic Understanding. For icon recognition module, we did not setup a baseline as this kind of method is mature. Therefore, we adopted the accuracy metric to evaluate our performance. Our model achieves 0.97 accuracy in testing dataset, and 0.95 in detections from our system. Details of the classification results for each icon type can be seen in supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.3">Status Recognition.</head><p>Similarly, we also used accuracy metric to evaluate of our status recognition module. Our model achieves 0.99 accuracy in testing dataset and 0.94 accuracy in detections from our element localisation model. Details of the classification results for each type can be seen in supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">RQ2: Accuracy in detecting dark patterns</head><p>We used the dark pattern dataset to evaluate the performance in detecting dark patterns. Overall, in the dark pattern dataset, UIGuard detects 1,568 instances of dark patterns in 1,304 UIs. In terms of the accuracy of classifying whether a UI contains malicious design or not, our system achieves 0.93 accuracy. Of the 4,999 benign UIs, only 192 (3.8%) are wrongly detected as malicious UIs; Of 1,353 malicious UIs, 82.2% are correctly detected as UIs with dark patterns. <ref type="table" coords="11,179.73,559.03,4.20,7.94" target="#tab_3">4</ref> shows the detailed results for each dark pattern type, and overall results for each strategy and for all detections. We considered two weighted average metric:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.1">Overall Performance. Table</head><p>(1) macro average with equal weights for all categories, and (2) micro average with weights based on category object counts. Overall, UIGuard achieves macro/micro average precisions of 0.83/0.82, recall of 0.82/0.77, and F1 score of 0.82/0.79, respectively, which demonstrates that our system can effectively and accurately detect dark patterns in mobile UIs. To avoid the potential bias from cases with limited examples, we also reported the results after excluding these cases (n&lt;10) and the macro/micro results are 0.82/0.84 for precision, 0.76/0.77 for recall, and 0.79/0.80 for F1. In terms of strategies, UIGuard also achieves around or over 0.8 for all metrics in Sneaking, Interface Inference and Forced Action strategies. In terms of the detailed dark pattern types, UIGuard reaches around or over 0.80 in F1 score in 7 types. Our system performs well in detecting certain types of dark patterns, such as NG-Pop-up AD, II-Preselection -Checkbox selected, II-AM-General Types -small close buttons, which shows the efficiency of our proposed system. However, UIGuard performs less well in detecting other types, such as NG-Nag to rate and NG-Nag to upgrade. While it remains good recall rates, it detects many false positives, which leads to a low performance in F1 scores. We manually checked the reason of false positives in these two types. As seen in Figure <ref type="figure" coords="11,371.88,602.86,8.30,7.94" target="#fig_6">3a</ref> and Figure <ref type="figure" coords="11,423.84,602.86,7.07,7.94" target="#fig_6">3b</ref>, our tool detects in-context rating and upgrade. However, based on the definition of these two dark pattern types, it has to appear as a pop-up window. Some additional techniques such as detecting the existence of pop-up windows can mitigate these kinds of false positives. In addition, the definition of the Disguised AD dark pattern is ambiguous, leading to poor performance in detecting it. For example, as seen in Figure <ref type="figure" coords="11,535.28,668.62,6.67,7.94" target="#fig_6">3c</ref>, we can see UIGuard detect the potential existence of dark patterns as it locates the ad icon. However, as this part of UI is clearly different from other parts as the content is quite different with a big picture,   we do not consider it as a disguised ad. Similar situation applies to Figure <ref type="figure" coords="12,79.94,445.65,7.26,7.94" target="#fig_6">3d</ref>. As people may have different understandings of what is a dark pattern, a more personalized detector that allows users to control which dark patterns they want to be alerted to may be beneficial in addressing this issue. "</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.2">Ablation Experiments.</head><p>We then conducted ablation experiments to understand the usefulness and necessity of each module in our system. To do this, we considered the text-only model as the Base Model (+Text-only), and gradually added +Icon Semantic , +Template Matching, +Status Recognition and +Color &amp; Grouping modules one by one. We calculated the same evaluation metrics for each ablation and noted the trend after the addition of each module.</p><p>As we can see in Table <ref type="table" coords="12,147.47,591.90,3.01,7.94" target="#tab_4">5</ref>, Base Model performs well in detecting Sneaking and Forced Action dark patterns because these types of dark patterns rely heavily on text content. In comparison, Base Model does not perform well in detecting Nagging and Interface Inference types, as pure text information is not enough for these types of dark patterns. After adding the Icon Semantic module, we can see an increase in performance in the Nagging type from 0.09 to 0.28 in F1. The Template Matching boosted performance further, as many dark patterns are advertisement related, and identifying the existence of advertisement can be very useful in detecting adrelated dark patterns. This also means the basic text content and icon semantic modules are insufficient for these types, and our template matching could mitigate errors inherited from the element detection module. The Status Recognition module specifically deals with checkbox-related dark patterns. While maintaining the precision, its recall goes higher from 0.75 to 0.80 for the overall performance. Finally, after adding the Color &amp; Grouping module, our system gains the ability to recognise dark patterns related to element relationships and color. Our final performance achieves 0.83 in precision, 0.82 in recall and 0.82 in F1 score. The ablation experiments show the usefulness and necessity of each module in our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">USEFULNESS</head><p>Existing research shows that indicating user the existence of dark patterns can mitigate the potential impacts of them <ref type="bibr" coords="12,514.51,635.74,13.49,7.94" target="#b16">[18]</ref>. In this section, we conducted a user study to measure participants' understanding and perception of dark patterns and the usefulness of UIGuard in detecting and explaining dark patterns. We aimed to answer a third research question, i.e., is the proposed system useful from the perspective of ordinary users and how they perceive the existence of dark patterns?</p><p>User Study Step 1: Demographic Questions</p><p>Step 2: Evaluate UIs based on their own understanding of DP</p><p>Step 3: Compare their results with UIGuard's results</p><p>Step 4: Evaluate another set of similar UIs</p><p>Step 5: Overall Feedback To ensure a diverse range of participants, we recruited individuals of various ages, genders, and levels of education through social media platforms such as LinkedIn, Twitter, and alumni networks. All participants were required to be at least 18 years old and have experience using mobile phones. There were no incentives or reimbursements offered for participation in the study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Before Education Education After Education</head><p>8.1.2 Procedure. The study was conducted in the form of an online survey 8 , which consists of five steps (see Figure <ref type="figure" coords="13,242.79,310.37,3.00,7.94" target="#fig_8">4</ref>). Before the survey starts, we gave a short introduction of our user study and asked for the consent from the participants to attend this study. After obtaining their consent, the first step collected demographic information from participants, including their gender, age range, level of education, career, frequency of mobile app usage and their knowledge of dark pattern.</p><p>In the second step, participants were asked to evaluate the existence of malicious UI designs in a set of 𝑛 UIs. They were provided with the definition of a dark pattern (i.e., "A malicious user interface is defined as a user interface using tricks that make you do things that you didn't mean to"), and were asked to interpret it themselves. If they identified any malicious designs, they were asked to click on the location of the dark pattern on the UI, provide a reason for their identification, and rate the severity and difficulty of the issue on a 5-point Likert scale. Reasons for their ratings were optional to control the length of the study. The severity is defined as how much impact it will have if they are tricked. The hardness for recognising the issue is defined as how hard it is to recognize the issue (e.g., how much time it will cost, how carefully the end-user should be).</p><p>In the third step, participants were shown the detection results from the tool for each of the k UIs from step 2 (similar to the output in Figure <ref type="figure" coords="13,90.01,551.46,3.00,7.94" target="#fig_1">2</ref>), and were asked to evaluate whether they identified the same issues as the tool. If not, they were asked to provide a reason for their discrepancy, rate the severity and difficulty of the issue, and provide their reasons (optional). The participants' own evaluations of the UIs were also shown to them so that they could recall what they recognise in Step 2. The reason that we ask the participants to compare their detections with the detection results from our tool, rather than the ground-truth results, is twofold. First, we want to understand the usefulness of our proposed tool by learning the reasons why people could not identify the issues by themselves. Second, we want to evaluate whether the participants can learn to critique the results from our system and learn something from imperfect AI, as AI systems are not always 8 An example procedure can be seen in <ref type="bibr" coords="13,164.98,702.79,8.21,6.18" target="#b7">[8]</ref> 100% correct. This practice can be beneficial during the process of comparing their results with our detections.</p><p>Step 4 was a practice session, in which we asked the participants to evaluate another 𝑚 UIs with similar issues as the UIs in Step 2 and asked the same questions as in Step 2. This step aimed to examine whether participants have learned the concept from the short session in step 3, and to understand the educational effect of the tool.</p><p>In the final step, participants were asked to provide overall feedback about the usefulness, presentation, advantages, and disadvantages of the tool, and to rate their knowledge of dark patterns again. They were also given the opportunity to provide any other comments. A brief version of the survey is provided in the supplementary materials. Note that our study passes the ethical assessment from our organisation, and all participants gave their consent on using their answers in our paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.3">Task Selection.</head><p>For our user study, the participants was required to evaluate 2 * 𝑛 + 𝑚 user interfaces. To control the time consumed by each study (around one hour), we sampled around 20 UIs for Steps 2 and 3, as well as for Step 4. In these sampled UIs, we aimed to include both correct and wrong detections from our tool in order to effectively test its usefulness. Additionally, we also included some benign UIs to evaluate the participants' understanding of ethical and unethical user interfaces. Given the total of 14 target dark pattern types, we sampled 1-2 instances per type. As such, we adopted a probability-based sampling strategy to ensure a fair representation of different levels of performance for each type of dark pattern from our tool. To align the UIs in Step 4 with Steps 2 and 3, we sampled UIs that were not included in Step 2 and had a similar issue. To minimize bias and ensure representative, we ensured that each UI was sampled from different apps. As a result, we sampled 23 UIs for Step 2 and 3, and 20 UIs<ref type="foot" coords="13,487.85,558.34,3.38,6.44" target="#foot_4">9</ref> for Step 4.</p><p>To conduct the user study, we used the Qualtrics platform 10 . Our pilot study indicated that the full survey (in which users need to evaluate in total 66 UIs) would take approximately one hour to complete. As we did not offer any financial incentives to participants, we decided to split the survey into four 15-minute segments. For each segment, we ensured that the UIs used in Step 4 were aligned with those used in Step 2. However, we also provided the option for participants to complete the full one-hour version of the survey if they preferred. The main distinction between the full survey and the 15-minute versions lies in the number of UIs that participants  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Results</head><p>8.2.1 Demographic Information. In total, 58 people participated in our user study. Of these, 24 the full 1-hour version and 34 completed a 15-min questionnaire. As seen in Figure <ref type="figure" coords="14,246.92,350.65,3.05,7.94" target="#fig_9">5</ref>, the participants were diverse in terms of gender, age, occupation, education level, and mobile phone usage. 24 of participants were female, 33 were male and 1 preferred not to say. The ages of the participants ranged from 18 to over 65, with 12 in the 18-24 age range, 39 in the 25-34 age range, 4 in the 35-44 age range, 2 in the 45-54 age range, and 1 over 65. Of these participants, 10 were students, 6 were software engineer or developers, 16 were researchers from different domains (e.g., software engineering, Human Computer Interaction (HCI), UI/UX, social, finance and AI), <ref type="bibr" coords="14,192.30,449.28,8.42,7.94" target="#b17">19</ref> preferred not to say. The rest were freelance, construction worker, data analyst, export operator, or accountant. The education levels of the participants varied, with 2 having a degree below a bachelor's degree, 14 having a bachelor's degree, 27 having a master's degree, and 15 having a doctoral degree. In terms of mobile phone usage, 1 participant used their phone less than 5 times per day, 10 used it 5-10 times per day, and 47 used it more than 10 times per day. In terms of knowledge of dark patterns, 20 participants were unaware of dark patterns, 19 had a little knowledge, 15 had some knowledge, 3 knew very well, and 1 considered themselves an expert. These results show that our participants were diverse. A detailed demographic distribution of each survey can be seen in Section A.</p><p>8.2.2 User perception of dark pattern. When evaluating the presence of malicious UI designs in step 2, participants identified a total of 474 instances. Upon manual review, we found that 163 (34.4%) of these instances matched with our target dark patterns, resulting in a recall rate of 0.185. In detail, for each dark pattern type, we showed the number of correct detection and recall in Table <ref type="table" coords="14,271.97,652.67,3.07,7.94" target="#tab_6">6</ref>. <ref type="foot" coords="14,280.35,650.52,6.76,6.44" target="#foot_5">11</ref>Among all dark pattern types, participants performed the best in detecting II-AM-Disguised AD, achieving 0.625 recall rate. From their provided reasons, we found that most participants can sense that the advertisements which mixed in the main content are malicious design, but only a few of them can clearly articulate the exact reasons. However, for other types, recall rates for other types were lower than 0.3, with particularly low rates for NG-Pop-up to upgrade, II-Preselection-No Checkbox, FA-Social Pyramid and Pay to avoid AD (all below 10%). The main reason for these low recall rates was lack of knowledge or awareness of these patterns, with some participants stating that these patterns are common and not a big deal. Additionally, some participants believed that FA-Social Pyramid was not a malicious design because users have the option to invite a friend or not.</p><p>We then reported the average severity and hardness ratings for each dark pattern type and the overall results in Table <ref type="table" coords="14,518.58,380.89,3.08,7.94" target="#tab_3">4</ref>. For both scores, we reported two average scores in terms of whether the participants successfully identified the malicious design in Step2 (TP) or were showed by our tool in Step 3 (FN). Generally, for most dark pattern types, participants had similar severity scores. For some dark pattern types, including NG-Pop up to upgrade, SN-Forced Continuity, II-AM-False Hierarchy, FA-General Types-Pay to avoid Ad, the average severity scores for TP were much lower than the scores for FN. This result indicates that participants who identified the dark patterns on their own considered them to be more severe than those shown to them by our tool. However, both groups agreed that dark patterns can cause frustration, waste time, and lead to financial loss. There was more variance in the ratings of difficulty. For NG-Pop up AD, II-AM-Disguised AD, II-AM General Types-small close buttons and FA-General Types-watch AD to unlock feature, FA-General Types-Pay to avoid Ad, participants who identified these instances independently perceived that it was easier to spot than other participants who can not. The only reverse pattern lies in SN-Forced Continuity, which was perceived as more difficult by those who were shown it by the tool. 8.2.3 User capabilities to learn from imperfect AI. In step 3 of our user study, we included 5 instances where our tool provided incorrect detections of dark patterns. We wanted to see if participants would be able to recognize these incorrect detections and learn from the correct ones. Out of the 92 reasons provided by participants for why they did not spot the wrong detections, only 12 (13%) recognized that the AI was wrong, 4 (4%) questioned the detection but were unsure, and the remaining 76 (83%) accepted the incorrect detections and blamed themselves for not spotting them. This highlights the challenge of distinguishing between accurate and inaccurate results from AI systems and the importance of assisting users in evaluating the output of AI systems critically. Fortunately, with our knowledge-driven dark pattern checker, which essentially uses a heuristics-based classifier, we can easily trace the reasoning behind its decisions. For example, in Figure <ref type="figure" coords="15,215.06,417.38,3.09,7.94" target="#fig_1">2</ref>, we can see that the system identified point 2 as a false hierarchy instance because it detected two related buttons, one with a grey background and the other with a white background, and the button labeled "Install" was highlighted while the button labeled "No Thanks" was difficult to notice. Providing this information to end-users can help them understand the decision made by the model and determine if it was correct or not.</p><p>8.2.4 Usefulness of our tool. In order to evaluate the effectiveness of our approach, we compared the results from Step 2 with those from Step 4. In Step 4, participants identified 570 instances of malicious UI design in the UIs presented to them. Of these, 453 (79.5%) matched our target dark patterns, yielding a recall rate of 57.8%.</p><p>Overall, the recall rates for most dark pattern types were significantly higher in Step 4 compared to those in Step 2, indicating that participants had learned from the short session in Step 3. However, the recall rates in Step 4 were still relatively low, suggesting that a short session may not be sufficient for participants to fully understand and avoid dark patterns. Powered with knowledge learned from Step 3, the hardness scores all went lower compared to the results in Step 2/3, while the severity scores were similar as in Step 2/3. To further assess the impact of our approach, we conducted a Wilcoxon signed-rank test <ref type="bibr" coords="15,151.57,679.57,14.66,7.94" target="#b40">[43]</ref> to determine whether participants' knowledge of dark patterns had increased. The null hypothesis was that participants' knowledge would remain the same, while the alternative hypothesis was that participants' knowledge would increase. The test yielded a p-value of 7.1 × 10 −9 , indicating that the null hypothesis could be rejected and that the difference in knowledge was statistically significant. This suggests that our approach was successful in increasing participants' knowledge of dark patterns.</p><p>Figure <ref type="figure" coords="15,353.49,537.11,4.17,7.94" target="#fig_10">6</ref> shows the boxplots of usefulness and presentation ratings of our tool. For both these two aspects, UIGuard obtained the scores of a median of 4 and the majority of them had a score falling in the range of 3 to 5, which demonstrates that our system is useful and the current presentation way is acceptable. Besides, among all 58 participants, 45 (77.6%) participants rated the usefulness at 4 or 5, and 42 (72.4%) of them rated the presentation at 4 or 5.</p><p>Generally, participants appreciated the usefulness and accuracy of the tool, and believed this tool could "help users detect many dark patterns that may not be easily found by users" and "avoid them being tricked", and thus brought them better user experience". They also thought they learned something from this study and their awareness on dark patterns was raised. Moreover, some considered this tool can be used as "a foundational training tool" to raise people awareness in dark patterns, "help app stores to examine apps" and "remind the engineers to enhance their design and think about the ethical concerns. " Apart from this, some said they get used to these tricks and could well recognise these tricks by themselves. One interesting thing is that one participant mentioned that "even if we know trick, we cannot avoid clicking it sometime. " In addition, some had different opinions on the concept of certain dark patterns like "Ads are the source of income for some free apps. Watching the ads out of users' choices shouldn't be identified as dark design. " In addition, some participants also expressed confusion over distinguishing true detections from false positives. In terms of presentation, most participants found the tool to be clear and informative, though some mentioned issues with visibility for elderly users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">DISCUSSION AND FUTURE WORK</head><p>As the first tool to detect pattern in mobile applications, our work has the potentials to benefit a variety of stakeholders, including end-users, app providers (designers and developers), and regulators. However, we also emphasise the potential misuse by the app providers, especially if it were actually used by regulators to establish policy. Additional measures should be carefully included to mitigate and avoid this risk.</p><p>End-users are the target users of mobile applications. UIGuard can help end-users to identify and avoid deceptive and manipulative designs in mobile apps, which can improve their user experience and protect them from financial or privacy harm. As seen in Figure <ref type="figure" coords="16,289.01,350.81,3.01,7.94" target="#fig_11">7</ref>, the user is using an app named Scrabble GO, and our UIGuard detects and highlights the presence of dark patterns (in this case, interface inference -aesthetic manipulation -false hierarchy). It also provides detailed information about the specific dark pattern type and its description when the user clicks on the red info icon in the top left corner. This helps users understand the nature of the dark pattern and make informed decisions about how to proceed. However, it is important to consider that not all users may want to receive alerts for every type of dark pattern. Therefore, extra considerations may be necessary when integrating our system into mobile phones.</p><p>App providers, including designers and developers, are the ones who design and develop the UIs of apps. However, the regulations on dark patterns are often impractical due to their generality, and the various forms of dark patterns, along with the expertise required to recognize them, make it challenging for designers and developers to avoid them. In fact, they can even fall victim to dark patterns themselves <ref type="bibr" coords="16,127.13,548.07,13.22,7.94" target="#b37">[39]</ref>. Therefore, our tool can serve as a guiding tool to improve their design practices and ethical considerations. For designers, UIGuard can alert them the potential dark patterns in shared designs in online design sharing platform they are considering using as inspiration. Developers can also use our tool to test for the potential inclusion of dark patterns in their own work. Additionally, providing app providers with clear information about which laws are being violated when dark patterns are detected may increase their desire to avoid using such designs in the future. As seen in Figure <ref type="figure" coords="16,109.82,646.70,3.13,7.94" target="#fig_13">8</ref>, the designers is browsing the shared design in dribbble platform, and click one design and consider it as trendy and fancy. With the additional assistance of some UI screen extraction techniques <ref type="bibr" coords="16,113.99,679.57,9.52,7.94" target="#b1">[2]</ref>, our UIGuard alerts the designers of the dark patterns, and the designer can also learn about dark patterns and gradually recognise the existence of dark patterns by themselves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>!</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>False Hierarchy</head><p>The option that favors the interest of app provider is more prominent ! However, there is also a risk that the app providers may misuse such a system to evade dark pattern detection, especially if it were actually used by regulators to establish policy. This potential for misuse underscores the importance of incorporating safeguards. Measures such as continuous algorithmic updates, wider criteria for detection, and stringent regulatory oversight can act as safeguards to prevent this system's misuse, thereby ensuring it continues to serve its original purpose of enhancing user experiences and maintaining app integrity.</p><p>Regulators. Many regulations, like EDPB <ref type="foot" coords="16,489.60,375.31,6.76,6.44" target="#foot_6">12</ref> , have published some legal documents or guidelines to constrain the usage of dark patterns. However, there are numerous mobile apps in the market. These apps keep updates, introducing new features, and new apps keep coming. It is hard and impossible for the regulators to examine all apps. Paired with some UI exploration tools, our UIGuard can assist regulators in monitoring and enforcing ethical practices in the mobile app market. As seen in Figure <ref type="figure" coords="16,483.10,454.17,3.13,7.94" target="#fig_15">9</ref>, based on UIGuard and paired with UI exploration techniques, it can automatically generate a dark pattern report for the regulators even the app markets to easier evaluate apps. Nevertheless, concerns about potential misuse of this technique persist. As such, the strategies highlighted in the preceding paragraph should be actively considered during implementation to guard against such risks.</p><p>Next, we discuss boundary of dark patterns and personalisation, design tradeoff and some limitaitons and future work.</p><p>Boundary of dark patterns and personalisation. The boundary of dark pattern can be a blurry one. While it is important to protect users from deceptive and manipulative design, it is also important to allow for customization and personalization to cater to different user needs. One way to address this issue is to provide adjustable warning levels and allow users to choose which dark patterns to detect and report. This could be done by giving users control over their own settings and allowing them to prioritize certain dark patterns, such as those involving money or privacy.    people", and another noted that "I think the dark pattern should be prioritized. For example, dark patterns that involve money or privacy should be given more importance by users. " In the future, it would be interesting to explore ways to balance the need for protection from dark patterns with personalization.</p><p>Design tradeoff. Dark patterns can be harmful to users by deceiving or manipulating them, but detecting and highlighting these patterns can also have negative consequences. In order to embed our system into a mobile phone, we need to use the accessibility service to modify the current user interface, which raises privacy concerns. Additionally, providing extra knowledge or hints through the tool may complicate the user interface, disrupt the app flow, and ultimately annoy users. To mitigate these issues, it may be necessary to consider better methods for integrating the tool and allow users to gradually turn off hints for certain types of dark patterns. False negative reports can also be burdensome, so it is important to consider how to address this issue.</p><p>Limitations and Future work. As the first work to automatically detect dark patterns in mobile apps, there are several directions we can improve and extend our work: First, the current work can only detect dark pattern types with signifiers, like disguised ads with ad icons. Patterns without signifiers may be worse but are not included in our current implementations. However, based on our findings from the user study (Table <ref type="table" coords="17,450.69,498.47,4.24,7.94" target="#tab_6">6</ref> and Section 8), participants detected only 62.5% of disguised ads. With our tool's education, recall rate increased by 27.52% to 79.7%. For worse practices without a signifier, we considered them as future work. Second, our current work only focused on a subset of dark pattern types, and future work could expand to include more types of dark patterns, particularly dynamic ones. Third, the taxonomy of dark patterns may evolve when new patterns emerges, our tool can not automatically adapted to the latest changes. Fortunately, our modular method allows easy integration of new rules and property identification modules for emerging DPs. The future work may also focus on the automated evolution of the tool to new concepts. However, we emphasise that monitoring and maintenance are essential for responsible AI practices. In addition, the system could be further improved by integrating with UI modification works <ref type="bibr" coords="17,514.49,651.89,9.40,7.94" target="#b2">[3,</ref><ref type="bibr" coords="17,526.12,651.89,11.57,7.94" target="#b25">27]</ref> to directly block the malicious designs instead of just highlighting them to users, which could decrease the cognitive burden and protect users from the harms. It is also worth researching on the tradeoff between a direct modification and the hint way. Moreover, studying the impact of the tool on user behavior, such as whether it causes users to be more cautious when using apps or whether it changes their app usage habits, would be an interesting point. Lastly, adding more clarification on the legal aspects of dark patterns could also be a valuable direction for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">THREATS TO VALIDITY 10.1 Internal Validity</head><p>We annotated DPs for evaluation, noticing that the concept's interpretation may vary <ref type="bibr" coords="18,128.45,205.23,13.61,7.94" target="#b16">[18,</ref><ref type="bibr" coords="18,144.53,205.23,10.21,7.94" target="#b19">21]</ref>. Hence, we used existing literature's taxonomy and examples for learning, and individually annotated Di's videos <ref type="bibr" coords="18,95.10,227.15,14.59,7.94" target="#b16">[18]</ref> to ensure a consistent and precise grasp of DPs. We also reported the inter-rater agreement results (Cohen's Kappa) in Section 6.2. The agreement for DP presence is 0.97, and for identifying specific types, it is 0.89. These results indicate high agreement between annotators, which can mitigate the potential threat.</p><p>While our UIGuard performs well in the testing dataset, we acknowledge that some kinds of data may be missed and we were committed to addressing this concern. By using the most comprehensive UI dataset, Rico, and carefully annotating the test set in Section 6.2, we ensured diverse coverage of UIs. Certain DP samples appear a limited number of times, but we actively mitigated potential threats by transparently presenting all statistics in Section 7.2 and in Table <ref type="table" coords="18,102.55,358.65,3.13,7.94" target="#tab_3">4</ref>. While the results have slight differences after excluding cases with limited examples, it does not affect the overall findings and conclusions. In addition, we released our dataset for future analysis of the potential issues. Adding more dark pattern instances and trendy UIs is in our plans for future work.</p><p>To enhance participation in our study, we segmented the UIs into four subsets, each resulting in a 15-minute survey. This introduced a potential sampling bias as different participants evaluate each survey. We mitigated this by randomly assigning around nine participants per survey. Our demographic analysis in Section A shows no significant differences across surveys. Furthermore, Section 8.2.4's evaluation of individual knowledge on dark patterns, unaffected by assignments, affirms UIGuard's effectiveness. In addition, additional threat may be introduced due to participant diversity. To counteract this, we sourced participants (n=58) with varied backgrounds from different platforms. Hence, we believe these threats are adequately mitigated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.2">External Validity</head><p>Our UIGuard was trained and tested on the Rico and Liu dataset, which was collected in 2017 and 2018. As the UIs keep updating and new design style emerges, we do not yet know the performance of our tool on the latest UIs. However, Rico dataset remains the most widely-used and high-quality dataset for Android UI designs <ref type="bibr" coords="18,278.30,624.78,13.39,7.94" target="#b14">[15]</ref>, which can prove the usefulness of our proposed technique to some extent. In addition, our approach does not make any specific assumption of UI designs. While the style and design rules keep changing over the years, the basic elements remain the same. The UI page always has basic elements, like buttons, images, and icons, which have learned and by our model. Our approach is not evaluated on new UIs, but this does not mean our approach and results are fundamentally limited to only those old UIs in Rico. Our experiments show that UIGuard achieves around 0.8 F1 scores under different calculations, which is great progress in this domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">CONCLUSION</head><p>In this work, we analysed existing dark pattern taxonomies for mobile platform, and integrated them into one unified taxonomy. Our characteristics analysis on the taxonomy categorised the dark pattern into two broad types, i.e., static and dynamic dark patterns, identified six core properties to recognise the existence of dark patterns on screen, and distilled knowledge from practice. We designed and presented UIGuard that extracts core properties from UI screenshots step by step, and evaluate the existence of dark pattern by pattern matching. Our experiments and user study demonstrate that our UIGuard is promising and useful. Our work illustrates a novel and systematic approach to mitigate the impacts brought from dark patterns, and many applications could be benefited from this.  Education Background: As seen in Table <ref type="table" coords="20,227.80,428.40,3.13,7.94" target="#tab_7">9</ref>, majority held a master's degree, with similar numbers of participants having bachelor's or doctoral degrees. Only a small number had qualifications below a bachelor's degree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Under Bachelor Bachelor Master Doctoral</head><p>Career Distribution: As seen in Table <ref type="table" coords="20,215.36,472.23,7.01,7.94" target="#tab_8">10</ref>, most participants preferred not to disclose their career. The full survey had many researchers and students, while the rest included people from various careers.</p><p>Mobile Usage: As seen in Table <ref type="table" coords="20,190.85,516.07,7.01,7.94" target="#tab_9">11</ref>, majority of participants reported using mobile apps over 10 times per day, indicating familiarity with mobile applications.</p><p>Knowledge on Dark Patterns: As seen in Table <ref type="table" coords="20,247.07,548.94,6.86,7.94" target="#tab_10">12</ref>, short versions of the survey received varied ratings, ranging from "not at all" to "some" with limited "very well" or "expert" ratings. The full version had more "not at all" and "a little" ratings.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,53.80,62.77,183.33,6.23;2,504.42,62.77,53.78,6.23;2,79.11,271.50,66.19,6.19;2,81.90,278.25,60.66,6.19;2,477.49,271.50,54.60,6.19;2,487.22,278.25,35.11,6.19;2,360.09,271.41,87.47,6.19;2,373.39,278.16,60.90,6.19;2,277.20,271.41,63.70,6.19;2,290.89,278.16,36.19,6.19;2,172.00,271.41,76.72,6.19;2,178.55,278.16,73.07,6.19;2,62.18,86.61,3.62,6.51;2,72.48,200.49,3.62,6.51;2,168.51,112.64,3.62,6.51;2,240.50,120.35,3.62,6.51;2,258.93,190.73,3.62,6.51;2,258.93,210.25,3.62,6.51;2,364.28,182.05,12.85,6.51;2,463.81,161.44,3.62,6.51;2,462.72,178.27,5.78,5.21;2,462.72,195.51,5.78,5.20;2,462.72,212.85,5.78,5.21;2,462.72,231.41,5.78,5.21;2,462.72,249.73,5.78,5.21;2,90.05,279.11,2.17,3.90;2,183.73,272.06,2.17,3.90;2,173.98,279.65,2.17,3.90;2,287.88,272.60,8.67,3.90;2,371.35,272.10,2.17,3.90;2,368.10,279.15,2.17,3.90;2,489.56,272.19,14.98,3.91;2,90.51,272.06,2.17,3.90"><head>UIST ' 23 ,</head><label>23</label><figDesc>October 29-November 1, 2023, San Francisco, CA, USA Jieshan Chen et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,53.80,244.17,504.41,7.70;7,53.80,255.13,504.40,7.70;7,53.80,266.09,504.40,7.70;7,53.80,277.05,172.96,7.70;7,56.71,86.58,79.58,141.50"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of our approach. Given a UI screenshot, our Property Extraction module extracts element coordinates and types, text content, icon semantic, element status, background and foreground colors and element relationship. Then, powered by knowledge, our Knowledge-Driven Dark Pattern Checker identifies potential dark patterns, delivering detection results and highlighting these patterns with a red box.</figDesc><graphic coords="7,56.71,86.58,79.58,141.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,317.96,701.13,145.21,7.84"><head></head><label></label><figDesc>2 https://digitaladvertisingalliance.org/participating</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="8,53.80,329.93,240.25,8.04;8,53.80,340.95,241.76,7.94;8,53.80,351.91,240.24,7.94;8,53.80,362.87,81.00,7.94;8,53.80,380.01,240.25,8.04;8,53.80,391.03,240.25,7.94;8,53.80,401.99,240.25,7.94;8,53.80,412.94,168.84,7.94"><head>5. 1 . 4</head><label>14</label><figDesc>Element Status recognition. Finer details on the status of UI elements are necessary as explored in Section 4. Therefore, to recognise the status of UI elements, we trained another light-weighted ResNet-18 [25] model. 5.1.5 Color Extraction. Color can impact the visual saliency to manipulate user perception. To extract the background and text color of UI elements, we use color histogram to first obtain the most frequently color as the background color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="9,53.80,62.77,238.29,6.23;9,374.86,62.77,183.33,6.23"><head></head><label></label><figDesc>Unveiling the Tricks: Automated Detection of Dark Patterns in Mobile Applications UIST '23, October 29-November 1, 2023, San Francisco, CA, USA</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="12,53.80,382.35,504.41,7.70;12,53.80,393.31,504.41,7.70;12,53.80,404.27,504.40,7.70;12,53.80,415.23,395.02,7.70;12,144.83,221.91,75.65,134.50"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Some examples of false positives. (a) A screenshot of a settings UI page featuring several items. One item includes a nagging pattern saying, "Rate the app! If you enjoy our app, please help us reach a five-star rating!" (b) A UI features a section at the bottom labeled "Upgrade to ad-free version." (c) A UI contains an embedded ad that noticeably differs from the rest of the content. (d) The UI includes an advertisement at the bottom, styled similarly to the other content.</figDesc><graphic coords="12,144.83,221.91,75.65,134.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="13,53.80,153.01,504.59,7.70;13,53.80,163.97,504.41,7.70;13,53.80,174.93,504.41,7.70;13,53.80,185.89,486.67,7.70;13,184.21,156.01,387.74,109.25"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: User Study Procedure: Prior to the study, participants was briefed on the study's purpose and procedure, and their consents were obtained. Step 1: Participants completed a demographic questionnaire. Step 2: Participants evaluated UIs based on their dark pattern understanding. Step 3: Participants compared their evaluations with UIGuard's detection results for the UIs from Step 2. Step 4: Participants evaluated a second set of similar UIs. Step 5: Participants provided overall feedback.</figDesc><graphic coords="13,184.21,156.01,387.74,109.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="14,207.74,210.45,196.52,7.70"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Participants' demographic information</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="15,317.96,427.56,240.24,7.70;15,317.96,438.49,78.83,7.74"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Boxplots for general usefulness and presentation ratings for UIGuard.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11" coords="16,317.96,223.98,241.34,7.70;16,317.96,234.91,240.24,7.74;16,317.96,245.90,55.84,7.70;16,472.17,84.96,61.15,124.41"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: While end-users use the mobile applications, UIGuard can highlight potential dark patterns and avoid them being tricked.</figDesc><graphic coords="16,472.17,84.96,61.15,124.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12" coords="16,317.69,651.43,240.51,7.94;16,317.96,662.39,240.25,7.94;16,317.96,673.35,240.25,7.94;17,288.20,220.81,62.09,10.51;17,62.64,220.95,53.79,10.51;17,431.23,159.85,1.73,5.91;17,428.78,133.63,24.18,5.25;17,428.78,139.43,54.29,5.25;17,428.78,145.36,47.43,5.25;17,169.46,173.18,7.09,10.51"><head></head><label></label><figDesc>These also are mentioned by our participant in the user study: one said "people should select what severe results they care most, and the tool should know this and customize its service to different Browsing design App designers ! Preselection The privacy policy / terms of use is consent by default ...</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13" coords="17,53.80,247.03,504.40,7.70;17,53.80,257.98,394.97,7.70;17,228.69,85.13,183.75,134.08"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Our system can be embedded into a browser, and automatically detect potential dark patterns in UIs in the design sharing platforms and in case the designers adopt some malicious designs without consciousness.</figDesc><graphic coords="17,228.69,85.13,183.75,134.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14" coords="17,238.56,340.85,21.98,7.21;17,482.14,354.83,0.96,3.28;17,325.96,290.00,68.23,9.17;17,322.23,299.81,46.42,7.21;17,322.23,307.43,42.79,7.21;17,322.23,315.04,48.81,7.21;17,322.23,322.66,72.76,7.21;17,322.23,329.71,45.14,7.21;17,322.23,344.37,18.84,7.21;17,322.23,351.99,40.26,7.21;17,322.23,398.74,3.24,5.24;17,346.24,362.58,32.38,4.59;17,346.24,367.23,47.50,4.59;17,346.24,372.03,45.94,4.59;17,346.24,377.24,39.27,4.59;17,346.24,381.90,17.40,4.59"><head></head><label></label><figDesc>to be a normal content and users may click without knowing it</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15" coords="17,53.80,437.62,504.40,7.70;17,53.80,448.58,67.76,7.70;17,430.95,282.51,71.66,127.40"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Our system could also be used with an app UI explorer and therefore help regulators to examine the potential existence of dark patterns.</figDesc><graphic coords="17,430.95,282.51,71.66,127.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,333.92,558.54,225.79,150.89"><head>Table 1 :</head><label>1</label><figDesc>Integrated taxonomy of Dark Patterns. From left to right, it shows the category, types, sub-types, and their descriptions. Following these, we show the characteristics analysis results. Hybrid in Column "Is Context-Independent?" means that the current case is context related but can easily be spotted by a single UI page.</figDesc><table coords="4,333.92,558.54,225.79,150.89"><row><cell>• Obstruction makes things harder to do that favours the</cell></row><row><cell>app company's interest. There are three types, namely Roach</cell></row><row><cell>Motel (easy to get in, hard to get out), Price Comparison Pre-</cell></row><row><cell>vention(unable to select product names to make comparisons</cell></row><row><cell>in other platforms) and Intermediate Currency (disconnect</cell></row><row><cell>users from real money by using virtual currency).</cell></row><row><cell>• Sneaking tries to hide, disguise or delay information that is</cell></row><row><cell>relevant to users. It has four types. Forced Continuity forces</cell></row><row><cell>users to auto continue the service when their purchase one</cell></row><row><cell>expires. Hidden Costs delays information like high tax rate or</cell></row><row><cell>delivery fee at the late stage of checkout. Sneak into Basket</cell></row><row><cell>quietly adds additional items to users without their consent.</cell></row><row><cell>Bait and Switch misleads users to action by giving false ex-</cell></row><row><cell>pectations.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,53.50,85.73,504.70,128.97"><head>Table 2 :</head><label>2</label><figDesc>Example patterns we distilled from our knowledge base to serve as the knowledge in identifying dark patterns. The full list can be seen in<ref type="bibr" coords="8,144.86,96.69,10.44,7.70" target="#b7">[8]</ref>.</figDesc><table coords="8,70.49,122.08,444.82,72.90"><row><cell>Type</cell><cell>Text Patterns (partial rules)</cell><cell>Need any other information?</cell></row><row><cell>FA -General -PRO (pay to avoid</cell><cell>...remove/without/disable/block/no .. ad/ ads/ advertisement/ advertisements/ advertising/adverts</cell><cell>No</cell></row><row><cell>ads)</cell><cell>...</cell><cell></cell></row><row><cell>FA -Watch to unlock features</cell><cell>...watch... ad/ads/advertisements/ advertisements/ advertising/ adverts...</cell><cell>No</cell></row><row><cell>NG -Pop up to Rate</cell><cell>if you enjoy/like ... apps, ...rate...</cell><cell>Star icons</cell></row><row><cell>NG -Pop-UP AD</cell><cell>Ad Text</cell><cell>AD-related Icons, Coordinates</cell></row><row><cell>II -Preselection</cell><cell></cell><cell></cell></row></table><note>.. consent/agree/give consent/ accept ... terms/ privacy policy/ policies/agreement]... Checkbox is checked II -Aesthetic Manipulation -False Hierarchy One of the options should contain texts like No/no thanks/ close/next time/later/ not now/skip/cancel to ensure the malicious intent Color &amp; Grouping</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="10,53.59,85.73,475.44,218.93"><head>Table 3 :</head><label>3</label><figDesc>Performance of element localisation models.</figDesc><table coords="10,53.59,112.65,475.44,192.00"><row><cell>Method</cell><cell cols="3">Non-text elements</cell><cell cols="2">Text Elements</cell><cell></cell><cell cols="2">All elements</cell><cell></cell></row><row><cell></cell><cell cols="3">Precision Recall F1</cell><cell cols="3">Precision Recall F1</cell><cell cols="3">Precision Recall F1</cell></row><row><cell>UIED</cell><cell>0.37</cell><cell>0.46</cell><cell>0.41</cell><cell>0.48</cell><cell>0.52</cell><cell>0.50</cell><cell>0.42</cell><cell>0.50</cell><cell>0.46</cell></row><row><cell>FRCNN(nontext)</cell><cell>0.48</cell><cell>0.52</cell><cell>0.50</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>/</cell></row><row><cell>FRCNN(nontext)+PaddleOCR</cell><cell>0.47</cell><cell>0.48</cell><cell>0.47</cell><cell>0.56</cell><cell>0.48</cell><cell>0.51</cell><cell>0.54</cell><cell>0.50</cell><cell>0.52</cell></row><row><cell>FRCNN(all)</cell><cell>0.56</cell><cell>0.44</cell><cell>0.49</cell><cell>0.58</cell><cell>0.57</cell><cell>0.57</cell><cell>0.60</cell><cell>0.53</cell><cell>0.56</cell></row><row><cell>UIGuard</cell><cell>0.56</cell><cell>0.43</cell><cell>0.49</cell><cell>0.62</cell><cell>0.62</cell><cell>0.62</cell><cell>0.61</cell><cell>0.55</cell><cell>0.58</cell></row><row><cell cols="4">categories and levels of popularity, ensuring the representativeness</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">of the dataset. In total, our test dataset consists of 6,352 UIs from</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">1,023 apps in 27 app categories. The annotation process consists of</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">two steps. First, we labelled the types of dark patterns present in</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">each UI. Then, we annotated the positions of these patterns within</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>each UI.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="11,317.66,85.73,247.28,374.50"><head>Table 4 :</head><label>4</label><figDesc>Detailed performance results for each dark pattern type</figDesc><table coords="11,321.33,122.10,243.61,338.13"><row><cell>DP Type</cell><cell cols="5"># of GT # of Detection Precision Recall F1</cell></row><row><cell>NG -Pop-up AD</cell><cell>178</cell><cell>156</cell><cell>0.82</cell><cell>0.72</cell><cell>0.77</cell></row><row><cell>NG -Pop-up to rate</cell><cell>1</cell><cell>10</cell><cell>0.10</cell><cell>1.00</cell><cell>0.18</cell></row><row><cell>NG -Pop-up to upgrade</cell><cell>9</cell><cell>38</cell><cell>0.21</cell><cell>0.89</cell><cell>0.34</cell></row><row><cell>SN -Forced Continuity</cell><cell>1</cell><cell>1</cell><cell>1.00</cell><cell>1.00</cell><cell>1.00</cell></row><row><cell>II -Preselection -Checkbox se-</cell><cell>146</cell><cell>128</cell><cell>0.86</cell><cell>0.75</cell><cell>0.80</cell></row><row><cell>lected</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>II -Preselection -No Checkbox</cell><cell>86</cell><cell>73</cell><cell>0.78</cell><cell>0.66</cell><cell>0.72</cell></row><row><cell>II -AM -False Hierarchy</cell><cell>273</cell><cell>187</cell><cell>0.65</cell><cell>0.45</cell><cell>0.53</cell></row><row><cell>II -AM -Disguised AD</cell><cell>46</cell><cell>94</cell><cell>0.24</cell><cell>0.50</cell><cell>0.33</cell></row><row><cell>II -AM -General Types -small</cell><cell>684</cell><cell>649</cell><cell>0.99</cell><cell>0.94</cell><cell>0.97</cell></row><row><cell>close buttons</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FA -Social Pyramid</cell><cell>16</cell><cell>28</cell><cell>0.50</cell><cell>0.88</cell><cell>0.64</cell></row><row><cell>FA -Privacy Zuckering</cell><cell>117</cell><cell>97</cell><cell>0.80</cell><cell>0.67</cell><cell>0.73</cell></row><row><cell>FA -General Types -Countdown</cell><cell>4</cell><cell>4</cell><cell>0.75</cell><cell>0.75</cell><cell>0.75</cell></row><row><cell>ads</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FA -General Types -watch AD to</cell><cell>2</cell><cell>3</cell><cell>0.67</cell><cell>1.00</cell><cell>0.80</cell></row><row><cell>unlock feature</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FA -General Types -Pay to avoid</cell><cell>97</cell><cell>100</cell><cell>0.87</cell><cell>0.90</cell><cell>0.88</cell></row><row><cell>Ad</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Nagging (NG)</cell><cell>188</cell><cell>204</cell><cell>0.67</cell><cell>0.73</cell><cell>0.70</cell></row><row><cell>Sneaking (SN)</cell><cell>1</cell><cell>1</cell><cell>1.00</cell><cell>1.00</cell><cell>1.00</cell></row><row><cell>Interface Inference (II)</cell><cell>1,235</cell><cell>1,131</cell><cell>0.85</cell><cell>0.77</cell><cell>0.81</cell></row><row><cell>Forced Action (FA)</cell><cell>236</cell><cell>232</cell><cell>0.79</cell><cell>0.78</cell><cell>0.79</cell></row><row><cell cols="2">Excluding cases with limited examples (n&lt;10)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Macro Average</cell><cell>1,643</cell><cell>1,512</cell><cell>0.82</cell><cell>0.76</cell><cell>0.79</cell></row><row><cell>Micro Average</cell><cell>1,643</cell><cell>1,512</cell><cell>0.84</cell><cell>0.77</cell><cell>0.80</cell></row><row><cell>OVERALL</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Macro Average</cell><cell>1,660</cell><cell>1,568</cell><cell>0.83</cell><cell>0.82</cell><cell>0.82</cell></row><row><cell>Micro Average</cell><cell>1,660</cell><cell>1,568</cell><cell>0.82</cell><cell>0.77</cell><cell>0.79</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="12,57.76,85.73,494.86,110.68"><head>Table 5 :</head><label>5</label><figDesc>Overall Performance on detecting dark patterns</figDesc><table coords="12,57.76,111.59,494.86,84.82"><row><cell>Method</cell><cell></cell><cell>Nagging</cell><cell></cell><cell></cell><cell>Sneaking</cell><cell></cell><cell cols="3">Interface Inference</cell><cell cols="2">Forced Action</cell><cell></cell><cell cols="2">OVERALL</cell></row><row><cell></cell><cell cols="2">Precision Recall</cell><cell>F1</cell><cell cols="2">Precision Recall</cell><cell>F1</cell><cell cols="2">Precision Recall</cell><cell>F1</cell><cell cols="2">Precision Recall</cell><cell>F1</cell><cell cols="2">Precision Recall</cell><cell>F1</cell></row><row><cell>Base Model (Text-only)</cell><cell>0.18</cell><cell>0.06</cell><cell>0.09</cell><cell>1.00</cell><cell cols="2">1.00 1.00</cell><cell>0.53</cell><cell>0.06</cell><cell>0.12</cell><cell>0.78</cell><cell>0.69</cell><cell>0.73</cell><cell>0.62</cell><cell>0.45</cell><cell>0.49</cell></row><row><cell>+ Icon Semantic</cell><cell>0.38↑</cell><cell cols="2">0.22↑ 0.28↑</cell><cell>1.00</cell><cell cols="2">1.00 1.00</cell><cell>0.53</cell><cell>0.06</cell><cell>0.12</cell><cell>0.78</cell><cell>0.69</cell><cell>0.73</cell><cell>0.67↑</cell><cell cols="2">0.49↑ 0.53↑</cell></row><row><cell>+ Template Matching</cell><cell>0.67↑</cell><cell cols="2">0.73↑ 0.70↑</cell><cell>1.00</cell><cell cols="2">1.00 1.00</cell><cell>0.89↑</cell><cell cols="2">0.59↑ 0.71↑</cell><cell>0.78</cell><cell>0.69</cell><cell>0.73</cell><cell>0.84↑</cell><cell cols="2">0.75↑ 0.79↑</cell></row><row><cell>+ Status Recognition</cell><cell>0.67</cell><cell>0.73</cell><cell>0.70</cell><cell>1.00</cell><cell cols="2">1.00 1.00</cell><cell>0.88</cell><cell cols="2">0.68↑ 0.77↑</cell><cell>0.79↑</cell><cell cols="2">0.78↑ 0.79↑</cell><cell>0.84</cell><cell cols="2">0.80↑ 0.81↑</cell></row><row><cell>+ Color&amp;Grouping</cell><cell>0.67</cell><cell>0.73</cell><cell>0.70</cell><cell>1.00</cell><cell cols="2">1.00 1.00</cell><cell>0.85↓</cell><cell cols="2">0.77↑ 0.81↑</cell><cell>0.79</cell><cell>0.78</cell><cell>0.79</cell><cell>0.83</cell><cell cols="2">0.82↑ 0.82↑</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="14,53.80,3.08,504.40,192.07"><head></head><label></label><figDesc>UIST'23, October 29-November 1, 2023, San Francisco, CA, USA Jieshan Chen et al.</figDesc><table coords="14,104.10,3.08,402.26,192.07"><row><cell cols="2">Knowledge on Dark Pattern day) 20</cell><cell>1</cell><cell>Degree</cell><cell>2</cell><cell cols="3">Age Range Gender 12</cell><cell>24</cell><cell></cell><cell></cell></row><row><cell>19</cell><cell></cell><cell>10</cell><cell></cell><cell>14</cell><cell></cell><cell>39</cell><cell></cell><cell>33</cell><cell></cell><cell></cell></row><row><cell>15</cell><cell></cell><cell>47</cell><cell></cell><cell>27</cell><cell></cell><cell>4</cell><cell></cell><cell>1</cell><cell></cell><cell></cell></row><row><cell>3</cell><cell></cell><cell>0</cell><cell></cell><cell>15</cell><cell></cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Demographic Information</cell><cell></cell><cell></cell></row><row><cell>Gender</cell><cell></cell><cell></cell><cell></cell><cell cols="2">24</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>33</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Female</cell><cell></cell><cell></cell><cell></cell><cell>Male</cell><cell>Prefer not to say</cell></row><row><cell>Age Range</cell><cell></cell><cell></cell><cell>12</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>39</cell><cell></cell><cell></cell><cell>4</cell><cell>2</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>18-24</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>25-34</cell><cell></cell><cell></cell><cell>35-44 45-54 &gt;=65</cell></row><row><cell>Degree</cell><cell>2</cell><cell></cell><cell>14</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>27</cell><cell></cell><cell></cell><cell>15</cell></row><row><cell>mobile apps (times per day) Frequency of using</cell><cell cols="3">1 &lt; 5 Under Bachelor 10 5-10</cell><cell cols="2">Bachelor</cell><cell></cell><cell></cell><cell>Master</cell><cell>&gt;10 47</cell><cell></cell><cell>Doctoral</cell></row><row><cell>Knowledge on Dark Pattern</cell><cell></cell><cell></cell><cell cols="2">20</cell><cell></cell><cell></cell><cell></cell><cell>19</cell><cell></cell><cell></cell><cell>15</cell><cell>3</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Not at all</cell><cell></cell><cell></cell><cell>A little</cell><cell></cell><cell></cell><cell>Some of it</cell><cell>Very well Expert</cell></row><row><cell></cell><cell>0%</cell><cell></cell><cell>10%</cell><cell cols="2">20%</cell><cell>30%</cell><cell>40%</cell><cell>50%</cell><cell>60%</cell><cell>70%</cell><cell>80%</cell><cell>90%</cell><cell>100%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="15,60.65,85.73,490.14,233.44"><head>Table 6 :</head><label>6</label><figDesc>Detailed statistics for user study.</figDesc><table coords="15,60.65,111.34,490.14,207.82"><row><cell>DP type</cell><cell></cell><cell></cell><cell cols="2">Step2 -before education</cell><cell></cell><cell></cell><cell cols="3">Step4 -after education</cell><cell></cell></row><row><cell></cell><cell cols="10"># of TP # of GT Recall Severity (TP/FN) Hardness(TP/FN) # of TP # of GT Recall Severity Hardness</cell></row><row><cell>NG-Pop-up AD</cell><cell>7</cell><cell>66</cell><cell>0.106</cell><cell>3.1/3.0</cell><cell>1.7/3.0</cell><cell>36</cell><cell>66</cell><cell>0.545</cell><cell>2.8</cell><cell>1.4</cell></row><row><cell>NG-Pop-up to rate</cell><cell>6</cell><cell>33</cell><cell>0.182</cell><cell>2.3/2.5</cell><cell>2.2/2.0</cell><cell>6</cell><cell>33</cell><cell>0.182</cell><cell>2.7</cell><cell>1.7</cell></row><row><cell>NG-Pop-up to upgrade</cell><cell>2</cell><cell>66</cell><cell>0.03</cell><cell>4.5/2.3</cell><cell>3.0/2.8</cell><cell>15</cell><cell>33</cell><cell>0.455</cell><cell>3.0</cell><cell>2.1</cell></row><row><cell>SN-Forced Continuity</cell><cell>4</cell><cell>33</cell><cell>0.121</cell><cell>4.2/1.5</cell><cell>3.2/1.5</cell><cell>18</cell><cell>33</cell><cell>0.545</cell><cell>3.5</cell><cell>2.3</cell></row><row><cell>II-Preselection-Checkbox selected</cell><cell>15</cell><cell>99</cell><cell>0.152</cell><cell>3.6/3.2</cell><cell>3.3/3.2</cell><cell>68</cell><cell>132</cell><cell>0.515</cell><cell>3.2</cell><cell>2.5</cell></row><row><cell>II-Preselection-No Checkbox</cell><cell>3</cell><cell>31</cell><cell>0.097</cell><cell>3.0/3.6</cell><cell>3.3/3.9</cell><cell>26</cell><cell>31</cell><cell>0.839</cell><cell>3.5</cell><cell>2.8</cell></row><row><cell>II-AM-False Hierarchy</cell><cell>15</cell><cell>66</cell><cell>0.227</cell><cell>3.5/2.0</cell><cell>2.7/2.9</cell><cell>54</cell><cell>99</cell><cell>0.545</cell><cell>2.6</cell><cell>2.4</cell></row><row><cell>II-AM-Disguised AD</cell><cell>40</cell><cell>64</cell><cell>0.625</cell><cell>3.0/3.0</cell><cell>2.5/3.4</cell><cell>51</cell><cell>64</cell><cell>0.797</cell><cell>2.9</cell><cell>2.5</cell></row><row><cell>II-AM-General Types-small close buttons</cell><cell>35</cell><cell>163</cell><cell>0.215</cell><cell>3.2/2.8</cell><cell>2.2/3.2</cell><cell>41</cell><cell>66</cell><cell>0.621</cell><cell>3.0</cell><cell>2.0</cell></row><row><cell>FA-Social Pyramid</cell><cell>0</cell><cell>33</cell><cell>0.0</cell><cell>-/2.5</cell><cell>-/3.5</cell><cell>21</cell><cell>33</cell><cell>0.636</cell><cell>2.9</cell><cell></cell></row><row><cell>FA-Privacy Zuckering</cell><cell>16</cell><cell>97</cell><cell>0.165</cell><cell>3.5/3.2</cell><cell>3.2/3.3</cell><cell>64</cell><cell>97</cell><cell>0.660</cell><cell>3.3</cell><cell>2.6</cell></row><row><cell>FA-General Types-Countdown ads</cell><cell>9</cell><cell>33</cell><cell>0.273</cell><cell>3.4/3.0</cell><cell>1.4/2.4</cell><cell>22</cell><cell>33</cell><cell>0.667</cell><cell>3.2</cell><cell>2.3</cell></row><row><cell>FA-General Types-watch AD to unlock feature</cell><cell>8</cell><cell>33</cell><cell>0.242</cell><cell>3.1/2.8</cell><cell>2.0/3.6</cell><cell>14</cell><cell>33</cell><cell>0.424</cell><cell>2.6</cell><cell>2.6</cell></row><row><cell>FA-General Types-Pay to avoid Ad</cell><cell>3</cell><cell>64</cell><cell>0.047</cell><cell>4.3/2.9</cell><cell>2.0/3.2</cell><cell>17</cell><cell>31</cell><cell>0.548</cell><cell>3.1</cell><cell>2.5</cell></row><row><cell>Overall</cell><cell>163</cell><cell>881</cell><cell>0.185</cell><cell>3.3/2.8</cell><cell>2.5/3.1</cell><cell>453</cell><cell>784</cell><cell>0.578</cell><cell>3.1</cell><cell>2.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="20,58.80,203.25,240.11,179.17"><head>Table 9 :</head><label>9</label><figDesc>Educational Background Distribution</figDesc><table coords="20,58.80,241.92,240.11,140.50"><row><cell></cell><cell cols="3">Students Software</cell><cell cols="3">researchers others prefer</cell></row><row><cell></cell><cell></cell><cell>engi-</cell><cell></cell><cell></cell><cell></cell><cell>not to</cell></row><row><cell></cell><cell></cell><cell>neer</cell><cell>/</cell><cell></cell><cell></cell><cell>say</cell></row><row><cell></cell><cell></cell><cell cols="2">develop-</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>ers</cell><cell></cell><cell></cell><cell></cell></row><row><cell>FULL</cell><cell>5</cell><cell>1</cell><cell></cell><cell>8</cell><cell>2</cell><cell>8</cell></row><row><cell>SHORT1</cell><cell>1</cell><cell>4</cell><cell></cell><cell>2</cell><cell>0</cell><cell>2</cell></row><row><cell>SHORT2</cell><cell>3</cell><cell>0</cell><cell></cell><cell>1</cell><cell>1</cell><cell>4</cell></row><row><cell>SHORT3</cell><cell>1</cell><cell>0</cell><cell></cell><cell>3</cell><cell>1</cell><cell>2</cell></row><row><cell>SHORT4</cell><cell>0</cell><cell>1</cell><cell></cell><cell>3</cell><cell>2</cell><cell>3</cell></row><row><cell>Total</cell><cell>10</cell><cell>6</cell><cell></cell><cell>17</cell><cell>6</cell><cell>19</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="20,114.63,388.47,118.29,7.70"><head>Table 10 :</head><label>10</label><figDesc>Career distribution.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="20,317.66,203.39,244.91,152.74"><head>Table 11 :</head><label>11</label><figDesc>Frequency of using mobile apps per day (times per day).</figDesc><table coords="20,323.31,249.44,239.25,106.69"><row><cell></cell><cell cols="5">Not at all A little Some Very well Expert</cell></row><row><cell>FULL</cell><cell>10</cell><cell>10</cell><cell>3</cell><cell>1</cell><cell>0</cell></row><row><cell>SHORT1</cell><cell>3</cell><cell>1</cell><cell>4</cell><cell>0</cell><cell>1</cell></row><row><cell>SHORT2</cell><cell>1</cell><cell>4</cell><cell>3</cell><cell>1</cell><cell>0</cell></row><row><cell>SHORT3</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>1</cell><cell>0</cell></row><row><cell>SHORT4</cell><cell>4</cell><cell>2</cell><cell>3</cell><cell>0</cell><cell>0</cell></row><row><cell>Total</cell><cell>20</cell><cell>19</cell><cell>15</cell><cell>3</cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="20,360.17,362.39,155.53,7.70"><head>Table 12 :</head><label>12</label><figDesc>Knowledge on dark patterns.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0">https://darkpatterns.uxp2.com/patterns/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1">We provide the list of UI element and examples in the supplementary materials</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2">All icon types can be seen in supplementary materials.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_3">We emailed the authors but did not get response from them.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_4">Excluding UIs that have no dark patterns but are detected by our tool</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_5">Note that we do not report the number of instances that participants spotted for each type because we cannot tell which type they are.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_6">https://edpb.europa.eu/our-work-tools/documents/publicconsultations/2022/guidelines-32022-dark-patterns-social-media_en</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A DETAILED DEMOGRAPHIC DISTRIBUTION</head><p>In this section, we show the detailed distributions of all five surveys used in Section 8. We in total have five kinds of survey, one full version (FULL), and four 15-min versions (SHORT1, SHORT2, SHORT3, SHORT4). As seen in Table <ref type="table" coords="19,451.68,624.78,3.43,7.94">7</ref>-12, no distinct pattern in demographic was found across different segments and the full survey.</p><p>Gender: As seen in Table <ref type="table" coords="19,429.89,657.66,3.13,7.94">7</ref>, all surveys had an even gender distribution, with near equal representation of male and female participants.</p><p>Age: As seen in Table <ref type="table" coords="19,413.17,690.53,3.13,7.94">8</ref>, most prevalent age range was 25-34, followed by 18-24. Fewer participants were in the 35-74 range.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="18,333.39,320.23,225.88,6.18;18,333.39,328.20,224.81,6.18;18,333.39,336.11,224.81,6.25;18,333.39,344.08,40.90,6.25" xml:id="b0">
	<analytic>
		<title level="a" type="main">A Game of Dark Patterns: Designing Healthy, Highly-Engaging Mobile Games</title>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Aagaard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miria</forename><forename type="middle">Emma Clausen</forename><surname>Knudsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Per</forename><surname>Bækgaard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kevin</forename><surname>Doherty</surname></persName>
		</author>
		<idno type="DOI">10.1145/3491101.3519837</idno>
	</analytic>
	<monogr>
		<title level="m">CHI Conference on Human Factors in Computing Systems Extended Abstracts</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2022-04-27">2022</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,333.39,352.11,224.81,6.18;18,333.39,360.08,224.81,6.18;18,333.39,367.99,224.81,6.25;18,333.39,375.96,224.81,6.25;18,333.39,383.93,93.49,6.25" xml:id="b1">
	<analytic>
		<title level="a" type="main">UIScreens: extracting user interface screens from mobile programming video tutorials</title>
		<author>
			<persName coords=""><forename type="first">Mohammad</forename><surname>Alahmadi</surname></persName>
			<idno type="ORCID">0000-0002-3399-2996</idno>
		</author>
		<author>
			<persName coords=""><forename type="first">Ahmad</forename><surname>Tayeb</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Abdulkarim</forename><surname>Malkadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Esteban</forename><surname>Parra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sonia</forename><surname>Haiduc</surname></persName>
		</author>
		<idno type="DOI">10.1145/3368089.3417935</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering</title>
				<meeting>the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-11-08">2020</date>
			<biblScope unit="page" from="1660" to="1664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,333.39,391.96,224.81,6.18;18,333.39,399.87,224.81,6.25;18,333.39,407.84,223.58,6.25" xml:id="b2">
	<analytic>
		<title level="a" type="main">Automated Repair of Size-Based Inaccessibility Issues in Mobile Applications</title>
		<author>
			<persName coords=""><forename type="first">Paul</forename><forename type="middle">T</forename><surname>Ali S Alotaibi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">William</forename><forename type="middle">Gj</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Halfond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="730" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,333.39,415.81,225.63,6.25;18,333.17,423.84,71.15,6.18" xml:id="b3">
	<monogr>
		<title level="m" type="main">Deceptive Design</title>
		<author>
			<persName coords=""><forename type="first">Harry</forename><surname>Brignull</surname></persName>
		</author>
		<idno type="DOI">10.29229/uzmj2022-2</idno>
		<ptr target="https://www.darkpatterns.org/" />
		<imprint>
			<date type="published" when="2010-08">2010. August 2022</date>
			<publisher>V.I.Romanovskiy Institute of Mathematics</publisher>
			<biblScope unit="volume">66</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,333.39,431.75,224.97,6.25;18,333.15,439.78,144.11,6.18" xml:id="b4">
	<monogr>
		<title level="m" type="main">Twitter: Deceptive Design@darkpatterns</title>
		<author>
			<persName coords=""><forename type="first">Harry</forename><surname>Brignull</surname></persName>
		</author>
		<ptr target="https://twitter.com/darkpatterns" />
		<imprint>
			<date type="published" when="2010-08">2010. August 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,333.39,447.69,224.81,6.25;18,333.39,455.66,93.17,6.25" xml:id="b5">
	<monogr>
		<title level="m" type="main">Template Matching Techniques in Computer Vision</title>
		<author>
			<persName coords=""><forename type="first">Roberto</forename><surname>Brunelli</surname></persName>
		</author>
		<idno type="DOI">10.1002/9780470744055</idno>
		<imprint>
			<date type="published" when="2009-03-27">2009</date>
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,333.39,463.69,225.88,6.18;18,333.39,471.66,224.81,6.18;18,333.39,479.57,224.81,6.25;18,333.39,487.54,87.00,6.25" xml:id="b6">
	<analytic>
		<title level="a" type="main">From UI design image to GUI skeleton</title>
		<author>
			<persName coords=""><forename type="first">Chunyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ting</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guozhu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhenchang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3180155.3180240</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International Conference on Software Engineering</title>
				<meeting>the 40th International Conference on Software Engineering</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018-05-27">2018</date>
			<biblScope unit="page" from="665" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,333.39,495.57,225.89,6.18;18,333.39,503.54,121.24,6.18" xml:id="b7">
	<monogr>
		<title level="m" type="main">Datasets and Detection Rules for UIGuard</title>
		<author>
			<persName coords=""><forename type="first">Jieshan</forename><surname>Chen</surname></persName>
		</author>
		<idno>08/07/2023</idno>
		<ptr target="https://zenodo.org/record/8126443" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,333.39,511.51,224.81,6.18;18,333.39,519.48,224.81,6.18;18,333.39,527.40,224.81,6.25;18,332.97,535.37,77.30,6.25" xml:id="b8">
	<analytic>
		<title level="a" type="main">Wireframe-based UI Design Search through Image Autoencoder</title>
		<author>
			<persName coords=""><forename type="first">Jieshan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chunyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhenchang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liming</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Grundy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jinshui</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3391613</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Software Engineering and Methodology</title>
		<title level="j" type="abbrev">ACM Trans. Softw. Eng. Methodol.</title>
		<idno type="ISSN">1049-331X</idno>
		<idno type="ISSNe">1557-7392</idno>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2020-06-16">2020. 2020</date>
			<publisher>Association for Computing Machinery (ACM)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,333.39,543.39,225.99,6.18;18,333.39,551.36,224.81,6.18;18,333.39,559.28,224.81,6.25;18,333.39,567.25,180.32,6.25" xml:id="b9">
	<analytic>
		<title level="a" type="main">Unblind your apps</title>
		<author>
			<persName coords=""><forename type="first">Jieshan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chunyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhenchang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liming</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guoqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jinshui</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3377811.3380327</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering</title>
				<meeting>the ACM/IEEE 42nd International Conference on Software Engineering</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-06-27">2020</date>
			<biblScope unit="page" from="322" to="334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,333.39,575.27,224.81,6.18;18,333.16,583.24,225.04,6.18;18,333.39,591.16,140.69,6.25" xml:id="b10">
	<analytic>
		<title level="a" type="main">Towards Complete Icon Labeling in Mobile Applications</title>
		<author>
			<persName coords=""><forename type="first">Jieshan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amanda</forename><surname>Swearngin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Titus</forename><surname>Barik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><surname>Nichols</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaoyi</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3491102.3502073</idno>
		<idno type="arXiv">arXiv:2207.04165</idno>
	</analytic>
	<monogr>
		<title level="m">CHI Conference on Human Factors in Computing Systems</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2022-04-29">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="18,333.39,599.18,224.81,6.18;18,333.16,607.15,225.04,6.18;18,333.39,615.07,177.34,6.25" xml:id="b11">
	<analytic>
		<title level="a" type="main">Towards Complete Icon Labeling in Mobile Applications</title>
		<author>
			<persName coords=""><forename type="first">Jieshan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amanda</forename><surname>Swearngin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Titus</forename><surname>Barik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><surname>Nichols</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaoyi</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3491102.3502073</idno>
	</analytic>
	<monogr>
		<title level="m">CHI Conference on Human Factors in Computing Systems</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2022-04-29">2022</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,333.39,623.09,224.81,6.18;18,333.39,631.06,224.81,6.18;18,333.39,638.98,224.81,6.25;18,333.39,646.95,224.81,6.25;18,333.39,654.92,146.19,6.25" xml:id="b12">
	<analytic>
		<title level="a" type="main">Object detection for graphical user interface: old fashioned or deep learning or a combination?</title>
		<author>
			<persName coords=""><forename type="first">Jieshan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mulong</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhenchang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chunyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liming</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guoqiang</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1145/3368089.3409691</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering</title>
				<meeting>the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-11-08">2020</date>
			<biblScope unit="page" from="1202" to="1214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,333.39,662.94,225.99,6.18;18,333.39,670.86,194.35,6.25" xml:id="b13">
	<analytic>
		<title level="a" type="main">GreaseVision: Rewriting the Rules of the Interface</title>
		<author>
			<persName coords=""><forename type="first">Siddhartha</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Konrad</forename><surname>Kollnig</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nigel</forename><surname>Shadbolt</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.dadc-1.2</idno>
		<idno type="arXiv">arXiv:2204.03731</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Dynamic Adversarial Data Collection</title>
				<meeting>the First Workshop on Dynamic Adversarial Data Collection</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="18,333.39,678.88,225.58,6.18;18,333.39,686.85,224.81,6.18;18,333.39,694.77,224.81,6.25;18,333.39,702.74,223.34,6.25" xml:id="b14">
	<analytic>
		<title level="a" type="main">An Early Rico Retrospective: Three Years of Uses for a Mobile App Dataset</title>
		<author>
			<persName coords=""><forename type="first">Biplab</forename><surname>Deka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bardia</forename><surname>Doosti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Forrest</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chad</forename><surname>Franzen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joshua</forename><surname>Hibschman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Afergan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ranjitha</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><surname>Nichols</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-82681-9_8</idno>
	</analytic>
	<monogr>
		<title level="m">Human–Computer Interaction Series</title>
				<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="page" from="229" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,69.23,89.10,225.58,6.18;19,69.03,97.07,225.01,6.18;19,69.23,104.98,224.81,6.25;19,69.23,112.95,195.29,6.25" xml:id="b15">
	<analytic>
		<title level="a" type="main">Rico</title>
		<author>
			<persName coords=""><forename type="first">Biplab</forename><surname>Deka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zifeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chad</forename><surname>Franzen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joshua</forename><surname>Hibschman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Afergan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><surname>Nichols</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ranjitha</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="DOI">10.1145/3126594.3126651</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology</title>
				<meeting>the 30th Annual ACM Symposium on User Interface Software and Technology</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017-10-20">2017</date>
			<biblScope unit="page" from="845" to="854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,69.23,136.92,224.81,6.18;19,69.23,144.89,224.81,6.18;19,69.23,152.80,224.81,6.25;19,69.23,160.77,119.72,6.25" xml:id="b16">
	<analytic>
		<title level="a" type="main">UI Dark Patterns and Where to Find Them</title>
		<author>
			<persName coords=""><forename type="first">Linda</forename><surname>Di Geronimo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Larissa</forename><surname>Braz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Enrico</forename><surname>Fregnan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fabio</forename><surname>Palomba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alberto</forename><surname>Bacchelli</surname></persName>
		</author>
		<idno type="DOI">10.1145/3313831.3376600</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2020 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-04-21">2020</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,69.23,168.80,225.99,6.18;19,69.23,176.77,224.81,6.18;19,69.23,184.68,60.69,6.25" xml:id="b17">
	<analytic>
		<title level="a" type="main">A densitybased algorithm for discovering clusters in large spatial databases with noise</title>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hans-Peter</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jörg</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaowei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">kdd</title>
				<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="226" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,69.23,192.71,225.64,6.18;19,69.23,200.68,224.81,6.18;19,69.23,208.59,171.33,6.25" xml:id="b18">
	<analytic>
		<title level="a" type="main">The Distressing Ads That Persist: Uncovering The Harms of Targeted Weight-Loss Ads Among Users with Histories of Disordered Eating</title>
		<author>
			<persName coords=""><forename type="first">Liza</forename><surname>Gak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Seyi</forename><surname>Olojo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Niloufar</forename><surname>Salehi</surname></persName>
		</author>
		<idno type="DOI">10.1145/3555102</idno>
		<idno type="arXiv">arXiv:2204.03200</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Human-Computer Interaction</title>
		<title level="j" type="abbrev">Proc. ACM Hum.-Comput. Interact.</title>
		<idno type="ISSNe">2573-0142</idno>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">CSCW2</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2022-11-07">2022. 2022</date>
			<publisher>Association for Computing Machinery (ACM)</publisher>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="19,69.23,216.62,225.89,6.18;19,69.23,224.53,224.81,6.25;19,69.23,232.50,158.72,6.25" xml:id="b19">
	<analytic>
		<title level="a" type="main">The Dark (Patterns) Side of UX Design</title>
		<author>
			<persName coords=""><forename type="first">Colin</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yubo</forename><surname>Kou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bryan</forename><surname>Battles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joseph</forename><surname>Hoggatt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Austin</forename><forename type="middle">L</forename><surname>Toombs</surname></persName>
		</author>
		<idno type="DOI">10.1145/3173574.3174108</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2018 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018-04-21">2018</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,69.23,240.53,224.81,6.18;19,69.23,248.50,224.81,6.18;19,69.23,256.41,224.81,6.25;19,69.23,264.38,124.08,6.25" xml:id="b20">
	<analytic>
		<title level="a" type="main">Dark Patterns and the Legal Requirements of Consent Banners: An Interaction Criticism Perspective</title>
		<author>
			<persName coords=""><forename type="first">Colin</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cristiana</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nataliia</forename><surname>Bielova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Toth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Damian</forename><surname>Clifford</surname></persName>
		</author>
		<idno type="DOI">10.1145/3411764.3445779</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2021 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021-05-06">2021</date>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,69.23,272.41,224.81,6.18;19,69.23,280.38,224.81,6.18;19,69.23,288.29,225.58,6.25;19,69.23,296.32,59.50,6.18" xml:id="b21">
	<analytic>
		<title level="a" type="main">A Comparative Study of Dark Patterns Across Web and Mobile Modalities</title>
		<author>
			<persName coords=""><forename type="first">Johanna</forename><surname>Gunawan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amogh</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Choffnes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Woodrow</forename><surname>Hartzog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christo</forename><surname>Wilson</surname></persName>
		</author>
		<idno type="DOI">10.1145/3479521</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Human-Computer Interaction</title>
		<title level="j" type="abbrev">Proc. ACM Hum.-Comput. Interact.</title>
		<idno type="ISSNe">2573-0142</idno>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">CSCW2</biblScope>
			<biblScope unit="page" from="1" to="29" />
			<date type="published" when="2021-10-13">2021. 2021</date>
			<publisher>Association for Computing Machinery (ACM)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,69.23,304.29,224.81,6.18;19,69.23,312.26,224.81,6.18;19,69.23,320.17,224.97,6.25;19,69.03,328.20,35.49,6.18" xml:id="b22">
	<analytic>
		<title level="a" type="main">Identifying User Needs for Advertising Controls on Facebook</title>
		<author>
			<persName coords=""><forename type="first">Hana</forename><surname>Habib</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sarah</forename><surname>Pearman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ellie</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ishika</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lorrie</forename><forename type="middle">Faith</forename><surname>Cranor</surname></persName>
		</author>
		<idno type="DOI">10.1145/3512906</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Human-Computer Interaction</title>
		<title level="j" type="abbrev">Proc. ACM Hum.-Comput. Interact.</title>
		<idno type="ISSNe">2573-0142</idno>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">CSCW1</biblScope>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="2022-03-30">2022. 2022</date>
			<publisher>Association for Computing Machinery (ACM)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,69.23,336.17,224.81,6.18;19,69.23,344.08,224.81,6.25;19,69.23,352.05,110.81,6.25" xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2016.90</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-06">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,69.23,360.08,225.99,6.18;19,69.23,367.99,134.14,6.25" xml:id="b24">
	<analytic>
		<title level="a" type="main">Preprint repository arXiv achieves milestone million uploads</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Ba</surname></persName>
		</author>
		<idno type="DOI">10.1063/pt.5.028530</idno>
		<idno type="arXiv">arXiv:1412.6980</idno>
	</analytic>
	<monogr>
		<title level="j">Physics Today</title>
		<title level="j" type="abbrev">Phys. Today</title>
		<idno type="ISSNe">1945-0699</idno>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<publisher>AIP Publishing</publisher>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="19,69.23,376.02,224.81,6.18;19,69.03,383.93,225.02,6.25;19,69.23,391.90,205.58,6.25" xml:id="b25">
	<analytic>
		<title level="a" type="main">I Want My App That Way: Reclaiming Sovereignty Over Personal Devices</title>
		<author>
			<persName coords=""><forename type="first">Konrad</forename><surname>Kollnig</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Siddhartha</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Max</forename><surname>Van Kleek</surname></persName>
		</author>
		<idno type="DOI">10.1145/3411763.3451632</idno>
	</analytic>
	<monogr>
		<title level="m">Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021-05-08" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,69.23,399.93,224.81,6.18;19,69.23,407.84,224.81,6.25;19,69.23,415.81,177.34,6.25" xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to Denoise Raw Mobile UI Layouts for Improving Datasets at Scale</title>
		<author>
			<persName coords=""><forename type="first">Gang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gilles</forename><surname>Baechler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Manuel</forename><surname>Tragut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1145/3491102.3502042</idno>
	</analytic>
	<monogr>
		<title level="m">CHI Conference on Human Factors in Computing Systems</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2022-04-27">2022</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,69.23,423.84,225.88,6.18;19,69.23,431.81,225.88,6.18;19,69.23,439.72,224.81,6.25;19,69.23,447.69,65.64,6.25" xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-Modal Repairs of Conversational Breakdowns in Task-Oriented Dialogs</title>
		<author>
			<persName coords=""><forename type="first">Toby Jia-Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jingya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Haijun</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Brad</forename><forename type="middle">A</forename><surname>Myers</surname></persName>
		</author>
		<idno type="DOI">10.1145/3379337.3415820</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology</title>
				<meeting>the 33rd Annual ACM Symposium on User Interface Software and Technology</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-10-20">2020</date>
			<biblScope unit="page" from="1094" to="1107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,69.23,455.72,225.88,6.18;19,69.23,463.63,225.51,6.25;19,69.23,471.60,225.88,6.25;19,69.07,479.63,15.08,6.18" xml:id="b28">
	<analytic>
		<title level="a" type="main">Screen2Vec: Semantic Embedding of GUI Screens and GUI Components</title>
		<author>
			<persName coords=""><forename type="first">Toby Jia-Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lindsay</forename><surname>Popowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Brad</forename><forename type="middle">A</forename><surname>Myers</surname></persName>
		</author>
		<idno type="DOI">10.1145/3411764.3445049</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2021 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021-05-06">2021</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,69.23,487.60,224.81,6.18;19,69.23,495.57,224.81,6.18;19,69.23,503.49,223.03,6.25" xml:id="b29">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common Objects in Context</title>
		<author>
			<persName coords=""><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-10602-1_48</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision – ECCV 2014</title>
				<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,69.23,511.51,224.81,6.18;19,69.23,519.43,224.81,6.25;19,69.23,527.40,223.31,6.25" xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning design semantics for mobile apps</title>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Thomas F Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><surname>Craft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ersin</forename><surname>Situ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Radomir</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ranjitha</forename><surname>Mech</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology</title>
				<meeting>the 31st Annual ACM Symposium on User Interface Software and Technology</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="569" to="579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,69.23,535.42,225.88,6.18;19,69.23,543.34,224.81,6.25;19,69.23,551.31,225.88,6.25;19,69.23,559.33,41.81,6.18" xml:id="b31">
	<analytic>
		<title level="a" type="main">Owl eyes</title>
		<author>
			<persName coords=""><forename type="first">Zhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chunyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Junjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuekai</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jun</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qing</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3324884.3416547</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering</title>
				<meeting>the 35th IEEE/ACM International Conference on Automated Software Engineering</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-12-21">2020</date>
			<biblScope unit="page" from="398" to="409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,69.23,567.30,225.58,6.18;19,69.23,575.27,224.81,6.18;19,69.23,583.19,225.51,6.25;19,69.23,591.16,123.52,6.25" xml:id="b32">
	<analytic>
		<title level="a" type="main">Dark Patterns at Scale</title>
		<author>
			<persName coords=""><forename type="first">Arunesh</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gunes</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Lucherini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marshini</forename><surname>Chetty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arvind</forename><surname>Narayanan</surname></persName>
		</author>
		<idno type="DOI">10.1145/3359183</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Human-Computer Interaction</title>
		<title level="j" type="abbrev">Proc. ACM Hum.-Comput. Interact.</title>
		<idno type="ISSNe">2573-0142</idno>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">CSCW</biblScope>
			<biblScope unit="page" from="1" to="32" />
			<date type="published" when="2019-11-07">2019. 2019</date>
			<publisher>Association for Computing Machinery (ACM)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,69.23,599.18,225.11,6.18;19,69.23,607.15,224.81,6.18;19,68.99,615.07,225.05,6.25;19,69.23,623.04,117.76,6.25" xml:id="b33">
	<analytic>
		<title level="a" type="main">Freely Given Consent? Studying Consent Notice of Third-Party Tracking and Its Violations of GDPR in Android Apps</title>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Trung Tin Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Backes</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Stock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security</title>
				<meeting>the 2022 ACM SIGSAC Conference on Computer and Communications Security</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2369" to="2383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,69.23,631.01,224.97,6.25;19,68.99,639.03,169.14,6.18" xml:id="b34">
	<monogr>
		<title level="m" type="main">Reddit: When Assholes Design Things</title>
		<author>
			<persName coords=""><forename type="first">Asshole</forename><surname>Reddit</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Design</surname></persName>
		</author>
		<ptr target="https://www.reddit.com/r/assholedesign/" />
		<imprint>
			<date type="published" when="2014-08">2014. August 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,69.23,647.00,225.63,6.18;19,69.03,654.92,225.02,6.25;19,69.23,662.89,134.38,6.25" xml:id="b35">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName coords=""><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,69.23,670.91,224.81,6.18;19,69.23,678.88,224.81,6.18;19,69.23,686.80,189.86,6.25" xml:id="b36">
	<monogr>
		<title level="m" type="main">News at a glance: Asia’s COVID-19 surge, melting winter sea ice, and ‘inflammatory’ arXiv papers</title>
		<author>
			<persName coords=""><forename type="first">Than</forename><surname>Htut Soe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cristiana</forename><surname>Teixeira Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marija</forename><surname>Slavkovik</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.adb2060</idno>
		<idno type="arXiv">arXiv:2204.11836</idno>
		<imprint>
			<date type="published" when="2022-03-17">2022. 2022</date>
			<publisher>American Association for the Advancement of Science (AAAS)</publisher>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="19,69.23,694.82,224.81,6.18;19,68.99,702.74,225.05,6.25;19,333.39,403.82,177.34,6.25" xml:id="b37">
	<analytic>
		<title level="a" type="main">“Developers Are Responsible”: What Ad Networks Tell Developers About Privacy</title>
		<author>
			<persName coords=""><forename type="first">Mohammad</forename><surname>Tahaei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kami</forename><surname>Vaniea</surname></persName>
		</author>
		<idno type="DOI">10.1145/3411763.3451805</idno>
	</analytic>
	<monogr>
		<title level="m">Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021-05-08">2021</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,333.39,427.73,225.57,6.25;19,333.39,435.70,224.81,6.25;19,333.39,443.67,220.15,6.25" xml:id="b38">
	<monogr>
		<title level="m" type="main">A chrome extension that protects consumers from marketing tricks when they shop online</title>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Anthony Ribando</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nicholas</forename><surname>Tung</surname></persName>
		</author>
		<ptr target="https://devpost.com/software/insite-qfpjcd" />
		<imprint>
			<date type="published" when="2019-09-06">2019. 6 Sep 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,333.39,451.69,224.81,6.18;19,333.39,459.61,224.81,6.25;19,333.39,467.58,217.38,6.25" xml:id="b39">
	<analytic>
		<title level="a" type="main">Improving random GUI testing with image-based widget detection</title>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><surname>Thomas D White</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guy</forename><forename type="middle">J</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis</title>
				<meeting>the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="307" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,333.39,475.55,225.51,6.25;19,333.39,483.52,112.56,6.25" xml:id="b40">
	<analytic>
		<title level="a" type="main">Individual Comparisons by Ranking Methods</title>
		<author>
			<persName coords=""><forename type="first">Frank</forename><surname>Wilcoxon</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-4612-4380-9_16</idno>
	</analytic>
	<monogr>
		<title level="m">Springer Series in Statistics</title>
				<imprint>
			<publisher>Springer New York</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="196" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,333.39,491.54,225.57,6.18;19,333.39,499.51,224.81,6.18;19,333.39,507.48,225.89,6.18;19,333.39,515.40,225.88,6.25;19,333.23,523.43,15.08,6.18" xml:id="b41">
	<analytic>
		<title level="a" type="main">Screen Recognition: Creating Accessibility Metadata for Mobile Applications from Pixels</title>
		<author>
			<persName coords=""><forename type="first">Xiaoyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lilian</forename><surname>De Greef</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amanda</forename><surname>Swearngin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Samuel</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kyle</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lisa</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qi</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><surname>Nichols</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Fleizach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Everitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">P</forename><surname>Bigham</surname></persName>
		</author>
		<idno type="DOI">10.1145/3411764.3445186</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2021 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021-05-06">2021</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,333.39,531.40,225.58,6.18;19,333.39,539.37,224.81,6.18;19,333.39,547.28,225.88,6.25;19,333.39,555.31,31.30,6.18" xml:id="b42">
	<analytic>
		<title level="a" type="main">EAST: An Efficient and Accurate Scene Text Detector</title>
		<author>
			<persName coords=""><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">He</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shuchang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Weiran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiajun</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2017.283</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-07">2017</date>
			<biblScope unit="page" from="5551" to="5560" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
