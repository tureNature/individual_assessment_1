<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AidUI: Toward Automated Recognition of Dark Patterns in User Interfaces</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-03-12">12 Mar 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,69.99,123.38,85.40,9.50"><forename type="first">S</forename><forename type="middle">M</forename><surname>Hasan Mansur</surname></persName>
						</author>
						<author>
							<persName coords="1,164.33,123.38,61.06,9.50"><forename type="first">Sabiha</forename><surname>Salma</surname></persName>
							<email>ssalma@gmu.edu</email>
						</author>
						<author>
							<persName coords="1,267.76,123.38,90.99,9.50;1,358.76,121.11,1.88,6.99"><forename type="first">Damilola</forename><surname>Awofisayo</surname></persName>
							<email>dami.awofisayo@duke.edu</email>
						</author>
						<author>
							<persName coords="1,444.06,123.38,59.29,9.50"><forename type="first">Kevin</forename><surname>Moran</surname></persName>
							<email>kpmoran@gmu.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">George Mason University</orgName>
								<address>
									<settlement>Fairfax</settlement>
									<region>VA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Duke University Durham</orgName>
								<address>
									<region>NC</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">George Mason University</orgName>
								<address>
									<settlement>Fairfax</settlement>
									<region>VA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">AidUI: Toward Automated Recognition of Dark Patterns in User Interfaces</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-03-12">12 Mar 2023</date>
						</imprint>
					</monogr>
					<idno type="MD5">F78BF35C7E337EB7CF33233C541F0395</idno>
					<idno type="arXiv">arXiv:2303.06782v1[cs.SE]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2024-03-02T21:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Dark Pattern</term>
					<term>UI Analysis</term>
					<term>UI Design</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Past studies have illustrated the prevalence of UI dark patterns, or user interfaces that can lead end-users toward (unknowingly) taking actions that they may not have intended. Such deceptive UI designs can be either intentional (to benefit an online service) or unintentional (through complicit design practices) and can result in adverse effects on end users, such as oversharing personal information or financial loss. While significant research progress has been made toward the development of dark pattern taxonomies across different software domains, developers and users currently lack guidance to help recognize, avoid, and navigate these often subtle design motifs. However, automated recognition of dark patterns is a challenging task, as the instantiation of a single type of pattern can take many forms, leading to significant variability.</p><p>In this paper, we take the first step toward understanding the extent to which common UI dark patterns can be automatically recognized in modern software applications. To do this, we introduce AIDUI, a novel automated approach that uses computer vision and natural language processing techniques to recognize a set of visual and textual cues in application screenshots that signify the presence of ten unique UI dark patterns, allowing for their detection, classification, and localization. To evaluate our approach, we have constructed CONTEXTDP, the current largest dataset of fully-localized UI dark patterns that spans 175 mobile and 83 web UI screenshots containing 301 dark pattern instances. The results of our evaluation illustrate that AIDUI achieves an overall precision of 0.66, recall of 0.67, F1-score of 0.65 in detecting dark pattern instances, reports few false positives, and is able to localize detected patterns with an IoU score of 0.84. Furthermore, a significant subset of our studied dark patterns can be detected quite reliably (F1 score of over 0.82), and future research directions may allow for improved detection of additional patterns. This work demonstrates the plausibility of developing tools to aid developers in recognizing and appropriately rectifying deceptive UI patterns.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Modern user interfaces (UIs) have unprecedented influence on the daily lives of users due to the increasing digitization of common tasks -ranging from financial transactions to shopping. As such, an emphasis on ease of use in the design of these interfaces has never been more critical. However, while UI designers typically strive to create interfaces that facilitate seamless completion of computing tasks, they are Fig. <ref type="figure" coords="1,326.83,537.85,2.99,6.91">1</ref>. This paper presents (i) a unified taxonomy of UI dark patterns, (ii) the CONTEXTDP dataset which contains 501 screenshots depicting 301 DP and 243 non-DP instances, and (iii) the AIDUI approach, which is able to identify and process visual and textual cues to detect and localize dark patterns. also capable of creating interfaces that may subtly mislead users into performing tasks they did not intend, but which may benefit business stakeholders. This dichotomy is influenced by competing design pressures: (i) designing easy to use UIs may increase the reputation and overall market share of an application; and (ii) designing UIs that intentionally influence users into performing certain actions may benefit the goals of application stakeholders.</p><p>This deceptive side of UI design has received increasing attention from various research communities in recent years, and has led to an increased shared understanding of a phe- nomenon referred to as dark patterns (DPs). While the notion of DPs appeared in research literature as early as 2010 <ref type="bibr" coords="2,280.93,256.39,15.27,8.64" target="#b10">[13]</ref>, Mathur et al. <ref type="bibr" coords="2,109.68,268.35,16.60,8.64" target="#b34">[37]</ref> recently defined dark patterns as:</p><p>"User interface design choices that benefit an online service by coercing, steering, or deceiving users into making decisions that, if fully informed and capable of selecting alternatives, they might not make".</p><p>To further illustrate the concept of a dark pattern, we provide an illustrative example of the Attention Distraction pattern in Figure <ref type="figure" coords="2,112.02,362.63,4.98,8.64" target="#fig_0">2</ref> where the size and color of a UI element is used to draw the user's attention and influence them into making a particular choice. In our example, as illustrated in Figure <ref type="figure" coords="2,77.91,398.49,3.74,8.64" target="#fig_0">2</ref>, there is a significant size difference between the text "Continue" &amp; "skip seat selection" options, making it more likely that a user might pay for a seat as opposed to forgoing the selection and being automatically assigned one for free.</p><p>There is ongoing work from the human-computer interaction (HCI) community into understanding the ethical considerations <ref type="bibr" coords="2,99.17,470.54,15.77,8.64" target="#b21">[24,</ref><ref type="bibr" coords="2,116.60,470.54,11.83,8.64" target="#b22">25]</ref>, constructions <ref type="bibr" coords="2,199.57,470.54,15.77,8.64" target="#b9">[12,</ref><ref type="bibr" coords="2,217.00,470.54,12.45,8.64" target="#b21">24,</ref><ref type="bibr" coords="2,231.11,470.54,12.45,8.64" target="#b23">26,</ref><ref type="bibr" coords="2,245.23,470.54,12.45,8.64" target="#b26">29,</ref><ref type="bibr" coords="2,259.34,470.54,12.45,8.64" target="#b27">30,</ref><ref type="bibr" coords="2,273.45,470.54,12.45,8.64" target="#b49">51,</ref><ref type="bibr" coords="2,287.57,470.54,12.45,8.64" target="#b50">52,</ref><ref type="bibr" coords="2,48.96,482.50,11.83,8.64" target="#b54">56]</ref>, user perspectives <ref type="bibr" coords="2,144.42,482.50,15.77,8.64" target="#b13">[16,</ref><ref type="bibr" coords="2,161.85,482.50,12.45,8.64" target="#b21">24,</ref><ref type="bibr" coords="2,175.96,482.50,11.83,8.64" target="#b33">36]</ref>, and practitioner perspectives <ref type="bibr" coords="2,70.51,494.45,16.60,8.64" target="#b22">[25]</ref> of DPs. Additionally, the community has worked to construct evidence-based taxonomies of generally agreed upon DPs present in modern UIs <ref type="bibr" coords="2,167.19,518.36,128.89,8.64">[12, 24-26, 29, 30, 37, 51, 52, 56]</ref>. The prevalence of these identified categories of DPs is undeniable, as recent studies have illustrated that nearly 11% of top shopping websites <ref type="bibr" coords="2,127.74,554.23,16.60,8.64" target="#b34">[37]</ref> and nearly 95% of the top 240 apps on Google Play <ref type="bibr" coords="2,117.92,566.18,16.60,8.64" target="#b15">[18]</ref> contain one or more defined DPs. It is important to note that such DPs may not always be the result of ill intent from a designers vantage point, and past work has suggested that the current prevalence of such patterns may be the result of complicit, and perhaps unintentional, design practices on the part of software designers and developers <ref type="bibr" coords="2,280.93,625.96,15.27,8.64" target="#b22">[25]</ref>.</p><p>However, regardless of the reason for their introduction and continued use, it is clear that deceptive UI patterns can have a negative effect on end-users. Such effects have been documented by prior work illustrating that, in many cases, DPs can result in financial loss <ref type="bibr" coords="2,192.38,686.05,10.79,8.64" target="#b6">[9,</ref><ref type="bibr" coords="2,204.83,686.05,11.83,8.64" target="#b24">27]</ref>, or in oversharing personal information <ref type="bibr" coords="2,141.07,698.00,15.77,8.64" target="#b16">[19,</ref><ref type="bibr" coords="2,158.50,698.00,11.83,8.64" target="#b54">56]</ref>. Additionally, a prior human study that examined end-user perception of DPs in mobile apps found that most users cannot recognize DPs <ref type="bibr" coords="2,516.68,52.43,15.27,8.64" target="#b15">[18]</ref>. Given the prevalence and potential negative effects of deceptive UI patterns, and past evidence that designers and developers may unintentionally introduce such designs into their apps <ref type="bibr" coords="2,543.95,88.29,15.27,8.64" target="#b22">[25]</ref>, developer-facing tools that can automatically recognize and signal the presence of such patterns could aid in avoiding these deceptive designs and improve overall software quality.</p><p>Despite the potential benefit of automated techniques for detecting DPs, there are two key challenges related to the design and implementation of such an approach. First, while the research community has made considerable progress in defining various types of DPs across a number of domains <ref type="bibr" coords="2,547.26,183.93,15.77,8.64" target="#b10">[13,</ref><ref type="bibr" coords="2,311.98,195.89,12.45,8.64" target="#b15">18,</ref><ref type="bibr" coords="2,326.10,195.89,12.45,8.64" target="#b22">25,</ref><ref type="bibr" coords="2,340.20,195.89,11.83,8.64" target="#b34">37]</ref>, there can still be significant variability in how these patterns are instantiated in software applications. That is, while the very etymology of the phrase "dark pattern" suggests the presence of strong semantic signals that characterize different deceptive UI designs, the actual implementation of such designs can vary significantly. This makes designing an approach to detect such patterns difficult. Second, the research community currently lacks a large dataset of DPs with fine-grained localization information mapped to a unified taxonomy of DPs. While prior work has led to the creation of existing datasets, such as the one produced by Mathur et al. <ref type="bibr" coords="2,327.86,327.39,16.60,8.64" target="#b34">[37]</ref> related to online shopping, these datasets do not contain localized DPs (e.g., bounding boxes that denote the location and spatial properties of the DPs) and such datasets are typically created with domain-specific categories of DPs.</p><p>To address the need for the automation of the detection of DPs in UIs, we introduce AIDUI (Aid for detecting UI Dark Patterns) -an approach that conducts textual, icon, color, and spatial analysis of a UI to automatically detect the presence of underlying DPs. The key idea underlying AidUI is that there exist several visual and textual cues, that when (co)-appearing, signify the presence of various dark patterns. By detecting these individual cues, and analyzing their (co)occurrence, AIDUI is able to overcome challenges related to the variability of dark patterns as they appear "in-the-wild". AIDUI is the first approach to attempt to detect DPs using a fully automated process, and performs cue detection using a combination of computer vision and natural language template matching techniques. AIDUI operates solely on visual data, requiring only a screenshot of a user interface as input, making it easily extensible to multiple software domains.</p><p>In the process of building and evaluating AIDUI we have constructed CONTEXTDP, the current largest dataset of fully-localized DP instances to UI screenshots, containing 258 screenshots with 301 DP instances. The DP instances map to a unified set of ten DP categories derived through merging common DP categories described in previous taxonomies <ref type="bibr" coords="2,349.91,638.23,15.77,8.64" target="#b10">[13,</ref><ref type="bibr" coords="2,367.33,638.23,12.45,8.64" target="#b22">25,</ref><ref type="bibr" coords="2,381.45,638.23,11.83,8.64" target="#b34">37]</ref>. This dataset spans multiple application domains, including 175 mobile app screenshots and 83 Web UI screenshots, for a total of 258 screenshots depicting 301 DPs. Additionally, we augment our dataset with a set of 243 screenshots (164 mobile &amp; 79 web) that do not contain DPs to investigate the likelihood of AidUI to detect false positives.</p><p>We conducted a comprehensive evaluation to measure the Nagging (57)</p><p>User's expected workflow is interrupted one or more times with unrelated events or interactions to make the user perform certain actions.</p><p>Nagging ( <ref type="formula" coords="3,86.65,97.85,5.74,8.30">57</ref>)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Obstruction (-)</head><p>The workflow is intentionally made more difficult for users to make them do certain actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intermediate Currency (-)</head><p>Roach Motel (-)</p><p>Price Comparison Prevention (-)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sneaking (-)</head><p>Hide, disguise, and delay the relevant information from the user.</p><formula xml:id="formula_0" coords="3,55.86,265.83,71.52,27.58">Bait &amp; Switch (-) Sneak into Basket (-)</formula><p>Hidden Cost (-)</p><p>Forced Continuity (-)</p><p>Forced Action <ref type="bibr" coords="3,277.78,46.39,12.77,9.65" target="#b8">(11)</ref> Requiring the user to perform a certain action to access (or continue to access) functionality</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Privacy Zuckering (-)</head><p>Social Pyramid (-)</p><p>Gamification <ref type="bibr" coords="3,270.13,120.08,10.83,8.30" target="#b8">(11)</ref> Forced Enrollment (-)</p><p>Urgency/Scarcity (78)</p><p>Accelerate user decision making and purchases by presenting limited availability, high demand, or a sale deadline</p><p>Countdown Timer <ref type="bibr" coords="3,287.68,207.22,12.40,8.30" target="#b25">(28)</ref> Limited Time Message <ref type="bibr" coords="3,302.20,226.50,12.27,8.30" target="#b23">(26)</ref> Low Stock Message <ref type="bibr" coords="3,291.68,245.78,11.61,8.30" target="#b16">(19)</ref> High Demand Message <ref type="bibr" coords="3,303.39,265.06,8.41,8.30" target="#b2">(5)</ref> Social Proof <ref type="bibr" coords="3,444.36,271.35,13.94,9.65" target="#b7">(10)</ref> Accelerate user decision making by using the social media influence of other users.</p><p>Activity Message <ref type="bibr" coords="3,455.81,306.00,11.85,8.30" target="#b7">(10)</ref> Testamonials (-)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Misdirection (145)</head><p>Use of visuals, language, and emotion to intrigue users into making a particular choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Default Choice (111)</head><p>Pressured Selling (-)</p><p>Toying with Emotion (-)</p><p>Trick Questions(-)</p><p>Friend Spam (-)</p><p>Disguised Ads <ref type="bibr" coords="3,446.31,186.80,11.35,8.30" target="#b18">(21)</ref> False Hierarchy (-)</p><p>Attention Distraction <ref type="bibr" coords="3,469.37,225.37,11.42,8.30" target="#b10">(13)</ref> Hidden Information (-)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unified DP Taxonomy</head><p>Subcategories in Blue come from the work of Gray et. al. <ref type="bibr" coords="3,244.70,307.81,10.58,8.29" target="#b22">[25]</ref>, subcategories in Green, come from the work of Bignull et. al. <ref type="bibr" coords="3,295.66,316.26,10.05,8.29" target="#b10">[13]</ref>, and subcategories in Red come from work of Mathur et. al. <ref type="bibr" coords="3,348.08,326.40,10.30,8.29" target="#b34">[37]</ref>. effectiveness and robustness of AIDUI. Using CONTEXTDP, we measured the precision, recall and F1-score of AIDUI's DP detection capabilities, as well as its localization performance in different settings. Additionally, we performed an ablation study to examine which components of our technique contributed most to the overall detection performance. We found that, across all DP instances in our dataset, AIDUI achieved an overall average precision of 0.66, average recall of 0.67 and average F1-score of 0.65. However, for five DPs, AIDUI was able to achieve higher precision (0.87), recall (0.80) and F1-score (0.82) compared to the five other DP categories, illustrating the variance in difficulty of detecting different types of patterns. Our analysis of localization performance illustrates that AidUI can provide useful localization that is specific to the various salient patterns it detects. Finally, our ablation study illustrated that combining text analysis with the color and spatial analysis led to the highest performance. In summary, the contributions of our work are as follows:</p><p>• AIDUI, the first automated approach capable of detecting the presence of a diverse set of DPs. To accomplish this, the proposed approach adapts techniques from natural language processing and computer vision to detect various cues that signify dark patterns. • CONTEXTDP, which is the current largest dataset of DP instances localized to UI screenshots. CONTEXTDP contains 162 web and 339 mobile screenshots depicting 301 DP and 243 Non-DP instances.</p><p>• A comprehensive empirical evaluation of AIDUI that measures the precision, recall, F1-score, localization performance, and the robustness of the approach. Our evaluation illustrates that AIDUI can detect DPs from a subset of our studied DP categories with higher precision, recall and F1-score, with useful localization results. • Online appendices <ref type="bibr" coords="3,411.19,470.53,8.63,8.64" target="#b3">[6]</ref><ref type="bibr" coords="3,419.82,470.53,4.32,8.64" target="#b4">[7]</ref><ref type="bibr" coords="3,424.13,470.53,8.63,8.64" target="#b5">[8]</ref> containing source code, experimental data, and trained models that can facilitate the replication of our results and encourage future work on automated detection of UI DPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. UNIFYING WEB AND MOBILE DARK PATTERNS</head><p>There has been a wealth of work from the general HCI community that has constructed DP taxonomies. One of the earliest taxonomies comes from Brignull <ref type="bibr" coords="3,486.04,564.27,16.60,8.64" target="#b10">[13]</ref> who not only coined the term "Dark Pattern", but also proposed a classification of DPs into different categories. The DP categories originally derived by Brignull et al. <ref type="bibr" coords="3,472.80,600.13,16.60,8.64" target="#b10">[13]</ref>   The authors analyzed pervasiveness of DPs in mobile applications using an approach similar to cognitive walk through techniques <ref type="bibr" coords="4,130.93,264.56,16.60,8.64" target="#b39">[42]</ref> whereas previous works <ref type="bibr" coords="4,255.20,264.56,15.77,8.64" target="#b22">[25,</ref><ref type="bibr" coords="4,272.62,264.56,12.45,8.64" target="#b34">37,</ref><ref type="bibr" coords="4,286.74,264.56,13.28,8.64" target="#b37">40]</ref> analyzed screenshots to classify DPs. Finally, Mathur et al.</p><p>[37] performed a semi-automated, large-scale collection of DPs in online shopping websites, and derived a taxonomy of 15 dark patterns grouped into 7 categories. In the course of their data collection, they found 1,818 instances of DPs on the top ≈11k shopping websites. More recent work has aimed to identify DPs that are specific to various different contexts or domains including (i) shopping web apps <ref type="bibr" coords="4,265.84,360.20,15.27,8.64" target="#b34">[37]</ref>, (ii) computer games <ref type="bibr" coords="4,119.69,372.15,15.27,8.64" target="#b54">[56]</ref>, (iii) privacy-centric software <ref type="bibr" coords="4,262.95,372.15,15.27,8.64" target="#b9">[12]</ref>, (iv) robotics <ref type="bibr" coords="4,84.55,384.11,15.27,8.64" target="#b26">[29]</ref>, and (v) digital consent forms <ref type="bibr" coords="4,229.71,384.11,15.27,8.64" target="#b49">[51]</ref>.</p><p>Given the somewhat complementary, yet disparate nature of existing taxonomies of DPs, we aimed to merge similar DP categories from existing taxonomies together and provide a larger landscape of patterns for mobile and web apps toward which we can design and evaluate our automated detection approach. Given that the scope of our work is primarily concerned with web and mobile UIs, we did not include many of the domain specific taxonomies mentioned above. Instead, our unified taxonomy is primarily a fusing of the various categories and subcategories derived by Gray et al. <ref type="bibr" coords="4,280.93,505.19,15.27,8.64" target="#b22">[25]</ref>, Mathur et al. <ref type="bibr" coords="4,103.74,517.15,16.60,8.64" target="#b34">[37]</ref> and Brignull et al. <ref type="bibr" coords="4,198.74,517.15,15.27,8.64" target="#b10">[13]</ref>. To build our unified taxonomy, first one author gathered one-two existing examples of each type of DPs that exists in the categories described by Gray et al. <ref type="bibr" coords="4,119.62,553.01,15.27,8.64" target="#b22">[25]</ref>, Mathur et al. <ref type="bibr" coords="4,208.81,553.01,15.27,8.64" target="#b34">[37]</ref>, and Birgnull et al. <ref type="bibr" coords="4,61.71,564.97,15.27,8.64" target="#b10">[13]</ref>. Next, two authors met to review each DP example and re-group the various categories under unified headings. Our final unified taxonomy, illustrated in Figure <ref type="figure" coords="4,230.24,588.88,3.74,8.64" target="#fig_1">3</ref>, spans 7 parent categories which organize a total of 27 classes that describe different DPs. Note that many of the DPs in our taxonomy are self-documenting (e.g., countdown timer), however, we provide full descriptions and examples of each DP in our online appendices <ref type="bibr" coords="4,125.11,648.65,8.02,8.64" target="#b3">[6]</ref><ref type="bibr" coords="4,133.13,648.65,4.01,8.64" target="#b4">[7]</ref><ref type="bibr" coords="4,137.14,648.65,8.02,8.64" target="#b5">[8]</ref>.</p><p>Not all dark patterns are created equal from viewpoint of the underlying UI motifs that signal their presence. For example, for the Sneak Into Basket DP type, typically there would be several actions and screens required to detect the presence of such a pattern, which introduces far more variability in its potential observed visual and textual cues. With this in mind we aimed to prioritize the detection strategy of AIDUI toward certain patterns that carry with them distinct visual and textual cues which both manifest on a single screen. We leave the detection of dynamic DPs involving multiple screens and actions for future work. To perform this prioritization, two authors met and further discussed the examples of each of the 27 classes of DPs and marked those that exhibited salient visual and textual cues, noting these cues for use in the later implementation of AIDUI. Thus, we identified a final set of 10 target DPs , toward which we oriented AIDUI'S analysis. The targeted DP categories are marked with a in Figure <ref type="figure" coords="4,552.78,348.24,3.74,8.64" target="#fig_1">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE AIDUI APPROACH</head><p>This section presents the AIDUI approach for automatically detecting DPs in UIs. The architecture of AIDUI, depicted in Figure <ref type="figure" coords="4,341.48,409.89,3.74,8.64">4</ref>, is designed around four main phases: 1 the Visual Cue Detection phase, which leverages a deep learning based object detection model to identify UI objects representing visual cues for DPs, 2 the UI &amp; Text Content Detection phase, which extracts UI segments containing both text and non-text content, 3 the DP Analysis phase, which employs text pattern matching, as well as color and spatial analysis techniques to analyze the extracted UI segments and identify a set of potential DPs, and 4 the DP Resolution phase, which uses results from both Visual Cue Detection and DP Analysis phases to predict a final set of underlying DPs in a given UI. It is important to note that AIDUI operates purely on pixel-based data from UI screenshots, making it extensible to different software domains. In the remainder of this section, we first discuss the motivation of the architecture of our approach and then discuss each of the four phases in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Approach Motivation</head><p>While studying existing DP taxonomies, we observed that different DP categories tend to include certain types of icons and text as well as exhibit distinct patterns in color brightness and spatial organization of UI elements. For example, instances from nagging category are likely to have both visual (i.e., rating related icons such as like buttons, stars, etc.) and textual cues (i.e., keywords such as "rate", "rating" etc.). Based on these observations regarding the visual and textual cues across different DP categories, we have identified five tasks that are required for detecting DPs. These include two detection tasks (i.e., visual cue detection and text content detection) as well as three analysis tasks related to properties of UI components (i.e., text, color, and spatial analysis). The major phases of our approach are designed around these detection and analysis tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Phase 1: Visual Cue Detection</head><p>The main goal of this phase is to identify the icons that can serve as important visual cues for detecting DPs. To accomplish this goal, our approach leverages deep learning based object detection techniques. In this case, we adapt an implementation of Faster R-CNN <ref type="bibr" coords="5,191.43,207.37,16.60,8.64" target="#b45">[47]</ref> to accurately identify and localize specific types of icons in a target UI.</p><p>The Visual Cue Detection phase receives a UI as an input, then uses Faster R-CNN model to detect the positions and bounding boxes of the target icons. Finally, through a mapping of detected icons to likely candidate DPs, it provides a list of potential DP categories. The output of this phase is a json file consisting of bounding boxes and DP categories for detected icons with the highest confidence scores.</p><p>1) Icon Detection: Typically, DL models such as the Faster R-CNN model are data hungry and need to be trained with manually labeled, large datasets. However, for the visual cues that we wish to detect (e.g., rating bars, like buttons, etc.), we only have a small set of examples. Labeling such UI components in existing UI datasets, such as RICO <ref type="bibr" coords="5,253.36,375.08,15.27,8.64" target="#b14">[17]</ref>, would be extremely time consuming, and hence would not scale. As such, we develop a fully-automated training data generation technique that allows for the creation of large sets of training data for our target visual cues. This training data generation technique has been successfully used in past work <ref type="bibr" coords="5,256.54,434.86,16.60,8.64" target="#b8">[11]</ref> to detect touch indicators on mobile UI screenshots with extremely high accuracy (e.g., 98%). It should be noted that this data generation procedure is completely independent and separate from the CONTEXTDP dataset. That is, we automatically generate the dataset to train our neural icon detection model such that no screens or apps used in the training of this model appear in the CONTEXTDP dataset.</p><p>We used the existing large scale RICO <ref type="bibr" coords="5,234.58,530.63,16.60,8.64" target="#b14">[17]</ref> UI dataset to aid in our data generation process. RICO, by far the largest repository of mobile app designs, contains 66k unique UI screens from 9.3k free Android apps spanning over 27 categories. First, we randomly sampled 1020 unique UIs of different apps from the RICO dataset. NExt, we randomly sampled different versions of the icons that we target to detect as visual cues which are freely available on icon or UI design websites. In total, we collect total 16 different versions of 6 icon types (i.e., 2 Google Ad icons, 3 loading icons, 3 like icons, 3 dislike icons, 2 star icons, and 3 toggle switch icons). Next, we programmatically overlay the icons on the UIs to create a dataset consisting of total 16320 images (i.e., 16 icons x 1020 UIs) and corresponding annotations in MSCOCO <ref type="bibr" coords="5,95.97,698.00,16.60,8.64" target="#b28">[31]</ref> format. Note that past work has illustrated a dataset of ≈ 16k screenshots to be sufficient to train a neural object detection approach to detect icons <ref type="bibr" coords="5,515.50,157.78,15.27,8.64" target="#b8">[11]</ref>. While creating synthesized data, we place randomly sized icons (i.e., foreground image) at random locations on the UIs (i.e., background image). To create training and validation sets, we split the dataset into 80%/20% respectively. We then use the open source machine learning framework PyTorch [3] and Torchvision <ref type="bibr" coords="5,364.80,229.51,11.62,8.64" target="#b2">[5]</ref> library to adapt an implementation of the Faster R-CNN <ref type="bibr" coords="5,372.90,241.47,16.60,8.64" target="#b45">[47]</ref> object detection model. Our trained model achieves an overall accuracy of 91% on our held out test set.</p><p>The output of the object detection model at inference time for a given input screenshot is at least one, and potentially multiple, bounding boxes of identified icons, where the model assigns each bounding box a list of potential icon categories ranked by confidence level. AidUI uses all identified bounding boxes for a given screenshot, and uses the category with the highest confidence score from the model to identify the icon category for each bounding box.</p><p>2) Mapping Detected Icons to DPs: As stated earlier in this section, some icons tend to relate to different types of DPs. Based on this observation, we translate the relationship between certain icons and DPs into a set of mapping rules (illustrated in Table <ref type="table" coords="5,398.82,410.65,3.04,8.64" target="#tab_1">I</ref>). Our approach uses these predefined rules for mapping the detected icon to potential DP(s). Thus, as a final output of Visual Cue Detection phase, a JSON file is produced that includes labels, bounding boxes and confidence scores of the detected icon(s) as well as a set of related DPs that are likely to be present in the UI. It should be noted that, AIDUI never uses only icon detection to determine whether there is a DP, but instead combines icon detection with the other visual and textual cues(section III-D) for DP prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Phase 2: UI &amp; Text Content Detection</head><p>To identify the general UI components and textual cues related to DPs, we first need to extract the UI segments that have textual content. To accomplish this, we adapted the implementation of the approach introduced by Xie et al. <ref type="bibr" coords="5,543.95,590.41,15.27,8.64" target="#b52">[54]</ref>, called UIED, that consists of two parts for detecting graphical and textual UI elements. For text detection and extraction, it leverages the EAST text detection model <ref type="bibr" coords="5,485.52,626.27,15.27,8.64" target="#b57">[59]</ref>. For graphical UI elements, it uses an unsupervised edge detection algorithm and CNN to locate and classify elements. As we are interested in extracting UI segments with text contents, our approach collects the OCR'd output in JSON format from UIED. The JSON file contains the text contents along with the corresponding bounding box information and serves as an input for the next phase (i.e., DP Analysis). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Phase 3: DP Analysis</head><p>In DP Analysis phase, the main goal is to apply different analysis techniques on the UI segments containing textual information to predict the potential underlying DPs associated with those segments. As stated earlier in section III-A, different DP categories tend to exhibit certain textual, visual and spatial patterns. Hence, DP Analysis phase incorporates different techniques for analyzing text, color, and spatial information extracted from the identified UI segments.</p><p>First, the UI text segments are provided as inputs. Then, text analysis is used to select the segments that have text contents showing patterns related to different DP categories. Next, color analysis is performed to categorize these selected segments based on their brightness level. Finally, in the spatial analysis, relative size information of neighbor segments is computed. The final output of the DP Analysis phase is a JSON file that combines the results from textual, color, and spatial analysis of the UI segments which is later analyzed during DP Resolution.</p><p>1) Text Analysis: To select the UI segments exhibiting specific textual cues, our approach leverages a pattern matching technique. Based on the observed textual cues for different DP categories, we define heuristic pattern matching rules. We defined corresponding lexical patterns for each of the targeted 10 DP categories that match keywords as well as sentence patterns. We provide a subset of these lexical patterns in Table <ref type="table" coords="6,89.56,493.75,6.64,8.64" target="#tab_2">II</ref> and provide a detailed list of these keywords and patterns in our online appendices <ref type="bibr" coords="6,210.63,505.70,8.02,8.64" target="#b3">[6]</ref><ref type="bibr" coords="6,218.65,505.70,4.01,8.64" target="#b4">[7]</ref><ref type="bibr" coords="6,222.66,505.70,8.02,8.64" target="#b5">[8]</ref>. When a match occurs, our approach returns a JSON object containing the segment information (i.e., UI coordinates, width, height etc.) of the matched text content and the corresponding DP category related to the pattern rule used. In the case that there are multiple matched contents under the same DP category, the content with the longest sequence is selected. We implement our linguistic pattern matching using spaCy [4], python library.</p><p>2) Color Analysis: Next, in the Color Analysis step, we categorize the detected segments from Text Analysis step based on their brightness. To accomplish this, our approach uses a color histogram analysis technique. In this analysis process, we first calculate the grayscale histogram of the segment where we use two bins, one for the intensity values in the range of 0-127 and another one for the intensity values ranging from 128-255. If 65% or more of the pixels have values in the range of 0-127, the segment is categorized as "darker" segment, whereas if 65% or more of the pixels have values in the range of 128-255, the segment is considered as a "brighter" one. In all other cases, the segment is categorized as a "normal" one. The reason for performing this classification of color intensity is that there are some DP types (such as Attention Distraction and Default Choice) wherein orthogonal brightness/contrast levels are likely to signal the presence of a DP. To implement color analysis, we use the Python API for OpenCV <ref type="bibr" coords="6,388.28,136.11,11.62,8.64" target="#b1">[2]</ref> to perform a color histogram analysis that provides the brightness measure mentioned earlier. For each segment, the Color Analysis step outputs a JSON object containing the histogram results and the associated brightness category (i.e., darker/brighter/normal).</p><p>3) Spatial Analysis: Some DP categories (such as Attention Distraction and Default Choice) are signified by size differences between their nearest components. Hence, in the Spatial Analysis step, our main goal is to find the neighbor segments around a given segment and compute the relative size of a segment in comparison to the size of the neighbors. Here, our approach conducts a two step spatial analysis. First, for a given segment, we calculate the neighborhood area around it by adding a proximity factor to the segment boundaries. The proximity factor is calculated as a very small percentage (e.g., ≈ 5%) of the segment size, which is derived empirically. If any other segment's boundary intersects with the boundary of the neighborhood, that segment is then considered as a neighbor of the current segment being analyzed. Once the neighbors are found, the second step is to compute the relative width and height of a segment with regard to the width and height of its neighbors. To do this, each segment's width and height are divided by the maximum width and height found in the neighborhood. Finally, for each segment, the Spatial Analysis step outputs a JSON object containing the information regarding neighborhood coordinates, neighbor segments and relative size (width and height).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Phase 4: DP Resolution</head><p>The final phase of our approach is the DP Resolution phase where our main goal is to identify the underlying DPs that are most likely to be present in the given UI. DP Resolution is a two step process, consisting of Segment Level Resolution and UI Level Resolution. In Segment Level Resolution, our approach identifies potential DPs in UI segments by considering the textual, color, and spatial analysis results (section III-D). In the UI Level Resolution, results from both Visual Cue Detection (section III-B) and Segment Level Resolution (section III-E1) are used to to predict a final set of underlying DPs in the UI. Localization is performed using the bounding boxes of the identified UI elements from the screen.</p><p>1) Segment Level Resolution: To identify potential DPs in a segment, we take into consideration the results from textual, color, and spatial analyses (section III-D). We employ a voting mechanism among the neighbor segments to resolve the most likely DPs at the segment level. In this process, a UI segment gets votes for a DP category from its neighbor segments if some specific textual, color, or spatial criteria are met. For instance, in case of text based resolution, a segment gets a vote from a neighbor segment if both of them exhibit textual patterns related to a similar DP. In the visual resolution, a segment gets a vote from its neighbor if the color brightness of the segment and the neighbor is opposite (i.e., brighter/darker). In the spatial resolution, a segment gets a vote from a neighbor if the difference of the relative height or width between the segment and the neighbor is more than a predefined threshold value. In the voting process, we not only calculate the number of votes but also compute a score for the earned votes. The score of the votes depends on the number of factors or cues satisfied in textual, color, and spatial resolution processes.</p><p>In summary, the voting process described above is akin to an equally weighted score across four features: (i) whether a target text pattern matched (text analysis); (ii) whether text patterns in neighbor segments matched (text analysis); (iii) whether there is high contrast in comparison to neighbor components (color analysis); and (iv) whether there is a size difference in comparison to neighbor components (spatial analysis). If the feature is present a 1 is assigned, and if a feature is not present, a 0 is assigned. The scores are calculated for the DP categories that correspond to the matched textual features from the analysis phase, for the individual features (textual/visual/color) defined for them. The scores are normalized depending upon how many analyses are relevant for a given DP. Next, in the UI level resolution, the scores are combined with the icon detection results using an 80/20 (scores/icon) ratio. The final output of the Segment Level Resolution is a JSON object where for each segment a set of DPs with corresponding votes and scores are reported.</p><p>2) UI Level Resolution: In this step, AIDUI makes the final prediction regarding any underlying DPs that are likely to be present in a given UI. To accomplish this, results from both Segment Level Resolution (section III-E1) and Visual Cue Detection (section III-B) are taken as input. For each identified DP category, we take the votes with the highest score for given DP category found in Segment Level Resolution. Later, if applicable, we also take into account the vote and confidence score from Visual Cue Detection. In the case that both types of information are present, we combine the confidence scores from the Segment Level Resolution and the Visual Cue Detection using an 80/20 ratio (which we derived empirically). The output of UI Level Resolution is a JSON object containing identified DP categories along with the number of votes, scores, and associated segment information for localization. Finally, the top (up to) two DP categories having the highest number of votes along with a certain confidence level are selected as the most likely DP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EVALUATION METHODOLOGY</head><p>In this section, we present the design of the study we employed for the evaluation of AIDUI. Our empirical study is aimed at assessing the performance and robustness of the approach as well as the contribution of different analysis modules to the overall effectiveness. To accomplish these study goals we formulated the following research questions: To collect the mobile UIs, we made use of the comprehensive video dataset by Geronimo et al. <ref type="bibr" coords="7,500.74,267.34,16.60,8.64" target="#b15">[18]</ref> which includes mobile app usage screen recordings and classifications of identified DPs at various timestamps in the videos. The publicly available dataset by Geronimo et al. <ref type="bibr" coords="7,497.13,303.20,16.60,8.64" target="#b15">[18]</ref> includes 15 videos of mobile apps and information about the timestamps of observed DPs in those videos. Based on the available videos and the timestamps data, we extract the frames from the videos according to given timestamps. As the public dataset only contains 15 videos, we contacted and worked with the authors of the paper to extract all relevant DPs from their entire corpus of user videos, while carefully excluding video segments that may contain personal information. During this process, we collected 2994 UI screenshots in total spanning over 68 apps from 3 categories (communication, entertainment, and music).</p><p>Similarly, based on the data provided by Mathur et al. <ref type="bibr" coords="7,543.95,434.85,15.27,8.64" target="#b34">[37]</ref>, we collected over 1500 web screenshots from popular shopping websites with known DPs by leveraging their publicly available dataset. To further bolster the number of DP examples and the generalizability of our dataset, we also randomly collected a combined 5000 mobile UI screenshots from the RICO dataset <ref type="bibr" coords="7,375.62,506.58,16.60,8.64" target="#b14">[17]</ref> (not used to the train the icon detection model) and web UI screenshots from popular Alexa 100 shopping websites respectively. After the curation process, we randomly selected a total of 501 (339 mobile and 162 web) screenshots for labeling. This sampling represents a 95% confidence level and 8% confidence interval for mobile apps, and 12% confidence interval for web UIs.</p><p>Next, three authors of this paper participated in a rigorous labeling process for both categorizing and creating bounding boxes for each observed DP in our sampled dataset. In order to ensure a consistent and agreed upon labeling strategy, prior to the labeling process, we conducted a comprehensive discussion among the authors to review the rationales behind the labeling decisions of our 501 UI screenshots. This process included examining and discussing two-three examples of each of the 10 targeted DPs from our taxonomy, and deriving a set of labeling and bounding box guidelines (these guidelines are available in our online appendix <ref type="bibr" coords="7,443.98,709.96,10.45,8.64" target="#b4">[7]</ref>). Then, two of the authors independently labeled each of the 501 UI screens using the Label Studio <ref type="bibr" coords="8,102.36,64.38,11.62,8.64" target="#b0">[1]</ref> application, resolving necessary conflicts. The labeling procedure took place over three rounds, wherein the agreement of the last round was higher than 80%, indicating strong agreement among the labelers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation Metrics</head><p>We evaluate AIDUI with respect to all RQs by using following metrics: precision, recall, F1-score, and IoU. The metrics are defined as follows:</p><p>• Precision describes the ability of the classifier to properly distinguish between true positives and false positives.</p><p>Precision is computed as Precision = TP/TP + FP, where TP is the number of true positives and FP is the number of false positives. In our context a TP (true positive) is a correct prediction that a given DP type exists on a given UI screen and matches the ground truth, and an FP (false positive) is when the approach predicts a DP type that is not present on a given screen in CONTEXTDP. • Recall is intuitively the ability of the classifier to find all the samples that are positive. Recall is computed as Recall = TP/TP + FN, where TP is the number of true positives and FN is the number of false negatives. In this context a false negative is a case when a DP is present on a screen, but AIDUI could not detect it. • F1-score is the harmonic mean of the precision and recall. F1-score is defined as F1-score = 2 × Precision × Recall/Precision + Recall • IoU computes the amount of overlap between the predicted and ground truth bounding boxes. IoU is defined as: IoU = Area of Overlap/Area of Union. Note that we define two versions of IoU for our evaluation. First, strict IoU operates according to the formula above, wherein predictions are directly compared to the ground truth.</p><p>Second we define a contained IoU wherein we set the IoU to 100% if the predicted bounding box falls within the ground truth bounding box. This is because during our investigation of the localization results, we found that AIDUI often predicts more specific bounding boxes for a given DP that could still be helpful to end users. Thus, this measure may offer a more realistic characterization of AIDUI'S performance by rewarding segments identified within the ground truth bounding box.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. RQ 1 &amp; RQ 2 : DP Detection Performance Across Domains</head><p>To answer RQ 1 , we assess the ability of AIDUI to accurately detect DP instances. This experiment aims to measure AIDUI's performance both as a whole, and for individual DP categories in order to determine AIDUI's effectiveness on individual DP categories. In this process, we evaluate AIDUI on CONTEXTDP (section IV-A) using the aforementioned evaluation metrics (precision, recall and F1-score) described in section IV-B. The evaluation metrics are computed using the predictions and derived ground truth from CONTEXTDP.</p><p>To answer RQ 2 we compare the detection results across both the web and mobile portions of the CONTEXTDP dataset. D. RQ 3 : Potential False Positive DP Detections AIDUI'S utility as a potential developer tool is predicated on the intention that it perform reasonably well at both detecting dark patterns when they do exist, and not triggering false alarms for screens that do not contain DP instances. Thus, to evaluate the potential of AIDUI to trigger false positives, we applied our technique to the 243 screenshots that do not contain DPs from CONTEXTDP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. RQ 4 : Localization Performance</head><p>We answer RQ 4 using both contained and strict IoU which are calculated by measuring the difference between the predicted bounding boxes from AIDUI and the ground truth bounding boxes from CONTEXTDP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. RQ 5 : Ablation Study of DP Analyses</head><p>To answer this RQ, we conduct an experiment that is aimed at exploring the contributions of the textual, color, and spatial analysis modules in detecting DPs. To accomplish this, we choose different combinations of modules while conducting the evaluation process. As text analysis acts as the base module, it is selected in all the combinations. The combinations that we use in the evaluation are: Text, Text + Color Analysis, Text + Spatial Analysis, and Text + Color + Spatial Analysis. For each combination, we calculate the same evaluation metrics that we use in RQ1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EMPIRICAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. RQ 1 : Detection Performance</head><p>Our main goal in RQ1 is to measure the performance of AIDUI in terms of detection/classification of both Non-DP and DP instances. In answering this RQ, we are interested in assessing AIDUI'S overall performance as well as performance specific to individual DP categories. We conduct the performance evaluation based on the metrics stated in section IV-B, i.e., precision, recall and F1-score. Table <ref type="table" coords="8,552.53,470.86,10.51,8.64" target="#tab_1">IV</ref> and III illustrate the aggregate and category-wise classification performance respectively.</p><p>From the classification results in table IV, we observe that AIDUI achieves an overall average precision of 66%, average recall of 67% and average F1-score of 65% in detecting DP instances. Moreover, category-wise results in Table <ref type="table" coords="8,553.08,542.59,9.95,8.64" target="#tab_2">III</ref> show that a subset of DPs (e.g., Activity Message, High Demand Message, Low Stock Message, Limited Time Message, Countdown Timer etc.) can reliably be detected with moderate to high precision, recall and F1-score values.</p><p>Instances where AIDUI fails to identify DPs are mostly due to deviations in textual or visual patterns exhibited by some of the DP categories. For instance, 32% of the Disguised Ads instances are correctly predicted whereas 26% of them are wrongly predicted as Non-DP. To better understand, we present one such representative instance of Disguised Ads in Figure <ref type="figure" coords="8,355.72,674.09,3.74,8.64">5</ref>. Here, at a first glance, the appearance of the advertisement looks very similar to regular content. Though on the top right corner there is an ad icon, this particular advertisement has a subtle difference compared to other typical    Similarly, for some DP categories our approach fails due to the limitation of the vocabulary or textual patterns that AidUI considers, as illustrated in Figure <ref type="figure" coords="9,217.89,555.12,9.96,8.64">4b</ref> for the Default Choice DP. Future work could focus toward resolving these limitations through the use of language modeling wherein embeddings from the models are used to measure textual similarity, and hence would be able to better account for semantically similar, yet lexically varied text. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. RQ 2 : Domain-specific Performance</head><p>Table <ref type="table" coords="9,346.55,245.14,9.95,8.64" target="#tab_2">III</ref> illustrates that AIDUI performs significantly better on web UIs as compared to the UIs from mobile applications. In fact, we observe an increase of 30% for avg. precision, 26% for avg. recall and 31% for avg. F1-score between the two portions of our dataset. Our insight regarding the significant performance difference in identifying DPs across mobile and web domain is related to the prevalence of different types of DPs in those domains. While developing our unified taxonomy, we observed that there are particular groups of DPs that have dominant presence in a particular domain, i.e., web or mobile. Moreover, we also observed that the patterns frequently found in web UIs typically skew toward textual cues whereas several patterns that are dominant in mobile UIs often contain both textual and visual cues. This phenomenon makes the DP identification task in mobile UIs more complex as compared to web UIs. As our current approach equally considers the outputs from text and color analysis, this could be one reason for the observed performance gap across the two software domains. Future work should aim to empirically examine the textual and visual differences in DPs across software domains in an effort to better inform future automated techniques.</p><p>Answer to RQ2: AIDUI shows significant difference in performance across web and mobile domains. This is likely due to the fact that the observed DPs in our mobile dataset contain more varied patterns that are more difficult to detect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. RQ 3 : False Positive DP Detection</head><p>To measure the rate of false positives, we applied AIDUI to a set of screens that, during the labeling process for ContextDP, were confirmed to not exhibit any dark patterns. Of the 243 to which we applied AIDUI, only 36 screens (≈15%) were identified as having false positives, and a vast majority of these fell into the Default Choice DP category. It should be noted that given the scoring procedure among the various components of AIDUI's approach, it is possible to calculate a confidence threshold that a given screen contains a DP, which could be used to help indicate the potential severity or confidence of predictions for future developer tools. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. RQ 4 : Localization Performance</head><p>The results in Table V illustrate that our approach achieves a fairly low strict IoU both overall and on a category-bycategory basis. However, this measurement approach presents a skewed view of the practical performance of AIDUI. This is because after examining both predicted and ground truth bounding boxes we noticed that this is largely due to precise bounding box selection by our text analysis module. Though our approach is actually able to localize a precise bounding box for various UIs, it is getting penalized because of having smaller intersections as compared to the ground truth, which tended to encompass areas between text, for example. Based on our observation, we defined the contained IoU to provide a more complete picture of AIDUI'S localization performance. The overall avg. contained IoU value 0.83 illustrates that AIDUI is properly localizing elements that exist within the ground truth bounding boxes, although the lablers of the CONTEXTDP dataset often felt that a larger portion of the screen should be considered to contain given DPs, usually due to negative space between text.</p><p>Answer to RQ4: When examining strict IoU, AIDUI performs poorly as it tends to localize smaller areas of the screen. However, when considering contained IoU, we find that the areas of the screen that AIDUI localizes consistently fall within the ground truth for the DPs in CONTEXTDP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. RQ 5 : Ablation Study of DP Analyses</head><p>Finally, we conduct an ablation study to examine the contribution of AIDUI'S textual, color and spatial analysis modules in detecting DPs. In this experiment, we remove one module at a time to understand the contribution of other modules. As stated earlier in section IV-F, text analysis serves as the foundation of our implemented approach. Hence, we include the text analysis module in every combination of modules we use in our ablation study. We first start with using all three modules (i.e., text + color + spatial). In later steps, we removed color and spatial modules respectively. Finally, we end up with using text analysis only.</p><p>Answer to RQ5: AIDUIachieves best results when all three, i.e., text, color and spatial, analysis modules are present. This suggests that there are orthogonal features that exist across data modalities that are useful for automated DP detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. RELATED WORK A. Automated Detection of Dark Patterns</head><p>Raju et al. <ref type="bibr" coords="10,374.83,557.99,16.60,8.64" target="#b43">[46]</ref> proposed an intended design aimed to analyze the source code of a loaded web page to detect the advertisements that correspond to certain dark pattern types. Another work by Liu et al. <ref type="bibr" coords="10,430.81,593.85,16.60,8.64" target="#b29">[32]</ref> proposed a framework that leverages automated app testing with ad traffic identification approach to detect devious ad contents. Unlike these works, which are solely focused on detecting suspicious advertisements, our approach is aimed to detect a wide range of different types of dark patterns coming from both mobile and web applications.  <ref type="bibr" coords="11,220.33,76.34,16.60,8.64" target="#b55">[57]</ref> and icon labeling <ref type="bibr" coords="11,66.65,88.29,16.60,8.64" target="#b11">[14]</ref> that can automatically infer accessibility metadata from screen pixels. Moran et al. introduced REDRAW <ref type="bibr" coords="11,280.93,100.25,15.27,8.64" target="#b35">[38]</ref>, which uses a combination of unsupervised computer vision and deep learning techniques to automatically prototype UI code for mobile apps from mock-ups. AIDUI differs from these above techniques in two major ways. First, the screen properties which are identified by AIDUI (DPs) are novel compared to properties inferred by past work (e.g., accessibility data, screen elements). Second, due to the nature of these properties, AIDUI employs different analysis techniques. While a majority of the techniques above used end-to-end deep learning, this is difficult given that dark pattern examples need to manually sourced, making large scale data collection challenging. Thus, AIDUI processes visual and textual cues, as opposed to training a purely deep learning-based classification solution -making it largely complementary to existing work.</p><p>Additionally, the software engineering research community has been working towards identifying UI display issues across multiple types of software including web <ref type="bibr" coords="11,218.56,303.63,15.77,8.64" target="#b31">[34,</ref><ref type="bibr" coords="11,235.98,303.63,11.83,8.64" target="#b32">35]</ref>, and mobile apps <ref type="bibr" coords="11,72.46,315.59,15.77,8.64" target="#b30">[33,</ref><ref type="bibr" coords="11,89.89,315.59,12.45,8.64" target="#b36">39,</ref><ref type="bibr" coords="11,104.01,315.59,12.45,8.64" target="#b53">55,</ref><ref type="bibr" coords="11,118.12,315.59,11.83,8.64" target="#b56">58]</ref>. These techniques tend to use a combination of deep learning and unsupervised computer vision techniques to detect varying types of display issues such as design guideline violations, internationalization issues, and deviations from design specifications. However, none of these techniques is capable of detecting dark patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ethics and Dark Patterns in UI/UX Design</head><p>A longstanding goal of the broader HCI research community has been to develop effective frameworks and guidelines to improve the UI/UX of applications Hence, investigating the ethical aspects of UI/UX from designer's perspectives is a growing area of interest in the HCI community and researchers have already explored a number of framings regarding ethics and values <ref type="bibr" coords="11,95.84,482.63,15.77,8.64" target="#b7">[10,</ref><ref type="bibr" coords="11,113.26,482.63,9.13,8.64" target="#b17">[20]</ref><ref type="bibr" coords="11,122.39,482.63,4.57,8.64" target="#b18">[21]</ref><ref type="bibr" coords="11,122.39,482.63,4.57,8.64" target="#b19">[22]</ref><ref type="bibr" coords="11,126.96,482.63,13.70,8.64" target="#b20">[23]</ref><ref type="bibr" coords="11,142.32,482.63,8.78,8.64" target="#b46">[48]</ref><ref type="bibr" coords="11,151.10,482.63,4.39,8.64" target="#b47">[49]</ref><ref type="bibr" coords="11,155.49,482.63,13.16,8.64" target="#b48">[50]</ref>. In this paper, we aim to build upon past research done in the topics of DPs and ethical UI/UX design by investigating how automated approaches for DP detection may equip developers with the tools necessary to understand/avoid deceptive designs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. LIMITATIONS &amp; THREATS TO VALIDITY</head><p>Limitations: AIDUI has various limitations that serve as motivation for future research. First, our the current implementation of AIDUI is targeted toward 10 DP categories, which means future work is required to developed automated techniques to detect other categories. As already discussed in section II, our current approach is focused on working with DP categories that are manifested by different visual and textual cues on a single screen. Thus, one clear future research direction is to develop detection mechanisms for dynamic DP categories that are characterized by multiple screens and user actions.</p><p>Another limitation, which we plan to address in future work, is that our current text analysis technique is solely based on heuristically defined pattern-matching rules and hence are difficult to apply to certain DP categories (e.g., toying with emotion) that involve semantically complex textual patterns to persuade the user into a particular action. As already discussed in section V, the current text analysis approach has also the limitation of not detecting semantically similar, yet lexically varied text. We plan to leverage neural language models to address these challenges in future work. Internal Validity: Threats to internal validity correspond to unexpected factors in the experiments that may contribute to observed results. The main threat to internal validity is related to the construction of the ContextDP dataset. However, we mitigated this threat by creating the dataset through an independent multi-author labeling procedure with high agreement. Another potential threat to internal validity is the construction of our training dataset for the FasterRCNN <ref type="bibr" coords="11,501.23,219.93,15.27,8.64" target="#b45">[47]</ref>. However, we followed best practices of prior work <ref type="bibr" coords="11,481.25,231.88,16.60,8.64" target="#b8">[11]</ref> and our trained model achieves high accuracy, mitigating this threat. Construct Validity: Threats to construct validity concern the operationalization of experimental artifacts. The main threat to construct validity is related to the measuring of the results of our approach in comparison to the ground truth. However, given that our approach outputs bounding boxes and DP labels, we were able to directly and automatically compute results. External Validity: Threats to external validity concern the generalization of the results. The main threats to our approach relate to the generalizability of the heuristics used to detect dark patterns. However, we derived these heuristics using 2-3 examples of each DP not included in ContextDP, and they appear to generalize well to the larger ContextDP dataset. Another threat to external validity is related to the generalizability of the ContextDP dataset itself. While we do not claim that our results generalize beyond this dataset, we did mitigate this threat by including a large number of examples across two different software domains (web and mobile).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSION</head><p>In this paper, we have taken the first steps toward investigating the feasibility of automated detection of UI dark patterns in mobile and web UIs. We unified similar DP categories from existing taxonomies together and derived CONTEXTDP, the largest current fully labeled and localized DP instances. Furthermore, we implemented AIDUI, a fully-automated DP detection and localization technique, which performs well on ContextDP. Our results show that AIDUI performs well, in terms of precision, recall and F1-score on a large subset of our studied DPs. We also illustrated that automated DP detection techniques appear to benefit from fusing multimodal data (e.g., text and visual information). The summative findings of this work illustrate the feasibility and promise of automated DP detection/localization techniques.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,69.66,230.06,209.68,6.91;2,48.96,50.54,251.05,166.77"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Example of an Attention Distraction Dark Pattern</figDesc><graphic coords="2,48.96,50.54,251.05,166.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,48.96,361.54,514.07,6.91;3,48.96,370.51,514.07,6.91;3,48.96,379.48,67.22,6.91"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Our Unified Dark Pattern Taxonomy -numbers in parentheses signify the number of examples of the DPs that are present in the CONTEXTDP dataset, and the signifies those patterns that AIDUI is designed to detect. Note that this unified taxonomy is a targeted combination of past works [13, 25, 37] with minor modifications.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,236.96,195.78,138.07,6.91;4,48.96,216.74,251.06,8.64;4,48.96,228.69,251.06,8.64;4,48.96,240.65,251.06,8.64;4,48.96,252.60,251.06,8.64;4,48.96,264.56,251.06,8.64;4,48.96,276.33,251.06,8.82;4,48.96,288.47,251.06,8.64;4,48.96,300.42,251.06,8.64;4,48.96,312.38,251.06,8.64;4,48.96,324.33,251.06,8.64;4,48.96,335.97,251.06,8.96;4,48.96,348.24,251.06,8.64;4,48.96,360.20,251.06,8.64;4,48.96,372.15,251.06,8.64;4,48.96,384.11,199.83,8.64;4,58.93,397.59,241.09,8.64;4,48.96,409.55,251.06,8.64;4,48.96,421.50,251.06,8.64;4,48.96,433.46,251.06,8.64;4,48.96,445.41,251.06,8.64;4,48.96,457.37,251.06,8.64;4,48.96,469.32,251.06,8.64;4,48.96,481.28,251.06,8.64;4,48.96,493.24,251.06,8.64;4,48.96,505.01,251.06,8.82;4,48.96,516.97,251.06,8.82;4,48.96,529.10,251.06,8.64;4,48.96,541.06,251.06,8.64;4,48.96,552.83,251.06,8.82;4,48.96,564.79,251.06,8.82;4,48.96,576.92,251.06,8.64;4,48.96,588.88,251.06,8.64;4,48.96,600.83,251.06,8.64;4,48.96,612.79,251.06,8.64;4,48.96,624.74,251.05,8.64;4,48.96,636.70,251.06,8.64;4,48.96,648.65,100.21,8.64;4,58.93,662.14,241.09,8.64;4,48.96,674.09,251.06,8.64;4,48.96,686.05,251.05,8.64;4,48.96,698.00,251.06,8.64;4,48.96,709.96,251.06,8.64;4,305.37,124.19,139.87,68.81"><head>1 UIFig. 4 .</head><label>14</label><figDesc>Fig. 4. Workflow of the AIDUI approachtaxonomy and extended the original meaning of Aesthetic Manipulation and Forced Action classes to include new DPs instances. The authors analyzed pervasiveness of DPs in mobile applications using an approach similar to cognitive walk through techniques<ref type="bibr" coords="4,130.93,264.56,16.60,8.64" target="#b39">[42]</ref> whereas previous works<ref type="bibr" coords="4,255.20,264.56,15.77,8.64" target="#b22">[25,</ref><ref type="bibr" coords="4,272.62,264.56,12.45,8.64" target="#b34">37,</ref><ref type="bibr" coords="4,286.74,264.56,13.28,8.64" target="#b37">40]</ref> analyzed screenshots to classify DPs. Finally, Mathur et al.[37] performed a semi-automated, large-scale collection of DPs in online shopping websites, and derived a taxonomy of 15 dark patterns grouped into 7 categories. In the course of their data collection, they found 1,818 instances of DPs on the top ≈11k shopping websites. More recent work has aimed to identify DPs that are specific to various different contexts or domains including (i) shopping web apps<ref type="bibr" coords="4,265.84,360.20,15.27,8.64" target="#b34">[37]</ref>, (ii) computer games<ref type="bibr" coords="4,119.69,372.15,15.27,8.64" target="#b54">[56]</ref>, (iii) privacy-centric software<ref type="bibr" coords="4,262.95,372.15,15.27,8.64" target="#b9">[12]</ref>, (iv) robotics<ref type="bibr" coords="4,84.55,384.11,15.27,8.64" target="#b26">[29]</ref>, and (v) digital consent forms<ref type="bibr" coords="4,229.71,384.11,15.27,8.64" target="#b49">[51]</ref>.Given the somewhat complementary, yet disparate nature of existing taxonomies of DPs, we aimed to merge similar DP categories from existing taxonomies together and provide a larger landscape of patterns for mobile and web apps toward which we can design and evaluate our automated detection approach. Given that the scope of our work is primarily concerned with web and mobile UIs, we did not include many of the domain specific taxonomies mentioned above. Instead, our unified taxonomy is primarily a fusing of the various categories and subcategories derived by Gray et al.<ref type="bibr" coords="4,280.93,505.19,15.27,8.64" target="#b22">[25]</ref>, Mathur et al.<ref type="bibr" coords="4,103.74,517.15,16.60,8.64" target="#b34">[37]</ref> and Brignull et al.<ref type="bibr" coords="4,198.74,517.15,15.27,8.64" target="#b10">[13]</ref>. To build our unified taxonomy, first one author gathered one-two existing examples of each type of DPs that exists in the categories described by Gray et al.<ref type="bibr" coords="4,119.62,553.01,15.27,8.64" target="#b22">[25]</ref>, Mathur et al.<ref type="bibr" coords="4,208.81,553.01,15.27,8.64" target="#b34">[37]</ref>, and Birgnull et al.<ref type="bibr" coords="4,61.71,564.97,15.27,8.64" target="#b10">[13]</ref>. Next, two authors met to review each DP example and re-group the various categories under unified headings. Our final unified taxonomy, illustrated in Figure3, spans 7 parent categories which organize a total of 27 classes that describe different DPs. Note that many of the DPs in our taxonomy are self-documenting (e.g., countdown timer), however, we provide full descriptions and examples of each DP in our online appendices<ref type="bibr" coords="4,125.11,648.65,8.02,8.64" target="#b3">[6]</ref><ref type="bibr" coords="4,133.13,648.65,4.01,8.64" target="#b4">[7]</ref><ref type="bibr" coords="4,137.14,648.65,8.02,8.64" target="#b5">[8]</ref>.Not all dark patterns are created equal from viewpoint of the underlying UI motifs that signal their presence. For example, for the Sneak Into Basket DP type, typically there would be several actions and screens required to detect the presence of such a pattern, which introduces far more variability in</figDesc><graphic coords="4,305.37,124.19,139.87,68.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="9,80.02,454.36,86.25,6.32;9,98.53,462.59,49.24,5.27;9,82.48,306.62,81.34,144.61"><head>Figure 4a :</head><label>4a</label><figDesc>Figure 4a: Example of difficult to detect disguised ad.</figDesc><graphic coords="9,82.48,306.62,81.34,144.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="9,182.50,454.36,86.60,6.32;9,198.49,462.59,54.63,5.27;9,89.27,480.97,170.45,6.91;9,48.96,495.34,251.06,8.64;9,48.96,507.30,251.06,8.64;9,48.96,519.25,251.06,8.64;9,48.96,531.21,251.06,8.64;9,48.96,543.16,251.06,8.64;9,48.96,555.12,251.06,8.64;9,48.96,567.07,251.06,8.64;9,48.96,578.85,251.06,8.82;9,48.96,590.98,251.06,8.64;9,48.96,602.94,251.06,8.64;9,48.96,614.89,185.83,8.64;9,185.13,306.62,81.34,144.61"><head>Figure 4b :Fig. 5 .</head><label>4b5</label><figDesc>Figure 4b: Example of difficult to detect Default Choice Fig. 5. Example of difficult to detect Dark Patterns advertisements. To detect such subtle visual cues, we may need to develop separate deep learning solution based on a comprehensive study of diverse types of UI advertisement contents.Similarly, for some DP categories our approach fails due to the limitation of the vocabulary or textual patterns that AidUI considers, as illustrated in Figure4bfor the Default Choice DP. Future work could focus toward resolving these limitations through the use of language modeling wherein embeddings from the models are used to measure textual similarity, and hence would be able to better account for semantically similar, yet lexically varied text.</figDesc><graphic coords="9,185.13,306.62,81.34,144.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="10,311.98,682.92,251.06,8.59;10,321.94,697.82,241.09,8.82;10,311.98,709.96,251.06,8.64;11,48.96,52.42,251.06,8.64;11,48.96,64.20,251.06,8.82;11,48.96,76.34,168.41,8.64"><head></head><label></label><figDesc>B. Automated UI Understanding &amp; Detection of Design Issues Wu et al. introduced Screen Parsing [53], and Chen et al. introduced UIED [15], both of which use computer vision and deep learning tehcniques to segment and classify UI elements from screenshot pixels. Zhang et al. and Chen et al. developed approaches for screen content recognition</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,334.27,43.76,203.69,99.01"><head>TABLE I MAPPING</head><label>I</label><figDesc>OF ICONS (VISUAL CUES) TO LIKELY DPS</figDesc><table coords="5,334.27,74.28,203.69,68.50"><row><cell>Icons</cell><cell>Likely DPs</cell></row><row><cell>Like</cell><cell>Nagging</cell></row><row><cell>Dislike</cell><cell>Nagging</cell></row><row><cell>Star</cell><cell>Nagging</cell></row><row><cell>Toggle Switch (on)</cell><cell>Default Choice</cell></row><row><cell>Ad</cell><cell>Nagging, Disgiused Ads</cell></row><row><cell>Ad Loader</cell><cell>Nagging, Gamification</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,54.94,36.79,241.59,141.40"><head>TABLE II MAPPING</head><label>II</label><figDesc>OF SAMPLE LEXICAL PATTERNS TO DPS</figDesc><table coords="6,54.94,64.34,241.59,113.85"><row><cell>Dark Pattern</cell><cell>Sample Lexical Patterns</cell></row><row><cell>Nagging</cell><cell>watch + &lt;ad/session&gt;</cell></row><row><cell>Gamification</cell><cell>&lt;ask/invite/refer&gt;+ friends + to + [subject] signup + for + &lt;credits/points/tokens&gt;</cell></row><row><cell>Default Choice</cell><cell>I + &lt;agree/consent&gt;+ to + [predicate]</cell></row><row><cell>Attention Distraction</cell><cell>I +&lt;decline/don't&gt;+ &lt;want/opt out/refuse</cell></row><row><cell></cell><cell>to&gt;+ [predicate]</cell></row><row><cell>Countdown Timer/ Limited Time</cell><cell>sale + &lt;ends/countdown/now&gt;; shop +</cell></row><row><cell>Message</cell><cell>&lt;now/within&gt;</cell></row><row><cell>Low Stock/ Limited Time/ High De-</cell><cell>&lt;apply/order&gt;+ by; only + [number] + in</cell></row><row><cell>mand</cell><cell>stock; [number] + available</cell></row><row><cell>Activity Message</cell><cell>[number] + items sold + [number] + time</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,311.98,52.04,251.06,199.89"><head></head><label></label><figDesc>What is the precision, recall and F1-score of AIDUI in detecting DP in UIs? • RQ 2 : How robust is AIDUI in detecting DPs in UIs from different domains (mobile/web)? • RQ 3 : How often does AIDUI detect false positives in screens that known to not contain DPs? • RQ 4 : How well is AIDUI able to localize DPs in UI screens? • RQ 5 : What is the contribution of different analysis modules in detecting DPs in UIs ? A. Derivation of the CONTEXTDP Dataset To evaluate AIDUI, we have derived CONTEXTDP, the current largest dataset of labeled UIs containing both DP and Non-DP instances. CONTEXTDP includes total 501 mobile and web UI screenshots that represent 301 DP instances as well as 243 Non-DP instances.</figDesc><table /><note>• RQ 1 :</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="10,51.56,38.78,508.91,346.68"><head></head><label></label><figDesc>AIDUI exhibits a false positive rate of 36/243 (14%) when applied to screens confirmed to not exhibit DPs. However, most of these misclassifications are into a single class Default Choice DP class, and could be further mitigated both by adjusting the sensitivity of cues for this class, and by displaying confidence scores for given predictions for future developer-facing tools.</figDesc><table coords="10,51.56,38.78,508.91,286.84"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>TABLE V</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">AIDUI LOCALIZATION PERFORMANCE</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>All</cell><cell></cell><cell></cell><cell></cell><cell>Mobile</cell><cell></cell><cell></cell><cell></cell><cell>Web</cell></row><row><cell>DP Category</cell><cell cols="4">Avg. Strict IoU Avg. Contained IoU</cell><cell cols="3">Avg. Strict IoU Avg. Contained IoU</cell><cell cols="3">Avg. Strict IoU Avg. Contained IoU</cell></row><row><cell>Activity Message</cell><cell>0.351397</cell><cell cols="2">0.740450</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell cols="2">0.351397</cell><cell>0.740450</cell></row><row><cell cols="2">High Demand Message 0.340726</cell><cell cols="2">0.730416</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell cols="2">0.340726</cell><cell>0.730416</cell></row><row><cell>Low Stock Message</cell><cell>0.223910</cell><cell cols="2">1.000000</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell cols="2">0.223910</cell><cell>1.000000</cell></row><row><cell cols="2">Limited Time Message 0.262617</cell><cell cols="2">0.808963</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell cols="2">0.262617</cell><cell>0.808963</cell></row><row><cell>Countdown Timer</cell><cell>0.231990</cell><cell cols="2">0.823345</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell cols="2">0.231990</cell><cell>0.823345</cell></row><row><cell>Attention Distraction</cell><cell>0.040522</cell><cell cols="2">1.000000</cell><cell></cell><cell>0.081790</cell><cell cols="2">1.000000</cell><cell cols="2">0.019887</cell><cell>1.000000</cell></row><row><cell>Default Choice</cell><cell>0.023972</cell><cell cols="2">0.569231</cell><cell></cell><cell>0.024030</cell><cell cols="2">0.542373</cell><cell cols="2">0.023409</cell><cell>0.833333</cell></row><row><cell>Disguised Ads</cell><cell>0.006635</cell><cell cols="2">0.916667</cell><cell></cell><cell>0.006635</cell><cell cols="2">0.916667</cell><cell>-</cell><cell></cell><cell>-</cell></row><row><cell>Nagging</cell><cell>0.015434</cell><cell cols="2">0.957007</cell><cell></cell><cell>0.015434</cell><cell cols="2">0.957007</cell><cell>-</cell><cell></cell><cell>-</cell></row><row><cell>Gamification</cell><cell>0.125986</cell><cell cols="2">0.777633</cell><cell></cell><cell>0.125986</cell><cell cols="2">0.777633</cell><cell>-</cell><cell></cell><cell>-</cell></row><row><cell>Overall</cell><cell>0.162319</cell><cell cols="2">0.832371</cell><cell></cell><cell>0.050775</cell><cell cols="2">0.838736</cell><cell cols="2">0.207705</cell><cell>0.848072</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>TABLE VI</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">CONTRIBUTION OF DIFFERENT MODULES</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>All</cell><cell></cell><cell></cell><cell>Mobile</cell><cell></cell><cell></cell><cell>Web</cell></row><row><cell></cell><cell></cell><cell cols="3">DP Instances: 301</cell><cell cols="3">DP Instances: 197</cell><cell cols="3">DP Instances: 104</cell></row><row><cell>Modules</cell><cell cols="2">Avg. Precision</cell><cell>Avg. Recall</cell><cell>Avg. F1-score</cell><cell>Avg. Precision</cell><cell>Avg. Recall</cell><cell>Avg. F1-score</cell><cell>Avg. Precision</cell><cell>Avg. Recall</cell><cell>Avg. F1-score</cell></row><row><cell>Text</cell><cell>0.75</cell><cell></cell><cell>0.44</cell><cell>0.40</cell><cell>0.75</cell><cell>0.32</cell><cell>0.27</cell><cell>0.74</cell><cell>0.67</cell><cell>0.70</cell></row><row><cell>Text + Color</cell><cell>0.78</cell><cell></cell><cell>0.45</cell><cell>0.42</cell><cell>0.78</cell><cell>0.33</cell><cell>0.29</cell><cell>0.74</cell><cell>0.67</cell><cell>0.70</cell></row><row><cell>Text + Spatial</cell><cell>0.65</cell><cell></cell><cell>0.65</cell><cell>0.63</cell><cell>0.63</cell><cell>0.61</cell><cell>0.60</cell><cell>0.78</cell><cell>0.73</cell><cell>0.75</cell></row><row><cell cols="2">Text + Color + Spatial 0.66</cell><cell></cell><cell>0.67</cell><cell>0.65</cell><cell>0.63</cell><cell>0.61</cell><cell>0.60</cell><cell>0.82</cell><cell>0.77</cell><cell>0.79</cell></row><row><cell>Answer to RQ3:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Note that since the publishing of this paper, Brignull moved to using the term "deceptive design" in place of "dark pattern" in an effort to be clearer and more inclusive.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>This work is supported in part by the NSF grants: CCF-2132285 and CCF-1955853. Any opinions, findings, and conclusions expressed herein are the authors and do not necessarily reflect those of the sponsors.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct coords="12,67.22,67.62,127.27,6.91" xml:id="b0">
	<analytic>
		<title level="a" type="main">The Mastering Studio</title>
		<author>
			<persName><forename type="first">Evren</forename><surname>Göknar</surname></persName>
		</author>
		<idno type="DOI">10.4324/9781315164106-5</idno>
		<ptr target="https://labelstud.io/" />
	</analytic>
	<monogr>
		<title level="m">Major Label Mastering</title>
				<imprint>
			<publisher>Focal Press</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="33" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,67.22,76.59,109.63,6.91" xml:id="b1">
	<analytic>
		<title level="a" type="main">CV Bling—OpenCV Inbuilt Demos</title>
		<author>
			<persName><forename type="first">Samarth</forename><surname>Brahmbhatt</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-4302-6080-6_3</idno>
		<ptr target="https://opencv.org/" />
	</analytic>
	<monogr>
		<title level="m">Practical OpenCV</title>
				<imprint>
			<publisher>Apress</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="13" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,67.22,103.49,201.20,6.91" xml:id="b2">
	<analytic>
		<title level="a" type="main">PyTorch-OOD: A Library for Out-of-Distribution Detection based on PyTorch</title>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Kirchheim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Filax</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Ortmeier</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvprw56347.2022.00481</idno>
		<ptr target="https://pytorch.org/vision/stable/index.html" />
	</analytic>
	<monogr>
		<title level="m">2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022-06">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,67.22,112.45,224.18,6.91" xml:id="b3">
	<monogr>
		<idno type="DOI">10.7287/peerj.preprints.1947v1/supp-1</idno>
		<ptr target="https://github.com/SageSELab/AidUI" />
		<title level="m">Aidui github repository</title>
				<imprint>
			<publisher>PeerJ</publisher>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,67.22,121.42,172.89,6.91" xml:id="b4">
	<analytic>
		<title level="a" type="main">AidUI: Toward Automated Recognition of Dark Patterns in User Interfaces</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Hasan Mansur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sabiha</forename><surname>Salma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damilola</forename><surname>Awofisayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Moran</surname></persName>
		</author>
		<idno type="DOI">10.1109/icse48619.2023.00166</idno>
		<ptr target="https://sagelab.io/aidui/" />
	</analytic>
	<monogr>
		<title level="m">2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2023-05">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,67.22,130.38,232.80,6.91;12,67.22,139.35,50.61,6.91" xml:id="b5">
	<monogr>
		<idno type="DOI">10.5281/zenodo.7578246</idno>
		<ptr target="https://doi.org/10.5281/zenodo.7578246" />
		<title level="m">Zenodo archive of aidui code and data</title>
				<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,67.22,148.32,232.80,6.91;12,67.22,157.28,162.27,6.91" xml:id="b6">
	<monogr>
		<title level="m" type="main">Affinion group faces class action after paying out claims to ags</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Asbury</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-02">Feb 2014. November 16. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,67.22,166.25,232.80,6.91;12,67.22,175.07,232.80,6.87;12,67.22,184.04,108.65,7.05" xml:id="b7">
	<analytic>
		<title level="a" type="main">What is &quot;critical&quot; about critical design?</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Bardzell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaowen</forename><surname>Bardzell</surname></persName>
		</author>
		<idno type="DOI">10.1145/2470654.2466451</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
				<meeting>the SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013-04-27">2013</date>
			<biblScope unit="page" from="3297" to="3306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,67.22,193.15,232.80,6.91;12,67.22,202.12,232.80,6.91;12,67.22,210.94,232.80,7.05;12,67.22,219.90,232.80,7.05;12,67.22,229.01,232.80,6.91;12,67.22,237.98,36.00,6.91" xml:id="b8">
	<analytic>
		<title level="a" type="main">Translating video recordings of mobile app usages into replayable scenarios</title>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Bernal-Cárdenas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oscar</forename><surname>Chaparro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrian</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denys</forename><surname>Poshyvanyk</surname></persName>
		</author>
		<idno type="DOI">10.1145/3377811.3380328</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering</title>
				<meeting>the ACM/IEEE 42nd International Conference on Software Engineering<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-06-27">2020</date>
			<biblScope unit="page" from="309" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,67.22,246.95,232.80,6.91;12,67.22,255.77,232.80,7.05;12,67.22,264.74,200.07,7.05" xml:id="b9">
	<analytic>
		<title level="a" type="main">Tales from the Dark Side: Privacy Dark Strategies and Privacy Dark Patterns</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Bösch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Erb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Kargl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henning</forename><surname>Kopp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Pfattheicher</surname></persName>
		</author>
		<idno type="DOI">10.1515/popets-2016-0038</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings on Privacy Enhancing Technologies</title>
		<idno type="ISSNe">2299-0984</idno>
		<imprint>
			<biblScope unit="volume">2016</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="237" to="254" />
			<date type="published" when="2016-07-14">2016</date>
			<publisher>Privacy Enhancing Technologies Symposium Advisory Board</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,67.22,273.85,232.80,6.91;12,67.22,282.81,136.04,6.91" xml:id="b10">
	<monogr>
		<title level="m" type="main">Dark patterns -user interfaces designed to trick people</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Brignull</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Miquel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Offer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,67.22,291.78,232.80,6.91;12,67.22,300.75,232.80,6.91;12,67.22,309.57,232.80,7.05;12,67.22,318.54,232.80,7.05;12,67.22,327.64,204.16,6.91" xml:id="b11">
	<analytic>
		<title level="a" type="main">Unblind your apps</title>
		<author>
			<persName><forename type="first">Jieshan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenchang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liming</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinshui</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3377811.3380327</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering</title>
				<meeting>the ACM/IEEE 42nd International Conference on Software Engineering<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-06-27">2020</date>
			<biblScope unit="page" from="322" to="334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,67.22,336.61,232.80,6.91;12,67.22,345.58,232.80,6.91;12,67.22,354.40,232.80,7.05;12,67.22,363.37,232.80,6.87;12,67.22,372.33,205.78,7.05" xml:id="b12">
	<analytic>
		<title level="a" type="main">Object detection for graphical user interface: old fashioned or deep learning or a combination?</title>
		<author>
			<persName><forename type="first">Jieshan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mulong</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenchang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liming</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoqiang</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1145/3368089.3409691</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering</title>
				<meeting>the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-11-08">2020</date>
			<biblScope unit="page" from="1202" to="1214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,67.22,381.44,232.80,6.91;12,67.22,390.27,232.80,7.05;12,67.22,399.38,71.55,6.91" xml:id="b13">
	<analytic>
		<title level="a" type="main">Malicious interface design</title>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Conti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Sobiesk</surname></persName>
		</author>
		<idno type="DOI">10.1145/1772690.1772719</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th international conference on World wide web</title>
				<meeting>the 19th international conference on World wide web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010-04-26">2010</date>
			<biblScope unit="page" from="271" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,67.22,408.34,232.80,6.91;12,67.22,417.31,232.80,6.91;12,67.22,426.13,232.80,7.05;12,67.22,435.10,232.80,7.05" xml:id="b14">
	<analytic>
		<title level="a" type="main">Rico</title>
		<author>
			<persName><forename type="first">Biplab</forename><surname>Deka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zifeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chad</forename><surname>Franzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Hibschman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Afergan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Nichols</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ranjitha</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="DOI">10.1145/3126594.3126651</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology</title>
				<meeting>the 30th Annual ACM Symposium on User Interface Software and Technology</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017-10-20">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,67.22,444.21,232.80,6.91;12,67.22,453.17,232.80,6.91;12,67.22,462.00,232.80,7.05;12,67.22,470.96,189.97,7.05" xml:id="b15">
	<analytic>
		<title level="a" type="main">UI Dark Patterns and Where to Find Them</title>
		<author>
			<persName><forename type="first">Linda</forename><surname>Di Geronimo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larissa</forename><surname>Braz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enrico</forename><surname>Fregnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Palomba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Bacchelli</surname></persName>
		</author>
		<idno type="DOI">10.1145/3313831.3376600</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2020 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-04-21">2020</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,67.22,480.07,232.80,6.91;12,67.22,488.90,232.80,7.05;12,67.22,497.86,232.80,6.87;12,67.22,506.97,55.61,6.91" xml:id="b16">
	<analytic>
		<title level="a" type="main">#darkpatterns</title>
		<author>
			<persName><forename type="first">Madison</forename><surname>Fansher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruthi</forename><forename type="middle">Sai</forename><surname>Chivukula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
		<idno type="DOI">10.1145/3170427.3188553</idno>
	</analytic>
	<monogr>
		<title level="m">Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018-04-20">2018</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,67.22,515.80,232.80,7.05;12,67.22,524.90,39.54,6.91" xml:id="b17">
	<monogr>
		<title level="m" type="main">Values at Play in Digital Games</title>
		<author>
			<persName><forename type="first">Mary</forename><surname>Flanagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helen</forename><surname>Nissenbaum</surname></persName>
		</author>
		<idno type="DOI">10.7551/mitpress/9016.001.0001</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,67.22,533.73,232.80,7.05;12,67.22,542.69,232.80,7.05;12,67.22,551.80,17.93,6.91" xml:id="b18">
	<analytic>
		<title level="a" type="main">A behavior model for persuasive design</title>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">J</forename><surname>Fogg</surname></persName>
		</author>
		<idno type="DOI">10.1145/1541948.1541999</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Conference on Persuasive Technology</title>
				<meeting>the 4th International Conference on Persuasive Technology</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009-04-26">2009</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,67.22,560.77,232.80,6.91;12,67.22,569.59,164.38,7.05" xml:id="b19">
	<analytic>
		<title level="a" type="main">In-Action Ethics: Table 1.</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Frauenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjo</forename><surname>Rauhala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geraldine</forename><surname>Fitzpatrick</surname></persName>
		</author>
		<idno type="DOI">10.1093/iwc/iww024</idno>
	</analytic>
	<monogr>
		<title level="j">Interacting with Computers</title>
		<title level="j" type="abbrev">Interact. Comput.</title>
		<idno type="ISSN">0953-5438</idno>
		<idno type="ISSNe">1873-7951</idno>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="220" to="236" />
			<date type="published" when="2017">2017</date>
			<publisher>Oxford University Press (OUP)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,67.22,578.56,232.80,7.05;12,67.22,587.53,210.63,7.05" xml:id="b20">
	<monogr>
		<title level="m" type="main">Human values, ethics, and design. The human-computer interaction handbook</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">H</forename><surname>Kahn</surname><genName>Jr</genName></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="1177" to="1201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,67.22,596.49,232.80,7.05;12,67.22,605.46,232.80,6.87;12,67.22,614.43,232.80,7.05;12,67.22,623.53,74.20,6.91" xml:id="b21">
	<analytic>
		<title level="a" type="main">What Kind of Work Do &quot;Asshole Designers&quot; Create? Describing Properties of Ethical Concern on Reddit</title>
		<author>
			<persName><forename type="first">Colin</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruthi</forename><forename type="middle">Sai</forename><surname>Chivukula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahreum</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1145/3357236.3395486</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 ACM Designing Interactive Systems Conference</title>
				<meeting>the 2020 ACM Designing Interactive Systems Conference<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-07-03">2020</date>
			<biblScope unit="page" from="61" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,67.22,632.50,232.80,6.91;12,67.22,641.32,232.80,7.05;12,67.22,650.29,200.73,7.05" xml:id="b22">
	<analytic>
		<title level="a" type="main">The Dark (Patterns) Side of UX Design</title>
		<author>
			<persName><forename type="first">Colin</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yubo</forename><surname>Kou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Battles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Hoggatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Austin</forename><forename type="middle">L</forename><surname>Toombs</surname></persName>
		</author>
		<idno type="DOI">10.1145/3173574.3174108</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2018 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018-04-21">2018</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,67.22,659.40,232.80,6.91;12,67.22,668.22,232.80,7.05;12,67.22,677.19,221.21,7.05" xml:id="b23">
	<analytic>
		<title level="a" type="main">Dark patterns in proxemic interactions</title>
		<author>
			<persName><forename type="first">Saul</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Boring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jo</forename><surname>Vermeulen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Dostal</surname></persName>
		</author>
		<idno type="DOI">10.1145/2598510.2598541</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on Designing interactive systems</title>
				<meeting>the 2014 conference on Designing interactive systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014-06-21">2014</date>
			<biblScope unit="page" from="523" to="532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,67.22,686.30,232.80,6.91;12,67.22,695.27,132.12,6.91" xml:id="b24">
	<analytic>
		<title level="a" type="main">Ranbaxy agrees to pay $500 million drug safety settlement</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="DOI">10.1007/s40278-013-3239-y</idno>
	</analytic>
	<monogr>
		<title level="j">Reactions Weekly</title>
		<title level="j" type="abbrev">Reactions Weekly</title>
		<idno type="ISSN">0114-9954</idno>
		<idno type="ISSNe">1179-2051</idno>
		<imprint>
			<biblScope unit="volume">1453</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="4" />
			<date type="published" when="2013-10">Oct 2013. November 16. 2020</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,67.22,703.11,232.80,6.91;12,67.22,711.93,97.00,7.05" xml:id="b25">
	<analytic>
		<title level="a" type="main">Designing the star user interface</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kimball</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">V E</forename><surname>Harslem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Byte</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="242" to="282" />
			<date type="published" when="1982">1982. 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,330.24,53.72,232.80,6.91;12,330.24,62.54,232.80,7.05;12,330.24,71.51,155.05,7.05" xml:id="b26">
	<analytic>
		<title level="a" type="main">Cuteness as a ‘Dark Pattern’ in Home Robots</title>
		<author>
			<persName><forename type="first">Cherie</forename><surname>Lacey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Caudwell</surname></persName>
		</author>
		<idno type="DOI">10.1109/hri.2019.8673274</idno>
	</analytic>
	<monogr>
		<title level="m">2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-03">2019. 2019</date>
			<biblScope unit="page" from="374" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,330.24,80.48,232.80,7.05;12,330.24,89.44,232.32,7.05" xml:id="b27">
	<analytic>
		<title level="a" type="main">Understanding Motivational Dark Patterns</title>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Lewis</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-4302-6422-4_8</idno>
	</analytic>
	<monogr>
		<title level="m">Irresistible Apps</title>
				<meeting><address><addrLine>Apress, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Apress</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="99" to="102" />
		</imprint>
	</monogr>
	<note>1st edition</note>
</biblStruct>

<biblStruct coords="12,330.24,98.55,232.80,6.91;12,330.24,107.52,232.80,6.91;12,330.24,116.34,232.80,7.05;12,330.24,125.45,17.93,6.91" xml:id="b28">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common Objects in Context</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-10602-1_48</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision – ECCV 2014</title>
				<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,330.24,134.42,232.80,6.91;12,330.24,143.24,232.80,7.05;12,330.24,152.21,232.79,7.05;12,330.24,161.32,130.63,6.91" xml:id="b29">
	<analytic>
		<title level="a" type="main">MadDroid: Characterizing and Detecting Devious Ad Contents for Android Apps</title>
		<author>
			<persName><forename type="first">Tianming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiapu</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tegawendé</forename><surname>Bissyandé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacques</forename><surname>Klein</surname></persName>
		</author>
		<idno type="DOI">10.1145/3366423.3380242</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference 2020</title>
				<meeting>The Web Conference 2020<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-04-20">2020</date>
			<biblScope unit="page" from="1715" to="1726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,330.24,170.28,232.80,6.91;12,330.24,179.11,232.80,7.05;12,330.24,188.07,232.80,6.87;12,330.24,197.04,232.80,7.05;12,330.24,206.15,127.17,6.91" xml:id="b30">
	<analytic>
		<title level="a" type="main">Owl eyes</title>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuekai</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3324884.3416547</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering</title>
				<meeting>the 35th IEEE/ACM International Conference on Automated Software Engineering<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-12-21">2020</date>
			<biblScope unit="page" from="398" to="409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,330.24,215.12,232.80,6.91;12,330.24,224.08,232.80,6.91;12,330.24,232.91,232.80,7.05;12,330.24,241.87,232.80,6.87;12,330.24,250.84,133.59,7.05" xml:id="b31">
	<analytic>
		<title level="a" type="main">Automated Repair of Internationalization Presentation Failures in Web Pages Using Style Similarity Clustering and Search-Based Techniques</title>
		<author>
			<persName><forename type="first">Sonal</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdulmajeed</forename><surname>Alameer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Mcminn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">G J</forename><surname>Halfond</surname></persName>
		</author>
		<idno type="DOI">10.1109/icst.2018.00030</idno>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE 11th International Conference on Software Testing, Verification and Validation (ICST)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-04">2018</date>
			<biblScope unit="page" from="215" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,330.24,259.95,232.80,6.91;12,330.24,268.91,232.80,6.91;12,330.24,277.74,232.80,6.87;12,330.24,286.70,148.33,7.05" xml:id="b32">
	<analytic>
		<title level="a" type="main">Using Visual Symptoms for Debugging Presentation Failures in Web Applications</title>
		<author>
			<persName><forename type="first">Sonal</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bailan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pooyan</forename><surname>Behnamghader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">G J</forename><surname>Halfond</surname></persName>
		</author>
		<idno type="DOI">10.1109/icst.2016.35</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Software Testing, Verification and Validation (ICST)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-04">2016</date>
			<biblScope unit="page" from="191" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,330.24,295.81,232.80,6.91;12,330.24,304.64,107.27,7.05" xml:id="b33">
	<analytic>
		<title level="a" type="main">Dark Design Patterns: An End-User Perspective</title>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Maier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rikard</forename><surname>Harr</surname></persName>
		</author>
		<idno type="DOI">10.17011/ht/urn.202008245641</idno>
	</analytic>
	<monogr>
		<title level="j">Human Technology</title>
		<title level="j" type="abbrev">Human Technology</title>
		<idno type="ISSNe">1795-6889</idno>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="170" to="199" />
			<date type="published" when="2020-08-31">2020</date>
			<publisher>Centre of Sociological Research, NGO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,330.24,313.75,232.80,6.91;12,330.24,322.71,232.80,6.91;12,330.24,331.54,232.80,7.05;12,330.24,340.50,112.83,7.05" xml:id="b34">
	<analytic>
		<title level="a" type="main">Dark Patterns at Scale</title>
		<author>
			<persName><forename type="first">Arunesh</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunes</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Lucherini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marshini</forename><surname>Chetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Narayanan</surname></persName>
		</author>
		<idno type="DOI">10.1145/3359183</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Human-Computer Interaction</title>
		<title level="j" type="abbrev">Proc. ACM Hum.-Comput. Interact.</title>
		<idno type="ISSNe">2573-0142</idno>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">CSCW</biblScope>
			<biblScope unit="page" from="1" to="32" />
			<date type="published" when="2019-11-07">2019</date>
			<publisher>Association for Computing Machinery (ACM)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,330.24,349.61,232.80,6.91;12,330.24,358.58,232.80,6.91;12,330.24,367.40,232.80,7.05;12,330.24,376.51,70.09,6.91" xml:id="b35">
	<analytic>
		<title level="a" type="main">Machine Learning-Based Prototyping of Graphical User Interfaces for Mobile Apps</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Moran</surname></persName>
			<idno type="ORCID">0000-0001-9683-5616</idno>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Bernal-Cardenas</surname></persName>
			<idno type="ORCID">0000-0002-6209-5346</idno>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Curcio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Bonett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denys</forename><surname>Poshyvanyk</surname></persName>
			<idno type="ORCID">0000-0002-5626-7586</idno>
		</author>
		<idno type="DOI">10.1109/tse.2018.2844788</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Software Engineering</title>
		<title level="j" type="abbrev">IIEEE Trans. Software Eng.</title>
		<idno type="ISSN">0098-5589</idno>
		<idno type="ISSNe">2326-3881</idno>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="196" to="221" />
			<date type="published" when="2020-02-01">2020</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,330.24,385.48,232.80,6.91;12,330.24,394.30,232.80,7.05;12,330.24,403.27,232.79,7.05;12,330.24,412.38,232.80,6.91;12,330.24,421.34,36.00,6.91" xml:id="b36">
	<analytic>
		<title level="a" type="main">Automated reporting of GUI design violations for mobile apps</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Bernal-Cárdenas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jelf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denys</forename><surname>Poshyvanyk</surname></persName>
		</author>
		<idno type="DOI">10.1145/3180155.3180246</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International Conference on Software Engineering</title>
				<meeting>the 40th International Conference on Software Engineering<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018-05-27">2018</date>
			<biblScope unit="page" from="165" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,330.24,430.31,232.80,6.91;12,330.24,439.13,232.80,7.05;12,330.24,448.10,232.80,7.05" xml:id="b37">
	<analytic>
		<title level="a" type="main">Impulse Buying</title>
		<author>
			<persName><forename type="first">Carol</forename><surname>Moser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarita</forename><forename type="middle">Y</forename><surname>Schoenebeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Resnick</surname></persName>
		</author>
		<idno type="DOI">10.1145/3290605.3300472</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2019 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019-05-02">2019</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,330.24,457.21,232.80,6.91;12,330.24,466.03,232.80,6.87;12,330.24,475.00,101.56,7.05" xml:id="b38">
	<analytic>
		<title level="a" type="main">Enhancing the explanatory power of usability heuristics</title>
		<author>
			<persName coords=""><forename type="first">Jakob</forename><surname>Nielsen</surname></persName>
		</author>
		<idno type="DOI">10.1145/191666.191729</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI conference on Human factors in computing systems celebrating interdependence - CHI &apos;94</title>
				<meeting>the SIGCHI conference on Human factors in computing systems celebrating interdependence - CHI &apos;94</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="152" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,330.24,483.96,232.80,7.05;12,330.24,492.93,197.21,7.05" xml:id="b39">
	<analytic>
		<title level="a" type="main">Usability inspection methods</title>
		<author>
			<persName coords=""><forename type="first">Jakob</forename><surname>Nielsen</surname></persName>
		</author>
		<idno type="DOI">10.1145/259963.260531</idno>
	</analytic>
	<monogr>
		<title level="m">Conference companion on Human factors in computing systems - CHI &apos;94</title>
				<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="413" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,330.24,501.90,232.80,7.05;12,330.24,510.86,232.45,7.05" xml:id="b40">
	<monogr>
		<title level="m" type="main">Designing Web Usability: The Practice of Simplicity</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Nielsen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>New Riders Publishing</publisher>
			<biblScope unit="page" from="166" to="167" />
			<pubPlace>Indianapolis</pubPlace>
		</imprint>
	</monogr>
	<note>Site design</note>
</biblStruct>

<biblStruct coords="12,330.24,519.97,232.80,6.91;12,330.24,528.80,232.80,6.87;12,330.24,537.76,100.68,7.05" xml:id="b41">
	<analytic>
		<title level="a" type="main">Heuristic evaluation of user interfaces</title>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rolf</forename><surname>Molich</surname></persName>
		</author>
		<idno type="DOI">10.1145/97243.97281</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI conference on Human factors in computing systems Empowering people - CHI &apos;90</title>
				<meeting>the SIGCHI conference on Human factors in computing systems Empowering people - CHI &apos;90</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,330.24,546.87,232.80,6.91;12,330.24,555.70,224.25,7.05" xml:id="b42">
	<monogr>
		<title level="m" type="main">Human-computer interaction</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Preece</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Benyon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Holland</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Carey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Addison-Wesley Longman Ltd</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,330.24,564.80,232.80,6.91;12,330.24,573.77,232.80,6.91;12,330.24,582.74,232.80,6.91" xml:id="b43">
	<analytic>
		<title level="a" type="main">Smart Dark Pattern Detection: Making Aware of Misleading Patterns Through the Intended App</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">Hrushikesava</forename><surname>Raju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saiyed</forename><forename type="middle">Faiayaz</forename><surname>Waris</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Adinarayna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijaya</forename><forename type="middle">Chandra</forename><surname>Jadala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">Subba</forename><surname>Rao</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-981-16-5157-1_72</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Intelligent Systems and Computing</title>
				<editor>
			<persName><forename type="first">S</forename><surname>Shakya</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><forename type="middle">E</forename><surname>Balas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Kamolphiwong</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K.-L</forename></persName>
		</editor>
		<imprint>
			<publisher>Springer Singapore</publisher>
			<date type="published" when="2021-10-26" />
			<biblScope unit="page" from="933" to="947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,330.24,591.56,232.80,7.05;12,330.24,600.67,122.29,6.91" xml:id="b44">
	<monogr>
		<author>
			<persName coords=""><surname>Du</surname></persName>
		</author>
		<title level="m">Sentimental Analysis and Deep Learning</title>
				<meeting><address><addrLine>Singapore; Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="933" to="947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,330.24,609.64,232.80,6.91;12,330.24,618.46,232.80,7.05;12,330.24,627.43,170.10,7.05" xml:id="b45">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</title>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
			<idno type="ORCID">0000-0001-7318-9658</idno>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/tpami.2016.2577031</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<title level="j" type="abbrev">IEEE Trans. Pattern Anal. Mach. Intell.</title>
		<idno type="ISSN">0162-8828</idno>
		<idno type="ISSNe">2160-9292</idno>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2015">2015</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,330.24,636.54,232.80,6.91;12,330.24,645.36,232.80,6.87;12,330.24,654.33,164.39,7.05" xml:id="b46">
	<analytic>
		<title level="a" type="main">Reflective design</title>
		<author>
			<persName><forename type="first">Phoebe</forename><surname>Sengers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kirsten</forename><surname>Boehner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shay</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">'jofish'</forename><surname>Kaye</surname></persName>
		</author>
		<idno type="DOI">10.1145/1094562.1094569</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th decennial conference on Critical computing: between sense and sensibility</title>
				<meeting>the 4th decennial conference on Critical computing: between sense and sensibility</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005-08-20">2005</date>
			<biblScope unit="page" from="49" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,330.24,663.29,232.80,7.05;12,330.24,672.26,171.52,7.05" xml:id="b47">
	<analytic>
		<title level="a" type="main">Values Levers</title>
		<author>
			<persName coords=""><forename type="first">Katie</forename><surname>Shilton</surname></persName>
		</author>
		<idno type="DOI">10.1177/0162243912436985</idno>
	</analytic>
	<monogr>
		<title level="j">Science, Technology, &amp; Human Values</title>
		<title level="j" type="abbrev">Science, Technology, &amp; Human Values</title>
		<idno type="ISSN">0162-2439</idno>
		<idno type="ISSNe">1552-8251</idno>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="374" to="397" />
			<date type="published" when="2013">2013</date>
			<publisher>SAGE Publications</publisher>
		</imprint>
	</monogr>
	<note>Science</note>
</biblStruct>

<biblStruct coords="12,330.24,681.37,232.80,6.91;12,330.24,690.33,232.80,6.91;12,330.24,699.16,232.80,7.05;12,330.24,708.12,199.82,7.05" xml:id="b48">
	<analytic>
		<title level="a" type="main">How to see values in social computing</title>
		<author>
			<persName><forename type="first">Katie</forename><surname>Shilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jes</forename><forename type="middle">A</forename><surname>Koepfler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><forename type="middle">R</forename><surname>Fleischmann</surname></persName>
		</author>
		<idno type="DOI">10.1145/2531602.2531625</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM conference on Computer supported cooperative work &amp; social computing</title>
				<meeting>the 17th ACM conference on Computer supported cooperative work &amp; social computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014-02-15">2014</date>
			<biblScope unit="page" from="426" to="435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,67.22,53.72,232.80,6.91;13,67.22,62.55,232.80,7.05;13,67.22,71.51,232.80,6.87;13,67.22,80.48,101.56,7.05" xml:id="b49">
	<analytic>
		<title level="a" type="main">(Un)informed Consent</title>
		<author>
			<persName><forename type="first">Christine</forename><surname>Utz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Degeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sascha</forename><surname>Fahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Schaub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Holz</surname></persName>
		</author>
		<idno type="DOI">10.1145/3319535.3354212</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security</title>
				<meeting>the 2019 ACM SIGSAC Conference on Computer and Communications Security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019-11-06">2019</date>
			<biblScope unit="page" from="973" to="990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,67.22,89.59,232.80,6.91;13,67.22,98.55,232.80,6.91;13,67.22,107.38,232.80,7.05;13,67.22,116.49,232.80,6.91;13,67.22,125.45,74.66,6.91" xml:id="b50">
	<analytic>
		<title level="a" type="main">Opt out of privacy or &quot;go home&quot;</title>
		<author>
			<persName><forename type="first">Fiona</forename><surname>Westin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sonia</forename><surname>Chiasson</surname></persName>
		</author>
		<idno type="DOI">10.1145/3368860.3368865</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the New Security Paradigms Workshop</title>
				<meeting>the New Security Paradigms Workshop<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019-09-23">2019</date>
			<biblScope unit="page" from="57" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,67.22,134.42,232.80,6.91;13,67.22,143.24,232.80,7.05;13,67.22,152.21,232.80,7.05;13,67.22,161.32,232.80,6.91;13,67.22,170.28,36.00,6.91" xml:id="b51">
	<analytic>
		<title level="a" type="main">Screen Parsing: Towards Reverse Engineering of UI Models from Screenshots</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Nichols</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">P</forename><surname>Bigham</surname></persName>
		</author>
		<idno type="DOI">10.1145/3472749.3474763</idno>
	</analytic>
	<monogr>
		<title level="m">The 34th Annual ACM Symposium on User Interface Software and Technology</title>
				<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021-10-10">2021</date>
			<biblScope unit="page" from="470" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,67.22,179.11,232.80,7.05;13,67.22,188.07,232.80,7.05;13,67.22,197.18,169.29,6.91" xml:id="b52">
	<analytic>
		<title level="a" type="main">UIED: a hybrid tool for GUI element detection</title>
		<author>
			<persName><forename type="first">Mulong</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sidong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenchang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieshan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyang</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1145/3368089.3417940</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering</title>
				<meeting>the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-11-08">2020</date>
			<biblScope unit="page" from="1655" to="1659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,67.22,206.15,232.80,6.91;13,67.22,215.12,232.80,6.91;13,330.24,53.58,232.80,7.05;13,330.24,62.55,220.60,7.05" xml:id="b53">
	<analytic>
		<title level="a" type="main">Don’t Do That! Hunting Down Visual Design Smells in Complex UIs Against Design Guidelines</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenchang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deheng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanping</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/icse43902.2021.00075</idno>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021-05">2021</date>
			<biblScope unit="page" from="761" to="772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,330.24,71.65,232.80,6.91;13,330.24,80.62,43.91,6.91" xml:id="b54">
	<monogr>
		<title level="m" type="main">Dark patterns in the design of games</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Zagal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Björk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lewis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,330.24,89.59,232.80,6.91;13,330.24,98.55,232.80,6.91;13,330.24,107.52,232.80,6.91;13,330.24,116.34,232.80,7.05;13,330.24,125.31,232.80,7.05;13,330.24,134.42,147.89,6.91" xml:id="b55">
	<analytic>
		<title level="a" type="main">Screen Recognition: Creating Accessibility Metadata for Mobile Applications from Pixels</title>
		<author>
			<persName><forename type="first">Xiaoyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lilian</forename><surname>De Greef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Swearngin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Nichols</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Fleizach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Everitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">P</forename><surname>Bigham</surname></persName>
		</author>
		<idno type="DOI">10.1145/3411764.3445186</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2021 CHI Conference on Human Factors in Computing Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021-05-06">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,330.24,143.39,232.80,6.91;13,330.24,152.35,232.80,6.91;13,330.24,161.18,232.80,7.05;13,330.24,170.14,232.80,7.05;13,330.24,179.25,204.16,6.91" xml:id="b56">
	<analytic>
		<title level="a" type="main">Seenomaly</title>
		<author>
			<persName><forename type="first">Dehai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenchang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liming</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinshui</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3377811.3380411</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering</title>
				<meeting>the ACM/IEEE 42nd International Conference on Software Engineering<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-06-27">2020</date>
			<biblScope unit="page" from="1286" to="1297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,330.24,188.22,232.80,6.91;13,330.24,197.04,232.80,7.05;13,330.24,206.15,17.93,6.91" xml:id="b57">
	<analytic>
		<title level="a" type="main">EAST: An Efficient and Accurate Scene Text Detector</title>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">He</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuchang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2017.283</idno>
		<idno>CoRR, abs/1704.03155</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-07">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
